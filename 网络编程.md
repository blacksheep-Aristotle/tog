# 基础网络编程

## 文件操作

在linux下，一切皆文件，目录是文件，称为目录文件，内容是该目录的目录项（但是目录只有内核可以编辑，超级用户也不可以编辑），设备也是设备文件，在/dev存放的就是一些设备文件，linux的文件系统(VFS:虚拟文件系统：提供一种机制，将各种不同的文件系统结合起来，并且提供统一的应用程序编程接口，我们可以不用考虑针对不同的文件系统去采用不同的读写方式)主要用于管理文件存储空间的分配，文件访问权限的维护，以及对文件的各种操作，我们可以通过系统调用一些函数来实现对文件的创建，打开，关闭，读写，删除，以及对其权限的修改等等，其实我们c语言的标准库和一些shell命令封装的还是操作系统提供给我们的底层的一些接口，即一些底层函数，例如c语言标准库中的fread,fwrite就是封装的read和write函数，shell命令中的chmod封装的就是chmod函数。

#### 调用fcntl改变文件描述符的属性

当多个用户共同使用、操作一个文件的情况，为了防止一个线程尚未执行完成时另一个线程在中间加塞，Linux 通常采用给文件上锁的方法，来避免共享的资源产生竞争的状态。

文件锁包括**建议性锁**和**强制性锁**。

**建议性锁要求每个上锁文件的进程都要检查是否有锁存**，并且尊重已有的锁。在一般情况下，内核和系统都不使用建议性锁。**强制性锁是由内 核执行的锁，当一个文件被上锁进行写入操作的时候，内核将阻止其他任何文件对其进行读写操作**。采用强制性锁对性能的影响很大，每次读写操作都必须检查是否有锁存在。

在 Linux 中，实现文件上锁的函数有lock和fcntl，其中**flock用于对文件施加建议性锁,而fcntl不仅可以施加建议性锁，还可以施加强制锁。同时，fcntl还能对文件的某一记录进行上锁，也就是记录锁。**

记录锁又可分为**读取锁和写入锁**，其中读取锁又称为共享锁，它能够使多个进程都能在文件的同一部分建立读取锁。而写入锁又称为排斥锁，在任何时刻只能有一个进程在文件的某个部分上建立写入锁。当然，**在文件的同一部分不能同时建立读取锁和写入锁。**

```c++
#include<unistd.h>  
#include<fcntl.h>  
int fcntl(int fd, int cmd);  
int fcntl(int fd, int cmd, long arg);  
int fcntl(int fd, int cmd ,struct flock* lock);  

fd：文件描述符，要操作的文件描述符
    

cmd：函数传入值
F_DUPFD 用来查找大于或等于参数arg 的最小且仍未使用的文件描述词, 并且复制参数fd 的文件描述词. 执行成功则返回新复制的文件描述词. 请参考dup2(). F_GETFD 取得close-on-exec 旗标. 若此旗标的FD_CLOEXEC 位为0, 代表在调用exec()相关函数时文件将不会关闭.
F_SETFD 设置close-on-exec 旗标. 该旗标以参数arg 的FD_CLOEXEC 位决定.
F_GETFL 取得文件描述词状态旗标, 此旗标为open()的参数flags.
F_SETFL 设置文件描述词状态旗标（将文件状态标志设置为第三个参数的值）, 可以更改的几个标志是:O_APPEND,O_NONBLOCK（非阻塞）,O_SYNC和O_ASYNC。
F_GETLK 取得文件锁定的状态.
F_SETLK 设置文件锁定的状态. 此时flcok 结构的l_type 值必须是F_RDLCK、F_WRLCK 或F_UNLCK. 如果无法建立锁定, 则返回-1, 错误代码为EACCES 或EAGAIN.
F_SETLKW 同F_SETLK 作用相同, 但是无法建立锁定时, 此调用会一直等到锁定动作成功为止. 若在等待锁定的过程中被信号中断时, 会立即返回-1, 错误代码为EINTR. 参数lock 指针为flock 结构指针

Lock：结构为flock，设置记录锁的具体状态，后面会详细说明
成功：0
出错：-1
struct flock  
{  
    short_l_type;    /*锁的类型*/ 共享锁（F_RDLCK,读锁）互斥锁（F_WDLCK,写锁） 解锁（F_UNLCK，删除之前加的锁）
    short_l_whence;  /*偏移量的起始位置：SEEK_SET,SEEK_CUR,SEEK_END*/  
    off_t_l_start;     /*加锁的起始偏移*/  
    off_t_l_len;    /*上锁字节*/  
    //l_whence确定文件内部的位置指针从哪开始，l_star确定从l_whence开始的位置的偏移量，两个变量一起确定了文件内的位置指针先所指的位置，即开始上锁的位置，然后l_len的字节数就确定了上锁的区域。l_whence,l_start,l_len都为0时，代表给整个文件都上锁
    pid_t_l_pid;   /*锁的属主进程ID */  
};   
```



```

```

用户空间 / 内核空间
现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。
操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。

进程切换
为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的，并且进程切换是非常耗费资源的。

进程阻塞
正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得了CPU资源），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。

文件描述符
文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。
文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。

缓存I/O
缓存I/O又称为标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，操作系统会将I/O的数据缓存在文件系统的页缓存中，即数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20210930145752951.png" alt="image-20210930145752951" style="zoom:67%;" />

## 网络编程中接受连接请求的套接字创建过程

#### **step1.调用socket函数创建套接字**

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020120119100858.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzE3MTE5,size_16,color_FFFFFF,t_70#pic_center)

```c++
#include <sys/socket. h> 
int socket(int domain, int type, int protocol);
int tcp_socket = socket(AF_INET, SOCK_STREAM, 0);  //创建TCP套接字
int udp_socket = socket(AF_INET, SOCK_DGRAM, 0);  //创建UDP套接字
如果紧接着调用bind，listen函数，此套接字成为服务器端套接字； 如果调用connect函数，将成为客户端套接字
严格来说套接字属于操作系统而非进程，只是进程拥有代表相应套接字的文件描述符。
```

domain：套接字中使用的协议族信息

![image-20210925233835686](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20210925233835686.png)

type：套接字数据传输类型信息

**套接字类型指 的是套接 的数据传输方式，通过socket函数的第 个参数传递，只有这样才能决定创建的套接字的数据传输方式 。**

protocol：计算机间通信中使用的信息协议

**TCP套接字的I/O缓冲**

write()/send() 并不立即向网络中传输数据，而是先将数据写入缓冲区中，再由TCP协议将数据从缓冲区发送到目标机器。一旦将数据写入到缓冲区，函数就可以成功返回，不管它们有没有到达目标机器，也不管它们何时被发送到网络，这些都是TCP协议负责的事情。

TCP协议独立于 write()/send() 函数，数据有可能刚被写入缓冲区就发送到网络，也可能在缓冲区中不断积压，多次写入的数据被一次性发送到网络，这取决于当时的网络情况、当前线程是否空闲等诸多因素，不由程序员控制。

read()/recv() 函数也是如此，也从输入缓冲区中读取数据，而不是直接从网络中读取。

![img](http://c.biancheng.net/cpp/uploads/allimg/151018/1-15101Q60GV27.jpg)

这些I/O缓冲区特性可整理如下：

- I/O缓冲区在每个TCP套接字中单独存在；

- I/O缓冲区在创建套接字时自动生成；

- 即使关闭套接字也会继续传送输出缓冲区中遗留的数据；

- 关闭套接字将丢失输入缓冲区中的数据。

  ```c++
  输入输出缓冲区的默认大小一般都是 8K，可以通过 getsockopt() 函数获取： 
   unsigned optVal;
   int optLen = **sizeof**(int);
   getsockopt(servSock, SOL_SOCKET, SO_SNDBUF, (char*)&optVal, &optLen);
   printf("Buffer length: %d\n", optVal);
  
   Buffer length: 8192
  ```

  ## TCP的阻塞模式

  对于TCP套接字（默认情况下），当使用 write()/send() 发送数据时：
  1) 首先会检查缓冲区，如果缓冲区的可用空间长度小于要发送的数据，那么 write()/send() 会被阻塞（暂停执行），直到缓冲区中的数据被发送到目标机器，腾出足够的空间，才唤醒 write()/send() 函数继续写入数据。

  2) 如果TCP协议正在向网络发送数据，那么输出缓冲区会被锁定，不允许写入，write()/send() 也会被阻塞，直到数据发送完毕缓冲区解锁，write()/send() 才会被唤醒。

  3) 如果要写入的数据大于缓冲区的最大长度，那么将分批写入。

  4) 直到所有数据被写入缓冲区 write()/send() 才能返回。

  当使用 read()/recv() 读取数据时：
  1) 首先会检查缓冲区，如果缓冲区中有数据，那么就读取，否则函数会被阻塞，直到网络上有数据到来。

  2) 如果要读取的数据长度小于缓冲区中的数据长度，那么就不能一次性将缓冲区中的所有数据读出，剩余数据将不断积压，直到有 read()/recv() 函数再次读取。

  3) 直到读取到数据后 read()/recv() 函数才会返回，否则就一直被阻塞。

  这就是TCP套接字的阻塞模式。所谓阻塞，就是上一步动作没有完成，下一步动作将暂停，直到上一步动作完成后才能继续，以保持同步性。





#### **step2.调用bind函数分配ip地址和端口号**

<img src="https://img-blog.csdnimg.cn/20201201191412330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzE3MTE5,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />

```c++
#include <sys/socket.h> 
struct sockaddr_in //IPv4
{
    sa_family_t     sin_family;地址族
    uint16_t        sin_port;16位TCP/UDP端口号
    struct in_addr  sin_addr;32位IP地址
    Char            sin_zero[8];不使用
}
struct in_addr
{
    In_addr_t       s_addr;32位IPV4地址
}
struct sockaddr_in6 //IPv6
{ 
    sa_family_t sin6_family;  //(2)地址类型，取值为AF_INET6
    in_port_t sin6_port;  //(2)16位端口号
    uint32_t sin6_flowinfo;  //(4)IPv6流信息
    struct in6_addr sin6_addr;  //(4)具体的IPv6地址
    uint32_t sin6_scope_id;  //(4)接口范围ID
};
struct sockaddr
{
    sa_family_t     sin_family;地址族
    char            sa_data[14];地址信息（IP地址和端口号等等）
}

int bind(int sockfd , struct sockaddr *myaddr, socklen_t addrlen)
sockfd:要分配地址信息（IP地址和端口号）的套接字文件描述符
myaddr:存有地址信息的结构体变量地址值
addrlen:第二个结构体变量的长度
诺bind函数调用成功，第二个参数指定的地址信息分配给第一个参数中的相应套接字
    
    
UNIX Domain SOCKET 是在Socket架构上发展起来的用于同一台主机的进程间通讯（IPC）。它不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序列号应答等。只是将应用层数据从一个进程拷贝到另一个进程。UNIX Domain SOCKET有SOKCET_DGRAM和SOCKET_STREAM两种模式，类似于UDP和TCP，但是面向消息的UNIX socket也是可靠的，消息既不会丢失也不会顺序错乱。
struct sockaddr_un {undefined
    sa_family_t sun_family; /* AF_UNIX */
    char sun_path[UNIX_PATH_MAX];   /* pathname */
};
```

不同CPU中， 4字节数型值1在内存空间的保存方式是不同的 4字节整数型值1可用2进制
表示如下
øøøøøeøø øøøøøøøθθøeøøøøø øøeøøθØ1 
CPU以这种顺 保存到内存， 外一些CPU则以倒序保存
øøøøøøel øθθøøøøø øøøøøøøø øøøøøøøø 
**若不考虑这些就收发数据则会出现问题，因为保存顺序的不同意味着对接收数据的解析顺序也不同**

**字节序 Order 与网络字节序**

CPU 向内存保存数据的方式有 两种，这意味着CPU解析数据的方式也分为
大端序 Big Endian 高位字节存放到低位地址
小端序 Little Endian :高位字节存放到高位地
**为了避免这个问题，在通过网络传输数据时约定统一，这种约定称为网络字节序( Network Byte Order )统一为大端序**

##### **先把数据数组转化成大端序格式再进行网络传输**

```c++
//字节序转换
h-主机 n-网络 s-short l-long ;
htons:把short型数据从主机字节序转换成网络字节序;
ntohs;把short型数据从网络字节序转换成主机字节序;
unsigned short htons;
unsigned short ntohs;
unsigned long  htonl;
unsigned long  ntohl;

unsigned short host_port=0*1234;
unsigned short net_port=hotns(host_port);//host ordered port;

//将字符串信息（IP地址）转换成网络字节序的32位整数型
in_addr_t inet_addr(const char*string);//如果IP地址不规范，返回INADDR_NONE;
int inet_aton(const char *string,struct in_addr*addr);
//string:需转换的IP地址的字符串地址值，addr：将保存转换结果的in_addr结构体变量的地址值;

char *addr1="1.2.3.4";
unsigned long con_addr=inet_addr(addr1);
if(con_addr==INADDR_NONE)
    error;
else
    printf("%#lx",con_addr);
0*4030201
    
char *addr1="1.2.3.4";
struct sockaddr_in addr_inet;
if(!inet_aton(addr1,&addr_inet.sin_addr))
    error;
else
    printf("%#x",addr_inet.sin_addr.s_addr);
0*4030201
    
//把网络字节序整数型IP地址转换字符串型
char *inet_ntoa(struct in_addr adr);
```

**网络地址初始化**

```c++
int serv_sock;
struct sockaddr_in addr;
char *serv_port="9190";          //声明端口号字符串
*创建服务器端套接字（监听套接字）*
serv_sock=socket(PF_INET, SOCK_STREAM, 0);
*地址信息初始化*
memset(&addr,0,sizeof(addr));    //结构体变量addr初始化为0
addr.sin_family=AF_INET;         //IPv4
addr.sin_addr.s_addr=inet_addr(INADDR_ANY); //利用INADDR_ANY分配服务器端的IP地址
addr.sin_port=htons(atoi(serv_port)); //将字符串的端口号转换成网络字节形式并初始化
*分配地址信息*
bind(serv_sock,(struct sockaddr*)&serv_addr,sizeof(serv_addr));
```



#### **step3.调用listen函数转为可接受请求状态**

**对于服务器端程序，使用 bind() 绑定套接字后，还需要使用 listen() 函数让套接字进入被动监听（当没有客户端请求时，套接字处于“睡眠”状态，只有当接收到客户端请求时，套接字才会被“唤醒”来响应请求。）状态，再调用 accept() 函数，就可以随时响应客户端的请求了。**

```c++
#include <sys/socket.h> 
int listen(int sockfd , int backlog); 
sock:需要进入监听状态的套接字;
backlog:请求队列的最大长度（例：若为5，则队列长度为5，最多让5个连接请求进入队列
```

当套接字正在处理客户端请求时，如果有新的请求进来，套接字是没法处理的，只能把它放进缓冲区，待当前请求处理完毕后，再从缓冲区中读取出来处理。如果不断有新的请求进来，它们就按照先后顺序在缓冲区中排队，直到缓冲区满。这个缓冲区，就称为请求队列（Request Queue）。

**缓冲区的长度（能存放多少个客户端请求）可以通过 listen() 函数的 backlog 参数指定**，但究竟为多少并没有什么标准，可以根据你的需求来定，并发量小的话可以是10或者20。

如果将 backlog 的值设置为 **SOMAXCONN**，就由系统来决定请求队列长度，这个值一般比较大，可能是几百，或者更多。

当请求队列满时，就不再接收新的请求，对于 Linux，客户端会收到 ECONNREFUSED 错误，对于 Windows，客户端会收到 WSAECONNREFUSED 错误。

**注意：listen() 只是让套接字处于监听状态，并没有接收请求。接收请求需要使用 accept() 函数。**



#### **step4.调用accept函数处理连接请求**

**accept函数受理连接请求等待队列中待处理的客户端连接请求**

```c++
#include<sys/socket.h>
int accept(int sockfd, struct sockaddr *addr,socklen_t *addrlen); 
sock 为服务器端套接字;
addr 为 sockaddr_in 结构体变量，保存发起连接请求的客户端地址信息的变量地址值;
addrlen 为参数 addr 的长度，可由 sizeof() 求得，被填入客户端地址长度;

socklen_t是一种数据类型，它其实和int差不多，在32位机下，size_t和int的长度相同，都是32 bits,但在64位机下，size_t（32bits）和int（64 bits）的长度是不一样的,socket编程中的accept函数的第三个参数的长度必须和int的长度相同。于是便有了socklen_t类型。
```



![image-20211031193819836](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211031193819836.png)



**accept() 返回一个新的套接字来和客户端通信，addr 保存了客户端的IP地址和端口号，而 sock 是服务器端的套接字。后面和客户端通信时，要使用这个新生成的套接字，而不是原来服务器端的套接字。**

listen() 只是让套接字进入监听状态，并没有真正接收客户端请求，listen() 后面的代码会继续执行，直到遇到 accept()。accept() 会阻塞程序执行（后面代码不能被执行），直到有新的请求到来。

#### 调用read函数读取数据

**size_t 是通过 typedef 声明的 unsigned int 类型；ssize_t 在 "size_t" 前面加了一个"s"，代表 signed，即 ssize_t 是通过 typedef 声明的 signed int 类型。**

```c++
#include<sys/socket.h>
ssize_t read(int fd,void*buf,size_t nbytes)
fd:要读取的文件的描述符;
buf:要接受数据的缓冲区地址:
nbyter:要读取的数据的字节数:

char *message[30];

str_len=read(sock, message, sizeof(message)-1);  
	if(str_len==-1)
		error_handling("read() error!");
	printf("Message from server: %s \n", message);  
```

read() 函数会从 fd 文件中读取 nbytes 个字节并保存到**缓冲区** buf，成功则返回读取到的字节数（但遇到文件结尾则返回0），失败则返回 -1。

**TCP不会因为缓冲溢出而丢失数据（因为TCP的滑动窗口协议，使TCP会控制数据流不超过缓冲大小）**

#### 调用write函数发送数据

```c++
#include<sys/socket.h>
ssize_t write(int fd, const void *buf, size_t nbytes);
fd 为要写入的文件的描述符;
buf 为要写入的数据的缓冲区地址;
nbytes 为要写入的数据的字节数;
```

write() 函数会将**缓冲区** buf 中的 nbytes 个字节写入文件 fd，成功则返回写入的字节数，失败则返回 -1。

write函数在数据移动到输出缓冲时返回，TCP的write会在数据传输完成时放回（因为TCP要保证对输出缓冲数据的传输）

**对方主机的输入缓冲剩余50字节空间时，若本方主机通过write 函数请求传输70字节，请问TCP如何处理这种情况?**

可存储在接收的相对主机的输入缓冲上的数据大小将传递给传输数据的主机。因此，在可用空间为 50 字节的情况下，即使请求 70 字节的数据传输，也不会传输超过 50 字节。其余部分存储在传输端的输出缓冲中，等待对方主机的输入按钮有余量。因此，它被称为滑动窗口，指向交换缓冲区中可用空间信息的协议，这也是作为 TCP 的一部分存在的协议。



![image-20210930150105644](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20210930150105644.png)

**服务器端**

收到连接请求后向请求者返回“hello word”回复

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

void error_handling(char *message);

int main(int argc, char *argv[])
{
	int serv_sock;
	int clnt_sock;

	struct sockaddr_in serv_addr;
	struct sockaddr_in clnt_addr;
	socklen_t clnt_addr_size;

	char message[]="Hello World!";
	
	if(argc!=2){
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}
	//调用socket函数创建服务端套接字。
	serv_sock=socket(PF_INET, SOCK_STREAM, 0);
	if(serv_sock == -1)
		error_handling("socket() error");
	//初始化服务端的地址
	memset(&serv_addr, 0, sizeof(serv_addr));
	serv_addr.sin_family=AF_INET;
	serv_addr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_addr.sin_port=htons(atoi(argv[1]));
	//调用bind函数分配ip地址和端口号
	if(bind(serv_sock, (struct sockaddr*) &serv_addr, sizeof(serv_addr))==-1 )
		error_handling("bind() error"); 
	//调用listen函数将套接字转为可接受连接状态
	if(listen(serv_sock, 5)==-1)
		error_handling("listen() error");
	
	clnt_addr_size=sizeof(clnt_addr);  
    //clnt addr作为输出型参数，当有客户端连接，将客户端的地址信息赋值给clnt addr
	//调用accept函数手里连接请求。如果再没有连接请求的情况下调用该函数，则不会返回，直到有连接请求为止。从队头取一个连接请求与客户端建立连接，并返回创建的套接字来和客户端通信
	clnt_sock=accept(serv_sock, (struct sockaddr*)&clnt_addr,&clnt_addr_size);
	if(clnt_sock==-1)
		error_handling("accept() error");  
	//如果42执行到本行，说明已经有了连接请求
	write(clnt_sock, message, sizeof(message));
	close(clnt_sock);	
	close(serv_sock);
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```

### **客户端**

#### 1.调用socket函数创建套接字

#### 2.调用connect函数向服务器发送连接请求

```c++
#include <sys/socket.h> 
int connect(int sockfd, struct sockaddr *serv_addr, socklen_t addrlen);
sockfd:客户端套接字;
servaddr:保存目标服务器端地址信息;
客户端的IP地址和端口在调用connect函数时于操作系统（内核）中自动分配
```

客户端调用connect函数后，发生以下情况之一才会返回：

服务器端接受连接请求（客户端的连接请求进入请求队列）

发生断网等异常情况而中断连接请求

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

void error_handling(char *message);

int main(int argc, char* argv[])
{
	int sock;
	struct sockaddr_in serv_addr;
	char message[30];
	int str_len;
	
	if(argc!=3){
		printf("Usage : %s <IP> <port>\n", argv[0]);
		exit(1);
	}
	//创建客户端套接字。连接协议是TCP，这个套接字是一个四元组。
	sock=socket(PF_INET, SOCK_STREAM, 0);
	if(sock == -1)
		error_handling("socket() error");
	//结构体serv_addr中初始化IP和端口信息，初始化值为目标服务器端套接字的IP和端口信息（客户端的IP和信息在调用connect时自动创建）
	memset(&serv_addr, 0, sizeof(serv_addr));
	serv_addr.sin_family=AF_INET;
	serv_addr.sin_addr.s_addr=inet_addr(argv[1]);
	serv_addr.sin_port=htons(atoi(argv[2]));
		
   //TCP三次握手，建立连接。
	if(connect(sock, (struct sockaddr*)&serv_addr, sizeof(serv_addr))==-1) 
		error_handling("connect() error!");
	//完成连接后接受服务器端传输的数据
	str_len=read(sock, message, sizeof(message)-1);
	if(str_len==-1)
		error_handling("read() error!");
	
	printf("Message from server: %s \n", message);  
	close(sock);
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```

![image-20211003183854633](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211003183854633.png)



### socket选项

为满足应用层需求，系统对TCP/IP层进行细节屏蔽和抽象，Socket层就相当于TCP/IP和应用层之间的中间层。

常用的socket/bind/accept/connect就是抽象出来的接口，使用它们可以快速进行网络程序开发，可见Socket中间层的重要性。

Socket选项就是为满足用户的定制化需求而生的。我们经常遇到的情况包括地址复用、端口复用、读写超时时间、读写缓冲区大小等。

在Linux的TCP/IP协议栈中包括很多Socket选项，它们会出现在TCP层、IP层、Socket层等，为此在读取和设置socket选项时需要指定level。

如图可以看到Socket层作为中间层以及各层支持的部分Socket选项:

![img](http://api.fly63.com/vue_blog/public/Uploads/20200101/5e0c137b227d0.jpg)



```c++
int getsockopt(int sock, int level, int optname, void *optval, socklen_t *optlen);  //获取socket文件描述符属性，成功时返回0，失败返回-1

int setsockopt(int sock, int level, int optname, const void *optval, socklen_t optlen);   //设置socket文件描述符属性，成功时返回0，失败返回-1

参数：  
sock：将要被设置或者获取选项的套接字。
level：选项所在的协议层。
    1)SOL_SOCKET:通用套接字选项.
	2)IPPROTO_IP:IP选项.
	3)IPPROTO_TCP:TCP选项.　
optname：需要访问的选项名。
optval：对于getsockopt()，指向返回选项值的缓冲。对于setsockopt()，指向包含新选项值的缓冲。
optlen：对于getsockopt()，作为入口参数时，选项值的最大长度。作为出口参数时，选项值的实际长度。对于setsockopt()，现选项的长度。


返回说明：  


成功执行时，返回0。失败返回-1，errno被设为以下的某个值  
EBADF：sock不是有效的文件描述词
EFAULT：optval指向的内存并非有效的进程空间
EINVAL：在调用setsockopt()时，optlen无效
ENOPROTOOPT：指定的协议层不能识别选项
ENOTSOCK：sock描述的不是套接字


```

========================================================================
　　　　　　　　　　　　SOL_SOCKET
\------------------------------------------------------------------------
SO_BROADCAST　　　　　　允许发送广播数据　　　　　　　　　　　　int
SO_DEBUG　　　　　　　　 允许调试　　　　　　　　　　　　　　　　int
SO_DONTROUTE　　　　　　不查找**路由**　　　　　　　　　　　　　　　int
SO_ERROR　　　　　　　　 获得套接字错误　　　　　　　　　　　　　int
SO_KEEPALIVE　　　　　　保持连接　　　　　　　　　　　　　　　　int
SO_LINGER　　　　　　　 延迟关闭连接　　　　　　　　　　　　　　struct linger
SO_OOBINLINE　　　　　　带外数据放入正常数据流　　　　　　　　　int
SO_RCVBUF　　　　　　　 接收缓冲区大小　　　　　　　　　　　　　int
SO_SNDBUF　　　　　　　 发送缓冲区大小　　　　　　　　　　　　　int
SO_RCVLOWAT　　　　　　 接收缓冲区下限　　　　　　　　　　　　　int
SO_SNDLOWAT　　　　　　 发送缓冲区下限　　　　　　　　　　　　　int
SO_RCVTIMEO　　　　　　 接收超时　　　　　　　　　　　　　　　　struct timeval
SO_SNDTIMEO　　　　　　 发送超时　　　　　　　　　　　　　　　　struct timeval
SO_REUSERADDR　　　　　 允许重用本地地址和端口　　　　　　　　　int
SO_TYPE　　　　　　　　 获得套接字类型　　　　　　　　　　　　　　　int

SO_BSDCOMPAT　　　　　　与BSD系统兼容　　　　　　　　　　　　　 int

　　　　　　　　　　　　IPPROTO_IP
\------------------------------------------------------------------------
IP_HDRINCL　　　　　　　在数据包中包含IP首部　　　　　　　　　　int
IP_OPTINOS　　　　　　　IP首部选项　　　　　　　　　　　　　　　int
IP_TOS　　　　　　　　　服务类型

IP_TTL　　　　　　　　　生存时间　　　　　　　　　　　　　　　　int

　　　　　　　　　　　　IPPRO_TCP
\------------------------------------------------------------------------
TCP_MAXSEG　　　　　　　TCP最大数据段的大小　　　　　　　　　　 int

TCP_NODELAY　　　　　　 不使用Nagle算法　　　　　　　　　　　　 int

SO_RCVBUF和SO_SNDBUF每个套接口都有一个发送缓冲区和一个接收缓冲区，使用这两个套接口选项可以改变缺省缓冲区大小。

//接收缓冲区
int nRecvBuf=32*1024;     //设置为32K
setsockopt(s,SOL_SOCKET,SO_RCVBUF,(const char*)&nRecvBuf,sizeof(int));


//发送缓冲区
int nSendBuf=32*1024;//设置为32K
setsockopt(s,SOL_SOCKET,SO_SNDBUF,(const char*)&nSendBuf,sizeof(int));

注意：

​    当设置TCP套接口接收缓冲区的大小时，函数调用顺序是很重要的，因为TCP的窗口规模选项是在建立连接时用SYN与对方互换得到的。对于客户，O_RCVBUF选项必须在connect之前设置；对于服务器，SO_RCVBUF选项必须在listen前设置。

#### socket定时器

网络程序经常要定期检测一个连接的活动状态。所以服务器程序通常管理许多定时事件，有效地组织这些定时事件，使之在预期的时间触发且不影响服务器，这对服务器的性能有重要的影响

![image-20211113212752787](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211113212752787.png)



```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdlib.h>
#include <assert.h>
#include <stdio.h>
#include <errno.h>
#include <fcntl.h>
#include <unistd.h>
#include <string.h>

int timeout_connect( const char* ip, int port, int time ) 
{
    int ret = 0;
    struct sockaddr_in address;
    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );

    int sockfd = socket( PF_INET, SOCK_STREAM, 0 );
    assert( sockfd >= 0 );

    struct timeval timeout;
    timeout.tv_sec = time;
    timeout.tv_usec = 0;
    socklen_t len = sizeof( timeout );
    ret = setsockopt( sockfd, SOL_SOCKET, SO_SNDTIMEO, &timeout, len );
    assert( ret != -1 );

    ret = connect( sockfd, ( struct sockaddr* )&address, sizeof( address ) );
    if ( ret == -1 )
    {
        if( errno == EINPROGRESS )  //超时对应的errno是EINPROGRESS。下面处理超时任务
        {
            printf( "connecting timeout\n" );
            return -1;
        }
        printf( "error occur when connecting to server\n" );
        return -1;
    }

    return sockfd;
}

int main( int argc, char* argv[] )
{
    if( argc <= 2 )
    {
        printf( "usage: %s ip_address port_number\n", basename( argv[0] ) );
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi( argv[2] );

    int sockfd = timeout_connect( ip, port, 10 );
    if ( sockfd < 0 )
    {
        return 1;
    }
    return 0;
}
```



**利用alarm和setitimer设置的定时闹钟来实现定时器**



**升序定时链表处理非活动连接**

```c++
#ifndef LST_TIMER
#define LST_TIMER

#include <time.h>

#define BUFFER_SIZE 64
class util_timer;
struct client_data //用户数据结构
{
    sockaddr_in address;
    int sockfd;
    char buf[ BUFFER_SIZE ];
    util_timer* timer; //定时器
};

class util_timer //定时器类
{
public:
    util_timer() : prev( NULL ), next( NULL ){}

public:
   time_t expire;   //超时时间
   void (*cb_func)( client_data* ); //回调函数
   //回调函数处理的用户数据，由定时器的执行者传递给回调函数
   client_data* user_data;
   util_timer* prev; //指向前一个定时器
   util_timer* next; //指向下一个定时器
};

class sort_timer_lst  //定时器链表。升序的双向链表
{
public:
    sort_timer_lst() : head( NULL ), tail( NULL ) {}
    ~sort_timer_lst()  //析构函数，链表销毁时删除所有定时器
    {
        util_timer* tmp = head;
        while( tmp )
        {
            head = tmp->next;
            delete tmp;
            tmp = head;
        }
    }
    void add_timer( util_timer* timer )  //将目标定时器添加到链表
    {
        if( !timer )
        {
            return;
        }
        if( !head )
        {
            head = tail = timer;
            return; 
        }
        if( timer->expire < head->expire ) //如果目标定时时间最小，插入头部
        {
            timer->next = head;
            head->prev = timer;
            head = timer;
            return;
        }
        add_timer( timer, head );
    }
    void adjust_timer( util_timer* timer )  //当定时任务发送变化，调整对应定时的位置
    {
        if( !timer )
        {
            return;
        }
        util_timer* tmp = timer->next;
        if( !tmp || ( timer->expire < tmp->expire ) )
        {
            return;
        }
        if( timer == head )
        {
            head = head->next;
            head->prev = NULL;
            timer->next = NULL;
            add_timer( timer, head );
        }
        else  //如果非头部且比后面长
        {
            timer->prev->next = timer->next;
            timer->next->prev = timer->prev;
            add_timer( timer, timer->next );
        }
    }
    void del_timer( util_timer* timer )  //从链表中删除定时器
    {
        if( !timer )
        {
            return;
        }
        if( ( timer == head ) && ( timer == tail ) )
        {
            delete timer;
            head = NULL;
            tail = NULL;
            return;
        }
        if( timer == head )
        {
            head = head->next;
            head->prev = NULL;
            delete timer;
            return;
        }
        if( timer == tail )
        {
            tail = tail->prev;
            tail->next = NULL;
            delete timer;
            return;
        }
        timer->prev->next = timer->next;
        timer->next->prev = timer->prev;
        delete timer;
    }
    void tick()  //SIGALRM信号每次被触发就在其信号处理函数执行一次tick
    {
        if( !head )
        {
            return;
        }
        printf( "timer tick\n" );
        time_t cur = time( NULL );
        util_timer* tmp = head;
        while( tmp )
        {
            if( cur < tmp->expire )  //判断定时器是否到期（超时）
            {
                break;
            }
            tmp->cb_func( tmp->user_data );   //到期调用定时器的回调函数
            head = tmp->next;
            if( head )
            {
                head->prev = NULL;
            }
            delete tmp;  //删除定时器
            tmp = head;
        }
    }

private:
    void add_timer( util_timer* timer, util_timer* lst_head )  //将定时器添加到节点lst_head之后的部分链表中
    {
        util_timer* prev = lst_head;
        util_timer* tmp = prev->next;
        while( tmp )
        {
            if( timer->expire < tmp->expire )
            {
                prev->next = timer;
                timer->next = tmp;
                tmp->prev = timer;
                timer->prev = prev;
                break;
            }
            prev = tmp;
            tmp = tmp->next;
        }
        if( !tmp )  //如果该定时器最长，插入尾部
        {
            prev->next = timer;
            timer->prev = prev;
            timer->next = NULL;
            tail = timer;
        }
        
    }

private:
    util_timer* head;
    util_timer* tail;
};

#endif
```



## 域名和网络地址

从浏览器到服务器

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211005211713178.png" alt="image-20211005211713178" style="zoom:70%;" />

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211005211621213.png" alt="image-20211005211621213" style="zoom: 80%;" />

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211005211645729.png" alt="image-20211005211645729" style="zoom:70%;" />

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211005211651752.png" alt="image-20211005211651752" style="zoom:70%;" />

DNS（Domain Name System）：当浏览器访问一个网站时，首先要通过DNS服务器把网站域名转换成IP地址，才能通过这个IP地址访问网站服务器。

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211005212239573.png" alt="image-20211005212239573" style="zoom:70%;" />

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211005212310034.png" alt="image-20211005212310034" style="zoom:70%;" />

当计算机内置的默认DNS服务器没有记载所要访问的域名的IP地址，则该DNS会访问其他DNS服务器。

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211005213043731.png" alt="image-20211005213043731" style="zoom: 60%;" />

#### 调用gethostbyname函数利用域名来获取IP地址

```c++
#include<netdb.h>
struct hostent *gethostbyname(const char *hostname);

struct hostent{
    char *h_name;  //official name
    char **h_aliases;  //alias list
    int  h_addrtype;  //host address type
    int  h_length;  //address lenght
    char **h_addr_list;  //address list
}
h_name：官方域名（Official domain name）。官方域名代表某一主页，但实际上一些著名公司的域名并未用官方域名注册。
h_aliases：别名，可以通过多个域名访问同一主机。同一IP地址可以绑定多个域名，因此除了当前域名还可以指定其他域名。
h_addrtype：gethostbyname() 不仅支持 IPv4，还支持 IPv6，可以通过此成员获取IP地址的地址族（地址类型）信息，IPv4 对应 AF_INET，IPv6 对应 AF_INET6。
h_length：保存IP地址长度。IPv4 的长度为4个字节，IPv6 的长度为16个字节。
h_addr_list：这是最重要的成员。通过该成员以整数形式保存域名对应的IP地址。对于用户较多的服务器，可能会分配多个IP地址给同一域名，利用多个服务器进行均衡负载。
```

<img src="http://c.biancheng.net/cpp/uploads/allimg/151110/1-151110203SY49.jpg" alt="img" style="zoom: 67%;" />



```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <netdb.h>
void error_handling(char *message);

int main(int argc, char *argv[])
{
	int i;
	struct hostent *host;
	if(argc!=2) {
		printf("Usage : %s <addr>\n", argv[0]);
		exit(1);
	}

	host=gethostbyname(argv[1]);
	if(!host)
		error_handling("gethost... error");
    //输出官方域名
	printf("Official name: %s \n", host->h_name);
	//输出除官方域名以外的域名
	for(i=0; host->h_aliases[i]; i++)
		printf("Aliases %d: %s \n", i+1, host->h_aliases[i]);
    
	printf("Address type: %s \n",
		(host->h_addrtype==AF_INET)?"AF_INET":"AF_INET6");
    //输出IP信息
	for(i=0; host->h_addr_list[i]; i++)
		printf("IP addr %d: %s \n", i+1,inet_ntoa(*(struct in_addr*)host->h_addr_list[i]));
	//转成in_addr是因为host->h_addr_list指向的字符串指针数组是由in_addr结构体组成的
    return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

root@my_linux:/tcpip# gcc gethostbyname.c -0 hostname 
root@my_linux:/tcpip# ./hostname www.naver.com 
Official name: www.g.naver.com 
Aliases 1: www.naver.com 
Address type: AF NET
IP addr 1: 202.131.29.70 
IP addr 2: 222.122.195.6
```



#### 调用gethostbyaddr函数利用IP地址获取域名

```c++
#include<netdb.h>
struct hostent *gethostbyaddr(const char*addr,socklen_t len,int family)
addr:IP地址信息
len:向第一个参数传递的地址信息的字节数（IPV4为4，IPV6为16）
family:传递地址族信息（IPV4为AF_INET，IPV6为AF_INET6
```



```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <netdb.h>
void error_handling(char *message);

int main(int argc, char *argv[])
{
	int i;
	struct hostent *host;
	struct sockaddr_in addr;
	if(argc!=2) {
		printf("Usage : %s <IP>\n", argv[0]);
		exit(1);
	}

	memset(&addr, 0, sizeof(addr));
	addr.sin_addr.s_addr=inet_addr(argv[1]);
	host=gethostbyaddr((char*)&addr.sin_addr, 4, AF_INET);
	if(!host)
		error_handling("gethost... error");

	printf("Official name: %s \n", host->h_name);

	for(i=0; host->h_aliases[i]; i++)
		printf("Aliases %d: %s \n", i+1, host->h_aliases[i]);
	
	printf("Address type: %s \n", 
		(host->h_addrtype==AF_INET)?"AF_INET":"AF_INET6");

	for(i=0; host->h_addr_list[i]; i++)
		printf("IP addr %d: %s \n", i+1,
					inet_ntoa(*(struct in_addr*)host->h_addr_list[i]));	
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}
root@my_linux:/tcpip# gcc gethostbyaddr.c -0 hostaddr 
root@my_linux:/tcpip# ./hostaddr 74.125.19.106 
Official name: nuq04s01-in-f106.goog1e.com 
Address type; AF_INET 
IP addr 1: 74.125.19.106
```



## TCP网络编程（Transmission Control Protocol）

**面向连接的编程，所以TCP又称基于流（stream）的套接字**

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211003164521535.png" alt="image-20211003164521535" style="zoom: 67%;" />

​                                                                                                          （tcp/ip协议栈）

**TCP即传输控制协议**，是一种面向连接的、可靠的、基于字节流的通信协议。简单来说**TCP就是有确认机制的UDP协议，**每发出一个数据包都要求确认，如果有一个数据包丢失，就收不到确认，发送方就必须重发这个数据包。

为了保证传输的可靠性，TCP 协议在 UDP 基础之上建立了**三次对话**的确认机制，也就是说，**在正式收发数据前**，必须和对方建立可靠的连接。由于建立过程较为复杂，我们在这里做一个形象的描述：

```c++
主机A：我想发数据给你，可以么？

主机B：可以，你什么时候发？

主机A：我马上发，你接着！
```

- 第一次：客户端向服务端发送一个 SYN（SEQ=x 客户端序号）报文给服务器端，进入SYN_SEND状态。

  第二次：服务器端收到SYN报文，回应一个SYN （SEQ=y 服务端序号）ACK(ACK=x+1 确认号=客户端序号+1）报文，进入[SYN_RECV](https://baike.baidu.com/item/SYN_RECV)状态。

  第三次：客户端收到服务器端的SYN报文，回应一个ACK(ACK=y+1）报文，进入Established状态。

- ![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193252937.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21heWlmYW5fYmxvZw==,size_16,color_FFFFFF,t_70)

**思考：为什么要进行三次握手，而不是两次呢？ 比如在第一次握手之后，服务器进入准备状态，然后发送消息给客户端，客户端也进入准备状态，这就完成了双方的确认了。**

- 回答：两次握手时，服务器提前进入准备状态之后，如果中途遇到网络中断，消息并没有传回给客户端，客户端将永远接不到服务器的给入状态，那么服务端将资源浪费在一个不存在的连接之上了。

思考2：**三次握手就很安全了吗？**

- 回答：在三次握手过程中，Server发送SYN-ACK之后，收到Client的ACK之前的TCP连接称为半连接（half-open connect），此时Server处于SYN_RCVD状态，当收到ACK后，Server转入ESTABLISHED状态。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。SYN攻击时一种典型的DDOS攻击，检测SYN攻击的方式非常简单，即当Server上有大量半连接状态且源IP地址是随机的，则可以断定遭到SYN攻击了，使用如下命令可以让之现行：



```bash
#netstat -nap | grep SYN_RECV
```



第一次：客户端向服务端发送 FIN + ACK 报文，同时携带序号为 X。 客户端进入 FIN-WAIT1

第二次：服务器端回复 ACK 报文。附带序号Z和确认序号X+1，表示服务器已经接受到了客服端的报文。但是由于服务器可能还在处理事务，因此，报文并不会携带FIN标志。状态：CLOSE WAIT

第三次：在一段时间之后，服务器已经处理完毕，发送带有 FIN和ACK的报文，序号为Y，确认序号为 X + 1  。 状态： ACK-LAST

第四次：客户端发送ACK报文，序号为 X+1，确认号Y+1 。 客户端进入： TIME_WAIT。服务端进图CLOSE（初始状态）

在这个过程我们可以发现，三次握手使得双方可以可靠地实现对话，但这如果在小信息量的通信中，似乎效率有些低。



![在这里插入图片描述](https://img-blog.csdnimg.cn/20181217193350959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21heWlmYW5fYmxvZw==,size_16,color_FFFFFF,t_70)

经过三次对话之后，主机A才会向主机B发送正式数据，而UDP是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发过去了。所以 TCP 能够保证数据包在传输过程中不被丢失，但美好的事物必然是要付出代价的，相比 UDP，TCP 实现过程复杂，消耗连接资源多，传输速度慢。

思考：**为什么建立连接是三次握手，而关闭连接却是四次挥手呢？**

- 回答：这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。

思考：**为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？**

- 原因有二：
   一、保证TCP协议的全双工连接能够可靠关闭
   二、保证这次连接的重复数据段从网络中消失



TCP 数据包和 UDP 一样，都是由首部和数据两部分组成，唯一不同的是，TCP 数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常 TCP 数据包的长度不会超过IP数据包的长度，以确保单个 TCP 数据包不必再分割。

**TCP和UDP分别是面向连接和非面向连接的。TCP的socket编程，收发两端（客户端和服务器端）都要有一一成对的socket，因此，发送端为了将多个发往接收端的包，更有效的发到对方，使用了优化方法（Nagle算法），将多次间隔较小且数据量小的数据，合并成一个大的数据块，然后进行封包。但是这样，接收端，就难于分辨出来了，必须提供科学的拆包机制。**

**TCP传输的数据没有数据边界（保护消息边界）！**

保护消息边界，就是指传输协议把数据当作一条独立的消息在网上
传输,接收端只能接收独立的消息.也就是说存在保护消息边界,接收
端一次只能接收发送端发出的一个数据包.
    而**面向流**则是指**无保护消息保护边界**的,如果**发送端连续发送数据,**
**接收端有可能在一次接收动作中,会接收两个或者更多的数据包.**

​    我们举个例子来说,例如,我们连续发送三个数据包,大小分别是2k,
4k , 8k,这三个数据包,都已经到达了接收端的网络堆栈中,使用TCP协议,我们
只要把接收的缓冲区大小设置在14k以上,我们就能够一次把所有的

数据包接收下来.只需要有一次接收动作.

  因为TCP采用流传输,是把数据当作一串数据流,他不认为数据是一个一个的消息.

   所以有很多人在使用tcp协议通讯的时候,并不清楚tcp是基于流的
传输,当**连续发送数据**的时候,他们时常**会认为tcp会丢包**.其实不然,
**因为当他们使用的缓冲区足够大时,他们有可能会一次接收到两个甚**
**至更多的数据包,而很多人往往会忽视这一点,只解析检查了第一个**
**数据包,而已经接收的其他数据包却被忽略了**.所以大家如果要作这
类的网络编程的时候,必须要注意这一点.

**结论：
**   根据以上所说，可以这样理解，TCP为了保证可靠传输，尽量减少额外
开销（每次发包都要验证），因此采用了流式传输，面向流的传输，
相对于面向消息的传输，可以减少发送包的数量。从而减少了额外开
销。但是，对于数据传输频繁的程序来讲，使用TCP可能会容易粘包。
当然，对接收端的程序来讲，如果机器负荷很重，也会在接收缓冲里
粘包。这样，就需要接收端额外拆包，增加了工作量。因此，这个特
别适合的是数据要求可靠传输，但是不需要太频繁传输的场合（
两次操作间隔100ms，具体是由TCP等待发送间隔决定的，取决于内核
中的socket的写法

**TCP无保护消息边界的解决
 针对这个问题，一般有3种解决方案：**

   **(1)发送固定长度的消息**

   **(2)把消息的尺寸与消息一块发送**

   **(3)使用特殊标记来区分消息间隔**



### 实现单服务的迭代服务器端/客户端（同一时刻只能服务一个客户端）

调用accept函数后，调用I/O相关的read，write函数，然后调用close关闭accept函数调用时创建的套接字。

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211003185635995.png" alt="image-20211003185635995" style="zoom:80%;" />

**服务器端**

功能：

服务器端在同一时刻只能与一个客户端相连，并提供服务

服务器端依次向5个客户端并推出

服务器端将从客户端接收的字符串传回客户端

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#define BUF_SIZE 1024
void error_handling(char *message);
int main(int argc, char *argv[])
{
	int serv_sock, clnt_sock;
	char message[BUF_SIZE];
	int str_len, i;

	struct sockaddr_in serv_adr;
	struct sockaddr_in clnt_adr;
	socklen_t clnt_adr_sz;

	if(argc!=2) {
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}

	serv_sock=socket(PF_INET, SOCK_STREAM, 0);
	if(serv_sock==-1)
		error_handling("socket() error");

	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port=htons(atoi(argv[1]));

	if(bind(serv_sock, (struct sockaddr*)&serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");

	if(listen(serv_sock, 5)==-1)
		error_handling("listen() error");

	clnt_adr_sz=sizeof(clnt_adr);
    //因为我的请求队列是5，所以调用5次accept函数，依次向5个客户端提供服务
	for(i=0; i<5; i++)
	{
		clnt_sock=accept(serv_sock, (struct sockaddr*)&clnt_adr, &clnt_adr_sz);
		if(clnt_sock==-1)
			error_handling("accept() error");
		else
			printf("Connected client %d \n", i+1);

		while((str_len=read(clnt_sock, message, BUF_SIZE))!=0)
			write(clnt_sock, message, str_len);

		close(clnt_sock);
	}
	//向5个客户端提供服务后关闭服务器端套接字并终止程序
	close(serv_sock);
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```

```shell
root@my_linux:/tcpip# gcc echo_server.c -0 eserver 
root@my_linux:/tcpip# ./eserver 9199 
Connected client 1 
Connected client 2 
Connected client 3
```

**客户端**

客户端接收用户输入的字符串并发送到服务器端

客户端输入Q结束服务

**此写法出现的基于TCP传输的bug：**

**Q1：当短时间内多次连续调用write函数，TCP可能会将多条消息打包成一个数据包传输，造成服务器端接收的是多条字符串组成的大字符串，用户输入的字符串则丢失了（相当于丢包）**

**Q2：当传输的数据太大，操作系统可能把数据分成多个数据包传送（write），而可能还未收到全部数据包就接收了（read）**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define BUF_SIZE 1024
void error_handling(char *message);

int main(int argc, char *argv[])
{
	int sock;
	char message[BUF_SIZE];
	int str_len;
	struct sockaddr_in serv_adr;

	if(argc!=3) {
		printf("Usage : %s <IP> <port>\n", argv[0]);
		exit(1);
	}
	
	sock=socket(PF_INET, SOCK_STREAM, 0);   
	if(sock==-1)
		error_handling("socket() error");
	
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=inet_addr(argv[1]);
	serv_adr.sin_port=htons(atoi(argv[2]));
	
	if(connect(sock, (struct sockaddr*)&serv_adr, sizeof(serv_adr))==-1)
		error_handling("connect() error!");
	else
		puts("Connected...........");
	
	while(1) 
	{
		fputs("Input message(Q to quit): ", stdout);
		fgets(message, BUF_SIZE, stdin);
		
		if(!strcmp(message,"q\n") || !strcmp(message,"Q\n"))
			break;

		write(sock, message, strlen(message));
		str_len=read(sock, message, BUF_SIZE-1);
		message[str_len]=0;
		printf("Message from server: %s", message);
	}
	
	close(sock);
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}
```

```shell
root@my_linux:/tcpip# gcc echo_client.c -0 eclient 
root@my_linux:/tcpip# ./eclient 127.0.9.1 9199 
Connected.... 
Input message(Q to quit): Good morning 
Message from server: Good morning 
Input message(Q to quit): Hi 
Message from server: Hi 
Input message(Q to quit): Q 
root@my_linux:/tcpip#
```

**改进：方法2**

**字符串多长就循环调用read读取几个字节**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define BUF_SIZE 1024
void error_handling(char *message);

int main(int argc, char *argv[])
{
	int sock;
	char message[BUF_SIZE];
	int str_len, recv_len, recv_cnt;
	struct sockaddr_in serv_adr;

	if(argc!=3) {
		printf("Usage : %s <IP> <port>\n", argv[0]);
		exit(1);
	}
	
	sock=socket(PF_INET, SOCK_STREAM, 0);   
	if(sock==-1)
		error_handling("socket() error");
	
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=inet_addr(argv[1]);
	serv_adr.sin_port=htons(atoi(argv[2]));
	
	if(connect(sock, (struct sockaddr*)&serv_adr, sizeof(serv_adr))==-1)
		error_handling("connect() error!");
	else
		puts("Connected...........");
	
	while(1) 
	{
		fputs("Input message(Q to quit): ", stdout);
		fgets(message, BUF_SIZE, stdin);
		
		if(!strcmp(message,"q\n") || !strcmp(message,"Q\n"))
			break;

		str_len=write(sock, message, strlen(message));
		
		recv_len=0;
		while(recv_len<str_len)
		{
			recv_cnt=read(sock, &message[recv_len], BUF_SIZE-1);
			if(recv_cnt==-1)
				error_handling("read() error!");
			recv_len+=recv_cnt;
		}
		
		message[recv_len]=0;
		printf("Message from server: %s", message);
	}
	
	close(sock);
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```



### 基于TCP的半关闭

前言：close并不是立即将fd关闭，而是将fd的调用次数减1，直到为0关闭，一次fork（）系统调用默认使父进程打开的sock调用次数+1，所以要父子进程都close才能将fd的值减为1

调用 close()/closesocket() 函数意味着完全断开连接，即不能发送数据也不能接收数据，这种“生硬”的方式可能会造成下面的问题。

![img](http://c.biancheng.net/cpp/uploads/allimg/151020/1-15102016331RI.jpg)

使用只关闭一部分数据交换中使用的流的方法（可以传输数据但不能接收，可以接收数据但不能传输）可以解决这个问题

```c++
#include<sys/socket.h>
int shutdown(int sock, int howto);
howto 在 Linux 下有以下取值：
SHUT_RD：断开输入流。套接字无法接收数据（即使输入缓冲区收到数据也被抹去），无法调用输入相关函数。
SHUT_WR：断开输出流。套接字无法发送数据，但如果输出缓冲区中还有未传输的数据，则将传递到目标主机。
SHUT_RDWR：同时断开 I/O 流。相当于分两次调用 shutdown()，其中一次以 SHUT_RD 为参数，另一次以 SHUT_WR 为参数。
```

shutdown() 用来关闭连接，而不是套接字，不管调用多少次 shutdown()，套接字依然存在，直到调用 close() / closesocket() 将套接字从内存清除。

调用 close()/closesocket() 关闭套接字时，或调用 shutdown() 关闭输出流时，都会向对方发送 FIN 包。FIN 包表示数据传输完毕，计算机收到 FIN 包就知道不会再有数据传送过来了。

**默认情况下，close()/closesocket() 会立即向网络中发送FIN包，不管输出缓冲区中是否还有数据，而shutdown() 会等输出缓冲区中的数据传输完毕再发送FIN包。也就意味着，调用 close()/closesocket() 将丢失输出缓冲区中的数据，而调用 shutdown() 不会。**



## UDP网络编程（User Datagram Protocol）

UDP 的全称是 `用户数据报协议(UDP，User Datagram Protocol)`，UDP 为应用程序提供了一种`无需建立连接`就可以发送封装的 IP 数据包的方法。如果应用程序开发人员选择的是 UDP 而不是 TCP 的话，那么该应用程序相当于就是和 IP 直接打交道的。

从应用程序传递过来的数据，会附加上多路复用/多路分解的源和目的端口号字段，以及其他字段，然后将形成的报文传递给网络层，网络层将运输层报文段封装到 IP 数据报中，然后尽力而为的交付给目标主机。最关键的一点就是，使用 UDP 协议在将数据报传递给目标主机时，发送方和接收方的运输层实体间是没有`握手`的。正因为如此，UDP 被称为是`无连接`的协议。

 对于UDP，不会使用块的合并优化算法，这样，实际上目前认为，是由于UDP支持的是一对多的模式，所以接收端的skbuff(套接字缓冲区）采用了链式结构来记录每一个到达的UDP包，在每个UDP包中就有了消息头（消息来源地址，端口等信息），这样，对于接收端来说，就容易进行区分处理了

### UDP 特点

UDP 协议一般作为流媒体应用、语音交流、视频会议所使用的传输层协议，我们大家都知道的 DNS 协议底层也使用了 UDP 协议，这些应用或协议之所以选择 UDP 主要是因为以下这几点

- **`速度快`**，采用 UDP 协议时，只要应用进程将数据传给 UDP，UDP 就会将此数据打包进 UDP 报文段并立刻传递给网络层，然后 TCP 有拥塞控制的功能，它会在发送前判断互联网的拥堵情况，如果互联网极度阻塞，那么就会抑制 TCP 的发送方。使用 UDP 的目的就是希望实时性。
- **`无须建立连接`**，TCP 在数据传输之前需要经过三次握手的操作，而 UDP 则无须任何准备即可进行数据传输。因此 UDP 没有建立连接的时延。如果使用 TCP 和 UDP 来比喻开发人员：TCP 就是那种凡事都要设计好，没设计不会进行开发的工程师，需要把一切因素考虑在内后再开干！所以非常`靠谱`；而 UDP 就是那种上来直接干干干，接到项目需求马上就开干，也不管设计，也不管技术选型，就是干，这种开发人员非常`不靠谱`，但是适合快速迭代开发，因为可以马上上手！
- **`无连接状态`**，TCP 需要在端系统中维护**连接状态**，连接状态包括接收和发送缓存、拥塞控制参数以及序号和确认号的参数，在 UDP 中没有这些参数，也没有发送缓存和接受缓存。因此，某些专门用于某种特定应用的服务器当应用程序运行在 UDP 上，一般能支持更多的活跃用户
- **`分组首部开销小`**，每个 TCP 报文段都有 20 字节的首部开销，而 UDP 仅仅只有 8 字节的开销。

> 这里需要注意一点，并不是所有使用 UDP 协议的应用层都是`不可靠`的，应用程序可以自己实现可靠的数据传输，通过增加确认和重传机制。所以使用 UDP 协议最大的特点就是速度快。

UDP是面向消息传输的，所以会保护消息边界：就是指传输协议把数据当作一条独立的消息在网上传输,接收端只能接收独立的消息.也就是说存在保护消息边界,接收端一次只能接收发送端发出的一个数据包. 

举个例子来说,例如,我们连续发送三个数据包,大小分别是2k, 4k , 8k,这三个数据包,都已经到达了接收端的网络堆栈中,如果使 用**UDP**协议,不管我们使用多大的接收缓冲区去接收数据,我们必须有 三次接收动作,才能够把所有的数据包接收完.

**注意**

UDP由于面向的是消息传输，它把所有接收到的消息都挂接到缓冲区的接受队列中，因此，它对于数据的提取分离就更加方便，但是，
它没有粘包机制，因此，当发送数据量较小的时候，就会发生数据包有效载荷较小的情况，也会增加多次发送的系统发送开销（系统调用，写硬件等）和接收开销。因此，应该最好设置一个比较合适的数据包的包长，来进行UDP数据的发送。（UDP最大载荷为1472，因此最好能每次传输接近这个数的数据量，这特别适合于视频，音频等大块数据的发送，同时，通过减少握手来保证流媒体的实时性）



#### 调用sento函数发送数据



```c++
#includ<sys/socket.h>
ssize_t sendto(int sock, void *buf, size_t nbytes, int flags, struct sockaddr *to, socklen_t addrlen); 
sock：用于传输UDP数据的套接字；
buf：保存待传输数据的缓冲区地址；
nbytes：带传输数据的长度（以字节计）；
flags：可选项参数，若没有可传递0；
to：存有目标地址信息的 sockaddr 结构体变量的地址；
addrlen：传递给参数 to 的地址值结构体变量的长度。
```

UDP 发送函数 sendto() 与TCP发送函数 write()/send() 的最大区别在于，sendto() 函数需要向他传递目标地址信息。因为创建好TCP套接字后，TCP套接字将保持与对方套接字的连接（TCP套接字知道目标地址信息），而UDP套接字不会保持连接状态（UDP套接字只有简单的邮筒功能），因此每次传输数据都要添加目标地址信息。



#### 调用recvfrom函数接收数据



```c++
#includ<sys/socket.h>
int recvfrom(SOCKET sock, char *buf, int nbytes, int flags, const struct sockaddr *from, int *addrlen);
sock：用于接收UDP数据的套接字；
buf：保存接收数据的缓冲区地址；
nbytes：可接收的最大字节数（不能超过buf缓冲区的大小）；
flags：可选项参数，若没有可传递0；
from：存有发送端地址信息的sockaddr结构体变量的地址；
addrlen：保存参数 from 的结构体变量长度的变量地址值。
```



**基于UDP的回声服务器端**



```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define BUF_SIZE 30
void error_handling(char *message);

int main(int argc, char *argv[])
{
	int serv_sock;
	char message[BUF_SIZE];  //缓冲区
	int str_len;
	socklen_t clnt_adr_sz;
	
	struct sockaddr_in serv_adr, clnt_adr;
	if(argc!=2){
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}
	
    //创建UDP套接字
	serv_sock=socket(PF_INET, SOCK_DGRAM, 0);
	if(serv_sock==-1)
		error_handling("UDP socket creation error");
	
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port=htons(atoi(argv[1]));
	
	if(bind(serv_sock, (struct sockaddr*)&serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");

	while(1) 
	{
		clnt_adr_sz=sizeof(clnt_adr);   //客户端信息
		str_len=recvfrom(serv_sock, message, BUF_SIZE, 0, (struct sockaddr*)&clnt_adr, &clnt_adr_sz);
		sendto(serv_sock, message, str_len, 0, (struct sockaddr*)&clnt_adr, clnt_adr_sz);
	}	
	close(serv_sock);
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```



**基于UDP的回声客户端**



```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define BUF_SIZE 30
void error_handling(char *message);

int main(int argc, char *argv[])
{
	int sock;
	char message[BUF_SIZE];
	int str_len;
	socklen_t adr_sz;
	
	struct sockaddr_in serv_adr, from_adr;
	if(argc!=3){
		printf("Usage : %s <IP> <port>\n", argv[0]);
		exit(1);
	}
	
	sock=socket(PF_INET, SOCK_DGRAM, 0);   
	if(sock==-1)
		error_handling("socket() error");
	
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=inet_addr(argv[1]);
	serv_adr.sin_port=htons(atoi(argv[2]));
	
	while(1)
	{
		fputs("Insert message(q to quit): ", stdout);
		fgets(message, sizeof(message), stdin);     
		if(!strcmp(message,"q\n") || !strcmp(message,"Q\n"))	
			break;
		sendto(sock, message, strlen(message), 0, (struct sockaddr*)&serv_adr, sizeof(serv_adr));
		adr_sz=sizeof(from_adr);
		str_len=recvfrom(sock, message, BUF_SIZE, 0, (struct sockaddr*)&from_adr, &adr_sz);

		message[str_len]=0;
		printf("Message from server: %s", message);
	}	
	close(sock);
	return 0;
}

void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}
```



## 对象序列化

假设如下场景：

客户端中有一个class player表示玩家，服务器需要实时根据player的状态对客户端做出相应反应。

```c++
class player
{
    public :
    player():{}
    private:
    int ...
        ...
}
void sendplayer(int socket,const player*sentplayer){
    send(socket,reinterpret_cast<const char*>(sentplayer),sizeof(player),0); //通过sent将player传递给客户端，需要将player转换成char*才能作为send输入。
}
```



```c++
void recvplayer(int clntsocket){
    player pla;	//创造or从对象池取一个对象
    recv(clntsocket,reinterpret_cast<const char*>(pla),sizeof(player),0);
}
```

欧克了，并且运行起来毫无问题。因为这个类只有少量的基本数据类型。

但是在实际应用中很少有这么简单的class。

例：

```c++
class player
{
    public :
    player():{}
    virtual void change();
    private:
    int ...
        ...
    char *name_;
    aObject* home_;
    std::vector<int> id_;
}
```

现在这个类有一个虚函数，所以在客户端上维护了一个虚函数表，还有一些指针指向不同的对象以及一个stl---vector。那么再简单的用write/read或sent/recv，服务器端接收到的指针还会有效吗？当然不！因为指针指向的服务器端的地址上有什么谁也不知道！

问题还不只这些，传输vector是安全的吗？如果vector内部也有许多指针指向内部的元素呢？事实上，任何黑匣子数据结构都是不安全的。

如何将客户端上复杂的类，结构，完整的传送到服务器端，这就是序列化的意义。

----------将内存中的随机访问格式转换成特定的比特流格式。

### 序列化

通过上面的例子，我们现在至少知道，简单的用一大块数据块来序列化整个对象是不安全的，而是应该对每个字段单独序列化。收集所有的相关数据到一个缓冲区，然后发送该缓冲区作为对象的代表。并且，对于非基本的数据结构，应该定义对应的序列化函数。

#### 使用流来发送和接收数据

输出流发送数据

```c++
#include <cstdlib>
#include <cstdint>
#include <type_traits>

#define STREAM_ENDIANNESS 0
#define PLATFORM_ENDIANNESS 0

class GameObject;
class LinkingContext;

class OutputMemoryStream
{
public:
	OutputMemoryStream() :
	mLinkingContext( nullptr )
	{ ReallocBuffer( 32 ); }
	
	~OutputMemoryStream()	{ std::free( mBuffer ); }
	
	//get a pointer to the data in the stream
	const 	char*		GetBufferPtr()	const	{ return mBuffer; }
			uint32_t	GetLength()		const	{ return mHead; }
	
			void		Write( const void* inData,
								size_t inByteCount );
	
	template< typename T > void Write( T inData )
	{
		static_assert( std::is_arithmetic< T >::value ||
					  std::is_enum< T >::value,
					  "Generic Write only supports primitive data types" );
		
		if( STREAM_ENDIANNESS == PLATFORM_ENDIANNESS )
		{
			Write( &inData, sizeof( inData ) );
		}
		else
		{
			T swappedData = ByteSwap( inData );
			Write( &swappedData, sizeof( swappedData ) );
		}
		
	}
	
	void Write( const std::vector< int >& inIntVector )
	{
		size_t elementCount = inIntVector.size();
		Write( elementCount );
		Write( inIntVector.data(), elementCount * sizeof( int ) );
	}
	
	template< typename T >
	void Write( const std::vector< T >& inVector )
	{
		uint32_t elementCount = inVector.size();
		Write( elementCount );
		for( const T& element : inVector )
		{
			Write( element );
		}
	}
	
	void Write( const std::string& inString )
	{
		size_t elementCount = inString.size() ;
		Write( elementCount );
		Write( inString.data(), elementCount * sizeof( char ) );
	}
	
	void Write( const GameObject* inGameObject )
	{
		uint32_t networkId = mLinkingContext->GetNetworkId( const_cast< GameObject* >( inGameObject ), false );
		Write( networkId );
	}
	
	
private:
			void		ReallocBuffer( uint32_t inNewLength );
	
	char*		mBuffer;
	uint32_t	mHead;
	uint32_t	mCapacity;
	
	LinkingContext* mLinkingContext;
};

void OutputMemoryStream::ReallocBuffer( uint32_t inNewLength )
{
	mBuffer = static_cast< char* >( std::realloc( mBuffer, inNewLength ) );
	//handle realloc failure
	//...
	mCapacity = inNewLength;
}

void OutputMemoryStream::Write( const void* inData,
								size_t inByteCount )
{
	//make sure we have space...
	uint32_t resultHead = mHead + static_cast< uint32_t >( inByteCount );
	if( resultHead > mCapacity )
	{
		ReallocBuffer( std::max( mCapacity * 2, resultHead ) );
	}
	
	//copy into buffer at head
	std::memcpy( mBuffer + mHead, inData, inByteCount );
	
	//increment head for next write
	mHead = resultHead;
}


```

输入流接收数据

```c++
class InputMemoryStream
{
public:
	InputMemoryStream( char* inBuffer, uint32_t inByteCount ) :
	mBuffer( inBuffer ), mCapacity( inByteCount ), mHead( 0 ),
	mLinkingContext( nullptr ) {}

	~InputMemoryStream()	{ std::free( mBuffer ); }
		
	uint32_t		GetRemainingDataSize() const
					{ return mCapacity - mHead; }
	
	void		Read( void* outData, uint32_t inByteCount );


	template< typename T > void Read( T& outData )
	{
		static_assert( std::is_arithmetic< T >::value ||
					   std::is_enum< T >::value,
					   "Generic Read only supports primitive data types" );
		Read( &outData, sizeof( outData ) );
	}
	
	template< typename T >
	void Read( std::vector< T >& outVector )
	{
		size_t elementCount;
		Read( elementCount );
		outVector.resize( elementCount );
		for( const T& element : outVector )
		{
			Read( element );
		}
	}
	
	void Read( GameObject*& outGameObject )
	{
		uint32_t networkId;
		Read( networkId );
		outGameObject = mLinkingContext->GetGameObject( networkId );
	}
	
private:
	char*		mBuffer;
	uint32_t	mHead;
	uint32_t	mCapacity;

	LinkingContext* mLinkingContext;
};


void InputMemoryStream::Read( void* outData,
							  uint32_t inByteCount )
{
	uint32_t resultHead = mHead + inByteCount;
	if( resultHead > mCapacity )
	{
		//handle error, no data to read!
		//...
	}
	
	std::memcpy( outData, mBuffer + mHead, inByteCount );
	
	mHead = resultHead;
}

```

注：因为网络问题，数据包可能丢失，所以可能出现接收到网络ID，却没有对应的成员变量。

##### 内存比特流来缩减需要传输的数据

默认的内存流只能读写整数字节的数据，如果希望用尽可能少的比特来表示数据，就需要以比特的角度来读写。



##### 内联或嵌入来序列化特定成员

如果发送的数据有指针或者并不清楚的数据结构，那最好将其对象一个个发送，接收端先创建相应大小的容器，再将接收的数据放进容器中。

```c++
void OutputMemoryStream::Write( const void* inData,
								size_t inByteCount )
{
	//make sure we have space...
	uint32_t resultHead = mHead + static_cast< uint32_t >( inByteCount );
	if( resultHead > mCapacity )
	{
		ReallocBuffer( std::max( mCapacity * 2, resultHead ) );
	}
	
	//copy into buffer at head
	std::memcpy( mBuffer + mHead, inData, inByteCount );
	
	//increment head for next write
	mHead = resultHead;
}

	void Write( const std::vector< int >& inIntVector )
	{
		size_t elementCount = inIntVector.size();
		Write( elementCount );     //首先序列化vector的长度，这样另一端反序列化的时候就能用它来给vector开相应的空间
		Write( inIntVector.data(), elementCount * sizeof( int ) );   //然后再序列化vector的每个数据
	}
	
	template< typename T >
	void Write( const std::vector< T >& inVector )
	{
		uint32_t elementCount = inVector.size();
		Write( elementCount );
		for( const T& element : inVector )
		{
			Write( element );
		}
	}
	
	void Write( const std::string& inString )
	{
		size_t elementCount = inString.size() ;
		Write( elementCount );
		Write( inString.data(), elementCount * sizeof( char ) );
	}
	
	void Write( const GameObject* inGameObject )
	{
		uint32_t networkId = mLinkingContext->GetNetworkId( const_cast< GameObject* >( inGameObject ), false );
		Write( networkId );
	}

```

相应的反序列化

```c++
template< typename T >
	void Read( std::vector< T >& outVector )
	{
		size_t elementCount;
		Read( elementCount );
		outVector.resize( elementCount );
		for( const T& element : outVector )
		{
			Read( element );
		}
	}
	
	void Read( GameObject*& outGameObject )
	{
		uint32_t networkId;
		Read( networkId );
		outGameObject = mLinkingContext->GetGameObject( networkId );
	}
```

简单来说，就是发送方分两块发送，一块是发送数据大小，二块是将容器内的每个数据单独发送。接收方接收到数据大小后，创建相应的数据类型并开同样的大小，而后顺序将接收的数据放进容器。

##### 链接

如果序列化的数据被不止一个指针引用

例：

```c++
 class Home : public Obj
 {
     ...
     vector<player*> playervec;
 }
 class player
 {
     ...
     Obj* myhome;
 }
```

此时再用内联/嵌入就会陷入死循环递归中。

解决方法是给每个多出引用的对象建立唯一的映射，通过序列化标识符实现序列化，另一端通过标识符查找引用对象来反序列化。

例：

该链接包含了网络ID和游戏对象之间的映射

```c++

#ifndef RoboCat_LinkingContext_h
#define RoboCat_LinkingContext_h

class GameObject;

class LinkingContext  //该链接创建了一个哈希表来保存映射关系，发送对象对应的ID，接收ID来找到对应的对象
{
public:
	
	LinkingContext() : 
	mNextNetworkId( 1 )
	{}

	uint32_t GetNetworkId( GameObject* inGameObject, bool inShouldCreateIfNotFound )
	{
		auto it = mGameObjectToNetworkIdMap.find( inGameObject );
		if( it != mGameObjectToNetworkIdMap.end() )
		{
			return it->second;
		}
		else if( inShouldCreateIfNotFound )
		{
			uint32_t newNetworkId = mNextNetworkId++;
			AddGameObject( inGameObject, newNetworkId );
			return newNetworkId;
		}
		else
		{
			return 0;
		}
	}
	
	GameObject* GetGameObject( uint32_t inNetworkId ) const
	{
		auto it = mNetworkIdToGameObjectMap.find( inNetworkId );
		if( it != mNetworkIdToGameObjectMap.end() )
		{
			return it->second;
		}
		else
		{
			return nullptr;
		}
	}

	void AddGameObject( GameObject* inGameObject, uint32_t inNetworkId )
	{
		mNetworkIdToGameObjectMap[ inNetworkId ] = inGameObject;
		mGameObjectToNetworkIdMap[ inGameObject ] = inNetworkId;
	}
	
	void RemoveGameObject( GameObject *inGameObject )
	{
		uint32_t networkId = mGameObjectToNetworkIdMap[ inGameObject ];
		mGameObjectToNetworkIdMap.erase( inGameObject );
		mNetworkIdToGameObjectMap.erase( networkId );
	}
    
    void wirte(const GameObject* inGameObject)
    {
        uint32_t GameID;
        GameID = GetNetworkId(inGameObject);
        write(GameID);
    }
	
    void read(GameObject* outGameObject)
    {
        uint32_t GameID;
        read(GameID);
        outGameObject=GetGameObject(GameID) ;
    }
private:
	std::unordered_map< uint32_t, GameObject* > mNetworkIdToGameObjectMap;
	std::unordered_map< const GameObject*, uint32_t > mGameObjectToNetworkIdMap;

	uint32_t mNextNetworkId;
};

#endif

```

##### 压缩



##### 抽象序列化

序列化的时候往往使用许多巧妙的数据结构或压缩技术来使传输的比特流尽可能短，而当你改变了一个成员变量的写法，那么必须改变它的读法，为了减少读写之间的耦合度，可以用一个结构体同时处理读和写的方法

例：输出输入流都继承自带有serialize方法的基类memorystream，

通过serialize，子类得到一个指向数据的指针和数据大小，然后采取合适的行为（读or写）。



##### 数据驱动的序列化

有时你并不需要将整个类传输给=过去，只传输类中的某个变量，这时可以建立一个反射系统，允许运行时访问类结构。

例：

```c++
class memodataoff{
    
}
```



## 并发服务器端

<img src="https://img-blog.csdnimg.cn/2020120119151093.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzE3MTE5,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:150%;" />

### **进程的概念**

在多道程序环境下，允许多个程序并发执行，此时它们将失去封闭性，并具有间断性及不可再现性的特征。为此引入了进程(Process)的概念，以便更好地描述和控制程序的并发执行，实现操作系统的并发性和共享性。

为了使参与并发执行的程序（含数据）能独立地运行，必须为之配置一个专门的数据结构，称为进程控制块(Process Control Block, PCB)。系统利用PCB来描述进程的基本情况和运行状态，进而控制和管理进程。相应地，由程序段、相关数据段和PCB三部分构成了进程映像（进程实体）。所谓创建进程，实质上是创建进程映像中的PCB；而撤销进程，实质上是撤销进程的PCB。值得注意的是，进程映像是静态的，进程则是动态的。

注意：PCB是进程存在的唯一标志！

从不同的角度，进程可以有不同的定义，比较典型的定义有：

- 进程是程序的一次执行过程。
- 进程是一个程序及其数据在处理机上顺序执行时所发生的活动。
- 进程是具有独立功能的程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位。

在引入进程实体的概念后，我们可以把传统操作系统中的进程定义为：”进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。“

程序在内存中的划分：

<img src="https://img-blog.csdnimg.cn/20190803071754478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE5MDE4Mjc3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:67%;" />

操作系统通过构建一个叫进程控制块(PCB)的数据结构来存放这些信息。（进程的程序段和数据段位于内存中的位置）

<img src="https://img-blog.csdnimg.cn/20190803061909998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE5MDE4Mjc3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 67%;" />

当程序运行的时候，操作系统会为该进程构建一个叫进程控制块(PCB)的数据结构，用以记录该进程的各种信息。
而 **进程实体 = PCB + 程序段 + 数据段。** (进程实体 别名 进程映像)



**进程的特征**

进程是由多程序的并发执行而引出的，它和程序是两个截然不同的概念。进程的基本特征是对比单个程序的顺序执行提出的，也是对进程管理提出的基本要求。

1. 动态性：进程是程序的一次执行，它有着创建、活动、暂停、终止等过程，具有一定的生命周期，是动态地产生、变化和消亡的。动态性是进程最基本的特征。

2. 并发性：指多个进程实体，同存于内存中，能在一段时间内同时运行，并发性是进程的重要特征，同时也是操作系统的重要特征。引入进程的目的就是为了使程序能与其他进程的程序并发执行，以提高资源利用率。

3. 独立性：指进程实体是一个能独立运行、独立获得资源和独立接受调度的基本单位。凡未建立PCB的程序都不能作为一个独立的单位参与运行。

4. 异步性：由于进程的相互制约，使进程具有执行的间断性，即进程按各自独立的、 不可预知的速度向前推进。异步性会导致执行结果的不可再现性，为此，在操作系统中必须配置相应的进程同步机制。

5. 结构性：每个进程都配置一个PCB对其进行描述。从结构上看，进程实体是由程序段、数据段和进程控制段三部分组成的。

   **在网络编程里，我们可一简单的认为进程就是 占有内存空间的正在运行的程序**

   *拥有几个运算设备的CPU被称为几核CPU，若进程数超过CPU核数，进程将分时使用CPU资源。（因为CPU的运算速度很快，所以我们会感到所有进程是同时运行的）*

**僵尸进程**

有些进程完成工作后本应该被销毁但是保留了下来，占用系统中的重要资源。在UNIX系统中，一个进程结束了，但是其父进程没有等待（调用wait/waitpid）它，那么它将变成一个僵尸进程。通过PS命令可以查看其带有defunct的标志，僵尸进程是一个早已死亡的进程，但在进程表中仍占据一个位置。

```
当一个进程创建了一个子进程时，他们的运行时异步的。即父进程无法预知子进程会在什么时候结束，那么如果父进程很繁忙来不及wait 子进程时，那么当子进程结束时，会不会丢失子进程的结束时的状态信息呢？处于这种考虑unix提供了一种机制可以保证只要父进程想知道子进程结束时的信息，它就可以得到。

这种机制是：在每个进程退出的时候，内核释放该进程所有的资源，包括打开的文件，占用的内存。但是仍然保留了一些信息（如进程号pid 退出状态 运行时间等）。这些保留的信息直到进程通过调用wait/waitpid时才会释放。这样就导致了一个问题，如果没有调用wait/waitpid的话，那么保留的信息就不会释放。比如进程号就会被一直占用了。但系统所能使用的进程号的有限的，如果产生大量的僵尸进程，将导致系统没有可用的进程号而导致系统不能创建进程。所以我们应该避免僵尸进程


这里有一个需要注意的地方。如果子进程先结束而父进程后结束，即子进程结束后，父进程还在继续运行但是并未调用wait/waitpid那子进程就会成为僵尸进程。


但如果子进程后结束，即父进程先结束了，但没有调用wait/waitpid来等待子进程的结束，此时子进程还在运行，父进程已经结束。那么并不会产生僵尸进程。应为每个进程结束时，系统都会扫描当前系统中运行的所有进程，看看有没有哪个进程时刚刚结束的这个进程的子进程，如果有，就有init来接管它，成为它的父进程。


同样的在产生僵尸进程的那种情况下，即子进程结束了但父进程还在继续运行（并未调用wait/waitpid）这段期间，假如父进程异常终止了，那么该子进程就会自动被init接管。那么它就不再是僵尸进程了。应为intit会发现并释放它所占有的资源。（当然如果进程表越大，init发现它接管僵尸进程这个过程就会变得越慢，所以在init为发现他们之前，僵尸进程依旧消耗着系统的资源）
```



#### 调用wait函数销毁僵尸进程

```c++
#include<sys/wait.h>
pid_t wait(int *status)		
    success:返回终止的子进程ID
    faild:返回-1
参数status是1个整型指针。如果参数status的值不是NULL，wait就会把子进程退出时的状态取出并存入其中，这是1个整数值（int），指出了子进程是正常退出还是被非正常结束的（1个进程也能够被其他进程用信号结束），和正常结束时的返回值，或被哪个信号结束的等信息。由于这些信息被寄存在1个整数的不同2进制位中，所以用常规的方法读取会非常麻烦，人们就设计了1套专门的宏（macro）来完成这项工作:WIFEXITED(status)
```



```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/wait.h>

int main(int argc, char *argv[])
{
	int status;
	pid_t pid=fork();   //此处创建的子进程将在第13行被return终止

	if(pid==0)
	{
		return 3;
	}
	else
	{
		printf("Child PID: %d \n", pid);
		pid=fork();    //此处创建的子进程将在21行被exit终止
		if(pid==0)
		{
			exit(7);
		}
		else
		{
			printf("Child PID: %d \n", pid);
			wait(&status);   //调用wait 函数。之前终止的子进程相关信息将保存到status 变量，同时相关子进程被完全销毁。
			//通过 IFE ED 宏验证子进程是否正常终止 如果正常退出，则调用WEXITSTATUS宏输出子进程的返回值。
			if(WIFEXITED(status))
				printf("Child send one: %d \n", WEXITSTATUS(status));
            //因为之前创建了两个进程，所以再次调用wait 函数和宏
			wait(&status);
			if(WIFEXITED(status))
				printf("Child send two: %d \n", WEXITSTATUS(status));
			sleep(30);     // Sleep 30 sec.为暂停父进程终止而插入的代码 此时可以查看子进程的状态
		}
	}
	return 0;
}
Child PIO: 12337 
Child PID: 12338 
Child send one: 3 
Child send two: 7 
```



#### 调用waitpid函数销毁僵尸进程

```c++
#include<sys/wait.h>
pid_t waitpid(pid_t pid,int *status,int options)
    pid：要销毁的已终止的目标子进程的ID，若填-1，则会销毁第一个终止的子进程
    options：传递头文件sys/wait.h中声明的常量WNOHANG ，即使没有终止的子进程也不会进入阻塞状，而是返回0并退出函数
```



```c++
#include <stdio.h>
#include <unistd.h>
#include <sys/wait.h>

int main(int argc, char *argv[])
{
	int status;
	pid_t pid=fork();
	
	if(pid==0)
	{
		sleep(15);
		return 24;   	
	}
	else
	{
		while(!waitpid(-1, &status, WNOHANG))
		{
			sleep(3);
			puts("sleep 3sec.");
		}

		if(WIFEXITED(status))
			printf("Child send %d \n", WEXITSTATUS(status));
	}
	return 0;
}

/*
root@my_linux:/home/swyoon/tcpip# gcc waitpid.c -o waitpid
root@my_linux:/home/swyoon/tcpip# ./waitpid
sleep 3sec.
sleep 3sec.
sleep 3sec.
sleep 3sec.
sleep 3sec.
Child send 24 
*/

```

1. 如果父进程的所有子进程都还在运行，调用wait将使父进程阻塞，而调用waitpid时如果在options参数中指定WNOHANG可以使父进程不阻塞而立即返回0。
2. wait等待第一个终止的子进程，而waitpid可以通过pid参数指定等待哪一个子进程。

所以，调用wait和waitpid不仅可以获得子进程的终止信息，还可以使父进程阻塞等待子进程终止，起到进程间同步的作用。**如果参数status不是空指针，则子进程的终止信息通过这个参数传出，如果只是为了同步而不关心子进程的终止信息，可以将statusc参数指定为NULL。** 

### 进程间的通信

概括性地说，进程间通信是指两个进程之间交换数据。但是从内存的角度看，可以理解为两个进程共有内存。因为共享的内存区域存在，可以进行数据交换

#### 调用pipe实现进程通信

```c++
#include<unistd.h>
int pipe(int fileds[2])
filedes[0] 通过管道接收数据时使用的文件描述符（管道出口）
filedes[1] 通过管道接收数据时使用的文件描述符（管道入口）
    
int fds1[2], fds2[2];两个管道，一个fds1，一个fds2
pid_t pid;
pipe(fds1), pipe(fds2);用pipe连接起来
pid=fork();
```



```c++
#include <stdio.h>
#include <unistd.h>
#define BUF_SIZE 30

int main(int argc, char *argv[])
{
	int fds[2];
	char str[]="Who are you?";
	char buf[BUF_SIZE];
	pid_t pid;
	
	pipe(fds);
	pid=fork();
	if(pid==0)
	{
		write(fds[1], str, sizeof(str));
	}
	else
	{
		read(fds[0], buf, BUF_SIZE);
		puts(buf);
	}
	return 0;
}
```

只用一个管道无法实现双向通信：因为向管道传递数据时，不管是哪个进程传的，先读的进程会把数据取走

```c++
#include <stdio.h>
#include <unistd.h>
#define BUF_SIZE 30

int main(int argc, char *argv[])
{
	int fds[2];
	char str1[]="Who are you?";
	char str2[]="Thank you for your message";
	char buf[BUF_SIZE];
	pid_t pid;

	pipe(fds);
	pid=fork();

	if(pid==0)
	{
		write(fds[1], str1, sizeof(str1)); 子进程传递的数据
		read(fds[0], buf, BUF_SIZE);子进程读取
		printf("Child proc output: %s \n",  buf);
	}
	else
	{
		read(fds[0], buf, BUF_SIZE);  //因为上面子进程比父进程更先读取，所以父进程不会收到管道里的消息
		printf("Parent proc output: %s \n", buf);
		write(fds[1], str2, sizeof(str2));
	}
	return 0;
}
Child proc output:Who are you?
```

实现双向通信使2个进程相互交换3次字符串。

```c++

#include <stdio.h>
#include <string.h>
#include <unistd.h>
#define BUF_SIZE 30

int main(int argc, char *argv[])
{
	int fds1[2], fds2[2];
	char str1[]="Do you like coffee?";
	char str2[]="I like coffee";
	char str3[]="I like bread";
	char * str_arr[]={str1, str2, str3};
	char buf[BUF_SIZE];
	pid_t pid;
	int i;
	
	pipe(fds1), pipe(fds2);
	pid=fork();
	
	if(pid==0)
	{
		for(i=0; i<3; i++)
		{
			write(fds1[1], str_arr[i], strlen(str_arr[i])+1);
			read(fds2[0], buf, BUF_SIZE);
			printf("Child proc output: %s \n",  buf);
		}
	}
	else
	{
		for(i=0; i<3; i++)
		{
			read(fds1[0], buf, BUF_SIZE);
			printf("Parent proc output: %s \n", buf);
			write(fds2[1], str_arr[i], strlen(str_arr[i])+1);
		}
	}
	return 0;
}



```

保存消息的回声服务器

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <signal.h>
#include <sys/wait.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define BUF_SIZE 100
void error_handling(char *message);
void read_childproc(int sig);

int main(int argc, char *argv[])
{
	int serv_sock, clnt_sock;
	struct sockaddr_in serv_adr, clnt_adr;
	int fds[2];
	
	pid_t pid;
	struct sigaction act;
	socklen_t adr_sz;
	int str_len, state;
	char buf[BUF_SIZE];
	if(argc!=2) {
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}

	act.sa_handler=read_childproc;
	sigemptyset(&act.sa_mask);
	act.sa_flags=0;
	state=sigaction(SIGCHLD, &act, 0);

	serv_sock=socket(PF_INET, SOCK_STREAM, 0);
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port=htons(atoi(argv[1]));
	
	if(bind(serv_sock, (struct sockaddr*) &serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");
	if(listen(serv_sock, 5)==-1)
		error_handling("listen() error");
	
	pipe(fds);
	pid=fork();
	if(pid==0)
	{
		FILE * fp=fopen("echomsg.txt", "wt");
		char msgbuf[BUF_SIZE];
		int i, len;

		for(i=0; i<10; i++)
		{
			len=read(fds[0], msgbuf, BUF_SIZE);
			fwrite((void*)msgbuf, 1, len, fp);
		}
		fclose(fp);
		return 0;
	}

	while(1)
	{
		adr_sz=sizeof(clnt_adr);
		clnt_sock=accept(serv_sock, (struct sockaddr*)&clnt_adr, &adr_sz);
		if(clnt_sock==-1)
			continue;
		else
			puts("new client connected...");

		pid=fork();
		if(pid==0)
		{
			close(serv_sock);
			while((str_len=read(clnt_sock, buf, BUF_SIZE))!=0)
			{
				write(clnt_sock, buf, str_len);
				write(fds[1], buf, str_len);
			}
			
			close(clnt_sock);
			puts("client disconnected...");
			return 0;
		}
		else
			close(clnt_sock);
	}
	close(serv_sock);
	return 0;
}

void read_childproc(int sig)
{
	pid_t pid;
	int status;
	pid=waitpid(-1, &status, WNOHANG);
	printf("removed proc id: %d \n", pid);
}
void error_handling(char *message)
{
	fputs(buf, stderr);
	fputc('\n', stderr);
	exit(1);
}
```



#### 调用socketpair创建双向通信管道

```c++
#include <sys/types.h>
#include <sys/socket.h>
int socketpair(int d, int type, int protocol, int sv[2])  //返回0：创建成功；返回-1：创建失败；
    
d:协议族，AF_LOCAL AF_UNIX SOCK_NOBLOCK SOCK_CLOEXEC;
type:协议 TCP:SOCK_STREAM SOCK_DGRAM SOCK_STREAM /  UDP:SOCK_DGRAM;
protocol:表示类型，只能为0;
sv[2]:套接字句柄对。
```

**1、该函数只能用于UNIX域（LINUX）下。
2、只能用于有亲缘关系的进程（或线程）间通信。
3、所创建的套节字对作用是一样的，均能够可读可写（而管道PIPE只能进行单向读或写）。**

- **例如，可以往sv[0]中写，从sv[1]中读；或者从sv[1]中写，从sv[0]中读； 
  如果往一个套接字(如sv[0])中写入后，再从该套接字读时会阻塞，只能在另一个套接字中(sv[1])上读成功；** 

**4、在读的时候，管道内必须有内容，否则将会阻塞；简而言之，该函数是阻塞的。**

-  **读、写操作可以位于同一个进程，也可以分别位于不同的进程，如父子进程。如果是父子进程时，一般会功能分离，一个进程用来读，一个用来写。因为文件描述副sv[0]和sv[1]是进程共享的，所以读的进程要关闭写描述符, 反之，写的进程关闭读描述符。** 

#### eventfd实现进程间通信



### 信号处理

信号是由操作系统传给进程的中断，会提早终止一个程序。在 UNIX、LINUX、Mac OS X 或 Windows 系统上，可以通过按 Ctrl+C 产生中断。

| SIGABRT | 程序的异常终止，如调用 **abort**。                      |
| ------- | ------------------------------------------------------- |
| SIGFPE  | 错误的算术运算，比如除以零或导致溢出的操作。            |
| SIGILL  | 检测非法指令。                                          |
| SIGINT  | 程序终止(interrupt)信号。（可以通过按 Ctrl+C 产生中断） |
| SIGSEGV | 非法访问内存。                                          |
| SIGTERM | 发送到程序的终止请求。                                  |

#### 调用raise函数生成信号

####  调用signal函数“注册信号”捕获事件

```c++
#include<signal.h>
void (*signal (int sig, void (*func)(int)))(int); //signal(registered signal, signal handler)
//第一个参数是一个整数，代表了信号的编号；第二个参数是一个指向信号处理函数的指针。
signal(SIGCHLD,mychild);
//子进程终止时调用mychild函数（mychild函数的参数应为int，返回值类型应为void）
signal(SIGALRM,timeout)
//已到alarm函数注册的时间，调用timeout函数
signal(SIGINT,keycontrol)
//输入CRTL+C时调用keycontrol函数
```

| SIGALRM | 已到alarm函数注册的时间                      |
| ------- | -------------------------------------------- |
| SIGFPE  | 错误的算术运算，比如除以零或导致溢出的操作。 |
| SIGCHLD | 子进程终止                                   |

```c++
#include <iostream>
#include <csignal>
#include <unistd.h>
 
using namespace std;
 
void signalHandler( int signum )
{
    cout << "Interrupt signal (" << signum << ") received.\n";
 
    // 清理并关闭
    // 终止程序  
 
   exit(signum);  
 
}
 
int main ()
{
    // 注册信号 SIGINT 和信号处理程序
    signal(SIGINT, signalHandler);  
 
    while(1){
       cout << "Going to sleep...." << endl;
       sleep(1);
    }
 
    return 0;
}

Going to sleep....
Going to sleep....
Going to sleep....
按 Ctrl+C 来中断程序，您会看到程序捕获信号，程序打印如下内容并退出：
Going to sleep....
Going to sleep....
Going to sleep....
Interrupt signal (2) received.
```



#### 调用sigaction函数进行信号处理



```c++
#include <signal.h>
struct sigaction
{
    void (*sa_handler) (int);//保存信号处理函数的地址值，此参数和signal()的参数handler 相同, 代表新的信号处理函数, 其他意义请参考signal().
    sigset_t sa_mask;//用来设置在处理该信号时暂时将sa_mask 指定的信号搁置.
    int sa_flags;//用来设置信号处理的其他相关操作, 下列的数值可用：
    void (*sa_restorer) (void);//此参数没有使用
}

int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);//成功返回0，失败返回-1
signum:与signal函数相同，sigaction()会依参数signum 指定的信号编号来设置该信号的处理函数. 参数signum 可以指定SIGKILL 和SIGSTOP 以外的所有信号。
    
	struct sigaction act;
	act.sa_handler=满足signum时调用的函数;//为了注册信号处理函数，声明sigaction结构体并在sa_handler储存函数指针
	sigemptyset(&act.sa_mask);//调用sigemptyset函数将sa_mask成员的所有位初始化为0
	act.sa_flags=0;
	sigaction(SIGALRM, &act, 0);//注册SIGALRM信号的处理器。调用alarm函数预约2秒后发生SIGALRM信号

```



```c++
#include <stdio.h>
#include <unistd.h>
#include <signal.h>

void timeout(int sig)
{
	if(sig==SIGALRM)
		puts("Time out!");
	alarm(2);
}

int main(int argc, char *argv[])
{
	int i;
	struct sigaction act;
	act.sa_handler=timeout;//为了注册信号处理函数，声明sigaction结构体并在sa_handler储存函数指针
	sigemptyset(&act.sa_mask);//调用sigemptyset函数将sa_mask成员的所有位初始化为0
	act.sa_flags=0;
	sigaction(SIGALRM, &act, 0);//注册SIGALRM信号的处理器。调用alarm函数预约2秒后发生SIGALRM信号

	alarm(2);

	for(i=0; i<3; i++)
	{
		puts("wait...");
		sleep(100);
	}
	return 0;
}
root@my_linux:/tcpip# gcc sigaction.c -0 sigaction 
root@my_ linux:/tcpip# ./sigaction 
wai t. . . 
Time out! 
wai t. . . 
Time out! 
wait. . . 
Time out! 
```

#### 利用信号处理函数杀死僵尸进程

```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <signal.h>
#include <sys/wait.h>

void read_childproc(int sig)
{
	int status;
	pid_t id=waitpid(-1, &status, WNOHANG);  //销毁的终止子进程的id
	if(WIFEXITED(status))  //status保存销毁的终止子进程的信息，用WIFEXITED输出
	{
		printf("Removed proc id: %d \n", id);
		printf("Child send: %d \n", WEXITSTATUS(status));
	}
}

int main(int argc, char *argv[])
{
	pid_t pid;
	struct sigaction act;
	act.sa_handler=read_childproc;
	sigemptyset(&act.sa_mask);
	act.sa_flags=0;
	sigaction(SIGCHLD, &act, 0);

	pid=fork();
	if(pid==0)
	{
		puts("Hi! I'm child process");
		sleep(10);
		return 12;
	}
	else
	{
		printf("Child proc id: %d \n", pid);
		pid=fork();
		if(pid==0)
		{
			puts("Hi! I'm child process");
			sleep(10);
			exit(24);
		}
		else
		{
			int i;
			printf("Child proc id: %d \n", pid);
			for(i=0; i<5; i++)//为了等待发生SIGCHLD信号，使父进程暂停五次（25秒）当子进程结束时会调用read_childproc函数。
			{
				puts("wait...");
				sleep(5);
			}
		}
	}
	return 0;
}

/*
root@my_linux:/home/swyoon/tcpip# gcc remove_zombie.c -o zombie
root@my_linux:/home/swyoon/tcpip# ./zombie
Hi! I'm child process
Child proc id: 9529 
Hi! I'm child process
Child proc id: 9530 
wait...
wait...
Removed proc id: 9530 
Child send: 24 
wait...
Removed proc id: 9529 
Child send: 12 
wait...
wait...

*/

```



#### 信号使用原则

在多线程程序中，使用signal的第一原则是不要使用signal 39。包括 ·

- 不要用signal作为IPC的手段，包括不要用SIGUSR1等信号来触发 服务端的行为。如果确实需要，可以用§9.5介绍的增加监听端口的方式 来实现双向的、可远程访问的进程控制。 ·
- 也不要使用基于signal实现的定时函数，包括 alarm/ualarm/setitimer/timer_create、sleep/usleep等等。 ·
- 不主动处理各种异常信号（SIGTERM、SIGINT等等），只用默认 语义：结束进程。有一个例外：SIGPIPE，服务器程序通常的做法是忽 略此信号，否则如果对方断开连接，而本机继续write的话，会导致程 序意外终止。 ·
- 在没有别的替代方法的情况下（比方说需要处理SIGCHLD信 号），把异步信号转换为同步的文件描述符事件。传统的做法是在 signal handler里往一个特定的pipe(2)写一个字节，在主程序中从这个 pipe读取，从而纳入统一的IO事件处理框架中去。现代Linux的做法是采 用signalfd(2)把信号直接转换为文件描述符事件，从而从根本上避免使 用signal handler 。

### 多进程服务器：通过创建多个进程提供服务

### 基于多任务的并发服务器

#### 调用fork函数创建进程

```c++
#include<unistd.h>
pid_t fork(void)  //fork()一般不能在多线程中调用，因为Linux的fork只克隆当前线程的thread of ctrl，不克隆其他线程。即，fork之后子进程只有一个线程，其他线程都消失了。
```

fork（）函数通过系统调用创建一个与原来进程几乎完全相同的进程(子进程），但如果初始参数或者传入的变量不同，两个进程也可以做不同的事。
    一个进程调用fork（）函数后，系统先给新的进程分配资源，例如存储数据和代码的空间。然后把原来的进程的所有值都复制到新的新进程中，只有少数值与原来的进程的值不同。相当于克隆了一个自己。

- 子进程继承父进程
  用户号UIDs和用户组号GIDs
  环境Environment
  堆栈
  共享内存
  打开文件的描述符
  执行时关闭（Close-on-exec）标志
  信号（Signal）控制设定
  进程组号
  当前工作目录
  根目录
  文件方式创建屏蔽字
  资源限制
  控制终端

- 子进程独有

  进程号PID
  不同的父进程号
  自己的文件描述符和目录流的拷贝
  子进程不继承父进程的进程正文（text），数据和其他锁定内存（memory locks，mloack，mlockall，fcntl），某些定时器（setitimer，alarm，timer_create）
  不继承异步输入和输出
  父进程和子进程拥有独立的地址空间和PID参数。

子进程从父进程继承了用户号和用户组号，用户信息，目录信息，环境（表），打开的文件描述符，堆栈，（共享）内存等。
经过fork()以后，父进程和子进程拥有相同内容的代码段、数据段和用户堆栈，就像父进程把自己克隆了一遍。**事实上，父进程只复制了自己的PCB块。而代码段，数据段和用户堆栈内存空间并没有复制一份，而是与子进程共享。只有当子进程在运行中出现写操作时，才会产生中断，并为子进程分配内存空间。由于父进程的PCB和子进程的一样，所以在PCB中断中所记录的父进程占有的资源，**

```c++
#include <stdio.h>
#include <unistd.h>
int gval=10;

int main(int argc, char *argv[])
{
	pid_t pid;
	int lval=20;
	gval++, lval+=5;

	pid=fork();
	if(pid==0)	// if Child Process 
		gval+=2, lval+=2;   在子进程下gval+=2, lval+=2
	else			// if Parent Process
		gval-=2, lval-=2;   在父进程下gval-=2, lval-=2;

    //在语句pid=fork()之前，只有一个进程在执行这段代码，但在这条语句之后，就变成两个进程在执行了
    //pid的值为什么在父子进程中不同。其实就相当于链表，进程形成了链表，父进程的pid(p 意味point)指向子进程的进程id, 因为子进程没有子进程，所以其pid为0.
    
	if(pid==0)  //pid==0,说明是子进程，子进程下gval+=2, lval+=2
		printf("Child Proc: [%d, %d] \n", gval, lval);
	else       //pid!=1，说明是父进程，父进程下gval-=2, lval-=2;
		printf("Parent Proc: [%d, %d] \n", gval, lval);
	return 0;
}
root@my_linux:/tcpip# gcc fork.c -0 fork 
root@my_linux:/tcpip# ./fork 
Child Proc: [13, 27] 
Parent Proc:[9, 23]
```

 **fork出错可能有两种原因：**
  **1）当前的进程数已经达到了系统规定的上限，这时errno的值被设置为EAGAIN。**
  **2）系统内存不足，这时errno的值被设置为ENOMEM。**

```c++
#include<unistd.h>
#include<stdio.h>
int main(void)
{
    int i=0;
    for(int i=0;i<2;i++){
        pid_t pid=fork();
        if(pid==0)
            printf("%d child %4d %4d %4d/n",i,getppid(),getpid(),pid);
        else
            printf("%d parent %4d %4d %4d/n",i,getppid(),getpid(),pid);
    }
    return 0;
}
    //假设parent先执行
 	0 parent 2043 3224 3225(第一次fork后，p3224（父进程） pid=3225（fork函数在父进程中返向子进程id）)
    0 child  3224 3225    0
    //两个进程并发执行代码所以输出两个parent
    1 parent 2043 3224 3226(i=0的parent 3224被fork（）后变成了3226的父进程)
    1 parent 3224 3225 3227(i=0的child 3225被fork（）后变成了3227的child的父进程)
    //同上else执行完后执行pid==0
    1 child     1 3227    0(i=0的child 3225的子进程3227的子进程)
    1 child     1 3226    0(i=1的parent 3224的子进程)
```

<img src="https://images2018.cnblogs.com/blog/1268332/201802/1268332-20180225234927120-1222664557.png" alt="img" style="zoom: 80%;" />

**对于这种N次循环的情况，执行printf函数的次数为2\*（1+2+4+……+2N-1）次，创建的子进程数为1+2+4+……+2N-1个。**

#### 调用vfork函数创建进程（性能更好一点，但是慎用）

```c++
#include<unistd.h>
pid_t vfork(void)
    //vfork创建的子进程不会复制父进程的代码段资源，而是通过exec系列函数直接加载一个可执行文件启动子进程,保证子进程先运行，在它调用 exec（进程替换） 或 _exit（退出进程）之后父进程才可能被调度运行。
    //但需要注意的是，由于vfork()毕竟还是产生一个新的进程，所以子进程拥有自己的进程描述符，拥有自己的寄存器，最重要的是，拥有自己的打开文件列表
    //子进程创建成功前，子进程暂时借用父进程的相关资源来加载子进程，而此时的父进程是阻塞状态，只有子进程创建成功后，父进程才继续运行。
	//在子进程中使用return会报错：这是由于return语句改变了当前的栈空间，但对于vfork来说，很多的实现为了提高效率不会复制父进程的栈空间，而是和父进程共享。在子进程中使用return后，父进程的栈空间就会发生变化，当父进程返回的时候系统就会报错。
```

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211010152452094.png" alt="image-20211010152452094" style="zoom:67%;" />

**子进程共享父进程的地址空间**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
int a = 10;
int main(int argc, char *argv[])
{
	pid_t pid;
	int b = 20;
	
	pid = vfork();	// 创建进程
	if(pid < 0){ // 出错
		perror("vfork");
	}
	
	if(0 == pid){ // 子进程
	
		a = 100, b = 200;
		printf("son: a = %d, b = %d\n", a, b);
		
		_exit(0); // 退出子进程，必须，否则父进程会一直阻塞造成死锁
	}else if(pid > 0){ // 父进程
		
		printf("father: a = %d, b = %d\n", a, b);	
	}
	
	return 0;
}
son: a = 100,b = 200
father: a = 100,b = 200//子进程修改 a, b 的值，会影响到父进程的 a, b（父子共享地址空间）
```

**如果子进程没有调用exec或exit会造成死锁**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
 
int main(int argc, char *argv[])
{
	pid_t pid;
	
	pid = vfork();	// 创建进程
	if(pid < 0){ // 出错
		perror("vfork");
	}
	
	if(0 == pid){ // 子进程
	
		printf("i am son\n");
		sleep(1);
		
		// 子进程没有调用 exec 或 exit
	}else if(pid > 0){ // 父进程
		
		printf("i am father\n");
		sleep(1);
	}
	
	return 0;
}
i am son
i am father
出错（父进程return的时候报错，并不会出现一直卡着的状态）
```



#### exec族函数

在Linux编程中我们使用fork()来创建新的进程，而子进程是父进程的副本，而有时我们希望通过子进程来执行另外的程序，而exec族函数可以按照指定的文件名或目录找到可执行的程序文件，并用来取代原来的数据段，代码段，堆栈段，该进程被完全替换为新程序。

exec调用并没有生成新进程。一个进程一旦调用exec函数，它本身就“死亡”了，系统把代码段替换成新的程序的代码，废弃了原有的数据段和堆栈段，并为新程序分配新的数据段与堆栈段，惟一保留的就是进程ID。对系统而言，还是同一个进程，不过执行的是另一个程序了。

```c++
#include <unistd.h>
extern char **environ;
int execl(const char *path, const char *arg, ...);//常用
int execlp(const char *file, const char *arg, ...);//常用
int execle(const char *path, const char *arg,..., char * const envp[]);
int execv(const char *path, char *const argv[]);//常用
int execvp(const char *file, char *const argv[]);//常用
int execvpe(const char *file, char *const argv[],char *const envp[]);
//都是以exec为前缀，那么不同的之后后面的一些字符，l表示命令行参数列表、p表示PATH环境变量、v表示使用参数数组、e使用环境变量数组。其中execvpe和execle一般不常用,exec族函数的函数执行成功后不会返回，调用失败则返回-1
path:可执行文件的路径
arg：命令行参数，至少一个，且以NULL结尾
file：可执行文件名，该文件必须存储在path环境变量的路径下
argv：存储命令行参数的字符串数组，最后一个必须是NULL
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210512134804742.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDQ3NTMxMQ==,size_16,color_FFFFFF,t_70#pic_center)

```c++
int main()
{
        execl("/bin/ls", "ls", "-l", NULL);
        perror("execl");
        exit(1);
}
//当前的进程调用ls这个可执行程序，并添加了-l参数。由于execl成功调用后这个进程的代码段都被替换了，自然下面的代码就不会再执行了，所以也就没有返回值了，但是当调用失败后就会返回-1并设置errno值。
int main()
{
    	execlp( "ls","ls", "-l", NULL);//不用加ls的路径,因为系统会去PATH中查找。
        perror("execl");
        exit(1);
}

total 16
-rw-r--r-- 1 charles charles  163 Feb 27 15:49 a.c
-rwxr-xr-x 1 charles charles 8384 Feb 27 15:49 test
int main(void)
{
        char *argv[] = {"ls", "-l", NULL};
        execv("/bin/ls", argv);
        perror("execl");
        exit(1);
}
```

**一般的exec函数族的错误原因：**

**1. 找不到文件或者路径，此时errno为ENOENT。**

**2. 数组argv和envp(环境变量数组)没有以NULL结尾，此时errno为EFAULT。**

**3. 没有对应可执行文件的运行权限，此时errno为EACCES。**

**在程序a中调用程序upper**

upper.c

```c++
#include <stdio.h>
#include <ctype.h>

int main(void)
{
	char s;
	while((s = getchar())!=EOF){
		putchar(toupper(s));
	}
	return 0;
}
```

程序a

```c++
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <unistd.h>

int main(int argc, char *argv[])
{
	if(argc != 2){                     // 接收到两个参数
		printf("Open error!\n");
		exit(1);
	}
	int fd = open(argv[1], O_RDONLY);  // 以只读的方式打开文本文件
	if(fd < 0){
		perror("open file");
		exit(1);
	}
	dup2(fd, STDIN_FILENO);            // 输入重定向，使STDIN_FILENO指向fd所指的文件
	close(fd);
    execv("./upper", "upper", NULL);   // 调用upper可执行文件
	perror("execl");
	exit(1);
}

```

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211010163101084.png" alt="image-20211010163101084" style="zoom:67%;" />

#### 利用fork实现多任务（进程）的并发服务器

**服务器端**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <signal.h>
#include <sys/wait.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define BUF_SIZE 30
void error_handling(char *message);
void read_childproc(int sig);
int main(int argc, char *argv[])
{
	int serv_sock, clnt_sock;
	struct sockaddr_in serv_adr, clnt_adr;

	pid_t pid;
	struct sigaction act;
	socklen_t adr_sz;
	int str_len, state;
	char buf[BUF_SIZE];
	if(argc!=2) {
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}

	act.sa_handler=read_childproc;
	sigemptyset(&act.sa_mask);
	act.sa_flags=0;
	state=sigaction(SIGCHLD, &act, 0);
	serv_sock=socket(PF_INET, SOCK_STREAM, 0);
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port=htons(atoi(argv[1]));

	if(bind(serv_sock, (struct sockaddr*) &serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");
	if(listen(serv_sock, 5)==-1)
		error_handling("listen() error");

	while(1)
	{
		adr_sz=sizeof(clnt_adr);
		clnt_sock=accept(serv_sock, (struct sockaddr*)&clnt_adr, &adr_sz);
		if(clnt_sock==-1)
			continue;
		else
			puts("new client connected...");
		pid=fork();//调用accept函数后调用fork函数，父子进程分别带有一个accept创建的套接字的描述符
		if(pid==-1)
		{
			close(clnt_sock);//如果出错也要关闭子进程的客户端套接字描述符
			continue;
		}
		if(pid==0) //子进程负责向客户端提供服务
		{
			close(serv_sock);//关闭子进程中的服务器套接字的描述符（理由如下）
			while((str_len=read(clnt_sock, buf, BUF_SIZE))!=0)
				write(clnt_sock, buf, str_len);
            
			close(clnt_sock);//关闭子进程的客户端套接字描述符
			puts("client disconnected...");
			return 0;
		}
		else
			close(clnt_sock); //关闭父进程的客户端套接字描述符，只有这个套接字的所有描述符都关闭（销毁）这个套接字才能被关闭，如果只关闭子进程的描述符不关闭父进程的描述符，那这个套接字会一直存在
	}
	close(serv_sock);
	return 0;
}

void read_childproc(int sig)
{
	pid_t pid;
	int status;
	pid=waitpid(-1, &status, WNOHANG);
	printf("removed proc id: %d \n", pid);
}
void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211009124208882.png" alt="image-20211009124208882" style="zoom:67%;" />

1个套接字中存在2个文件描述符时，只有2个文件描述符都终止 (销毁)后， 才能销毁套接字。如果维持阁中的连接状态，即使子进程销毁了与客户端连接的套接字文件描述符，也无法完全销毁套接字(服务器端套接字同样如此) 因此，调用fork 函数后，要将无关的套接字文件描述符关掉

![image-20211009124250888](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211009124250888.png)

#### 分割TCP的I/O程序（将读/写。。功能由不同进程完成）



<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211009131550437.png" alt="image-20211009131550437" style="zoom:67%;" />

这么写的优点：简化代码，父进程只需编写接收数据的代码，子进程只需编写发送数据的代码。并且可以提高频繁交换数据的程序性能

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211009132008036.png" alt="image-20211009132008036" style="zoom:67%;" />



```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

#define BUF_SIZE 30
void error_handling(char *message);
void read_routine(int sock, char *buf);
void write_routine(int sock, char *buf);

int main(int argc, char *argv[])
{
	int sock;
	pid_t pid;
	char buf[BUF_SIZE];
	struct sockaddr_in serv_adr;
	if(argc!=3) {
		printf("Usage : %s <IP> <port>\n", argv[0]);
		exit(1);
	}

	sock=socket(PF_INET, SOCK_STREAM, 0);
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=inet_addr(argv[1]);
	serv_adr.sin_port=htons(atoi(argv[2]));

	if(connect(sock, (struct sockaddr*)&serv_adr, sizeof(serv_adr))==-1)
		error_handling("connect() error!");

	pid=fork();
	if(pid==0)
		write_routine(sock, buf);
	else
		read_routine(sock, buf);

	close(sock);
	return 0;
}

void read_routine(int sock, char *buf)
{
	while(1)
	{
		int str_len=read(sock, buf, BUF_SIZE);
		if(str_len==0)
			return;

		buf[str_len]=0;
		printf("Message from server: %s", buf);
	}
}
void write_routine(int sock, char *buf)
{
	while(1)
	{
		fgets(buf, BUF_SIZE, stdin);
		if(!strcmp(buf,"q\n") || !strcmp(buf,"Q\n"))
		{
			shutdown(sock, SHUT_WR);//调用 shutdown 函数向服务器端传递EOF 。当然，执行第63 行的return 语句后可以调用第39行的 close 函数传递EOF。但现在已通过第33 行的fork 函数调用复制了文件描述符，此时无法通过1次 close 函数调用传递EOF ，因此需要通过 shutdown函数调用另外传递，用shurdown是因为父进程只用了描述符中的输出流，所以只关闭输出流就好了，并且如果输出缓冲区中还有未传输的数据，则将传递到目标主机。防止传输数据丢失
			return;
		}
		write(sock, buf, strlen(buf));
	}
}
void error_handling(char *message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```

### 多线程服务器：通过生成与客户端等量的线程提供服务

多线程是多任务处理的一种特殊形式，多任务处理允许让电脑同时运行两个或两个以上的程序。一般情况下，两种类型的多任务处理：**基于进程和基于线程**。

- 基于进程的多任务处理是程序的并发执行。
- 基于线程的多任务处理是同一程序的片段的并发执行。

多线程程序包含可以同时运行的两个或多个部分。这样的程序中的每个部分称为一个线程，每个线程定义了一个单独的执行路径。

优缺点：

　　线程执行开销小，但是不利于资源的管理和保护。线程适合在SMP机器（双CPU系统）上运行。

　　进程执行开销大，但是能够很好的进行资源管理和保护。进程可以跨机器前移。

何时使用多进程，何时使用多线程？

对资源的管理和保护要求高，不限制开销和效率时，使用多进程。

要求效率高，频繁切换时，资源的保护管理要求不是很高时，使用多线程。

比如说，对于计算密集型的任务，多进程效率会更高一下；而对于IO密集型的任务（比如文件操作，网络爬虫），采用多线程编程效率更高。

![img](http://c.biancheng.net/uploads/allimg/190814/2-1ZQ4123404U9.gif)

#### 线程函数

```c++
#include <pthread.h>
int pthread_create (pthread_t *thread,const pthread_attr_t* attr, void*(* start_routine)(void *),void * restrict arg) //成功时返回0
thread	指向线程标识符指针。保存新创建线程ID的变量地址值
attr	用于传递线程属性的参数，可以指定线程属性对象，也可以使用默认值 NULL。
start_routine	线程运行函数起始地址，一旦线程被创建就会执行。相当于线程的main函数，在单独执行流中执行的函数指针
arg	运行函数的参数。它必须通过把引用作为指针强制转换为 void 类型进行传递。如果没有传递参数，则使用 NULL。
    
#include <pthread.h>
pthread_exit (status)  //pthread_exit 用于显式地退出一个线程。通常情况下，pthread_exit() 函数是在线程完成工作后无需继续存在时被调用。
//如果 main() 是在它所创建的线程之前结束，并通过 pthread_exit() 退出，那么其他线程将继续执行。否则，它们将在 main() 结束时自动被终止。
```



```c++
#include <iostream>
// 必须的头文件
#include <pthread.h>
 
using namespace std;
 
#define NUM_THREADS 5
 
// 线程的运行函数
void* say_hello(void* args)
{
    cout << "Hello Runoob！" << endl;
    return 0;
}
 
int main()
{
    // 定义线程的 id 变量，多个变量使用数组
    pthread_t tids[NUM_THREADS];
    for(int i = 0; i < NUM_THREADS; ++i)
    {
        //参数依次是：创建的线程id，线程参数，调用的函数，传入的函数参数
        int ret = pthread_create(&tids[i], NULL, say_hello, NULL);
        if (ret != 0)
        {
           cout << "pthread_create error: error_code=" << ret << endl;
        }
    }
    //等各个线程退出后，进程才结束，否则进程强制结束了，线程可能还没反应过来；
    pthread_exit(NULL);
}
$ ./test.o
Hello Runoob！
Hello Runoob！
Hello Runoob！
Hello Runoob！
Hello Runoob！
```

```c++
#include <stdio.h>
#include <pthread.h>
void* thread_main(void *arg);

int main(int argc, char *argv[]) 
{
	pthread_t t_id; //定义线程的id
	int thread_param=5;
	//创建的线程id，线程参数，调用的函数，传入的函数参数
	if(pthread_create(&t_id, NULL, thread_main, (void*)&thread_param)!=0)
	{
		puts("pthread_create() error");
		return -1;
	}; 	
	sleep(10);  puts("end of main");
	return 0;
}

// 线程的运行函数
void* thread_main(void *arg)  //arg为pthread_create的最后一个参数
{
	int i;
	int cnt=*((int*)arg);
	for(i=0; i<cnt; i++)
	{
		sleep(1);  puts("running thread");	 
	}
	return NULL;
}
running thread
running thread
running thread
running thread
running thread
end of main
```



**连接和分离线程**

```c++
int pthread_join (pthread_t threadid,void* status) //成功时返回0，调用该函数的进程（线程）将进入等待状态，直到threadid的线程终止。并返回线程的main函数返回值
threadid:该id的线程终止后才会从该函数返回
status:保存线程的main函数返回值的指针变量地址
int pthread_detach (threadid) 
```



```c++
#include <iostream>
#include <cstdlib>
#include <pthread.h>
#include <unistd.h>
 
using namespace std;
 
#define NUM_THREADS     5
 
void *wait(void *t)
{
   int i;
   long tid;
 
   tid = (long)t;
 
   sleep(1);
   cout << "Sleeping in thread " << endl;
   cout << "Thread with id : " << tid << "  ...exiting " << endl;
   pthread_exit(NULL);
}
 
int main ()
{
   int rc;
   int i;
   pthread_t threads[NUM_THREADS];
   pthread_attr_t attr;
   void *status;
 
   // 初始化并设置线程为可连接的（joinable）
   pthread_attr_init(&attr);
   pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);
 
   for( i=0; i < NUM_THREADS; i++ ){
      cout << "main() : creating thread, " << i << endl;
      rc = pthread_create(&threads[i], NULL, wait, (void *)&i );
      if (rc){
         cout << "Error:unable to create thread," << rc << endl;
         exit(-1);
      }
   }
 
   // 删除属性，并等待其他线程
   pthread_attr_destroy(&attr);
   for( i=0; i < NUM_THREADS; i++ ){
      rc = pthread_join(threads[i], &status);
      if (rc){
         cout << "Error:unable to join," << rc << endl;
         exit(-1);
      }
      cout << "Main: completed thread id :" << i ;
      cout << "  exiting with status :" << status << endl;
   }
 
   cout << "Main: program exiting." << endl;
   pthread_exit(NULL);
}

main() : creating thread, 0
main() : creating thread, 1
main() : creating thread, 2
main() : creating thread, 3
main() : creating thread, 4
Sleeping in thread 
Thread with id : 4  ...exiting 
Sleeping in thread 
Thread with id : 3  ...exiting 
Sleeping in thread 
Thread with id : 2  ...exiting 
Sleeping in thread 
Thread with id : 1  ...exiting 
Sleeping in thread 
Thread with id : 0  ...exiting 
Main: completed thread id :0  exiting with status :0
Main: completed thread id :1  exiting with status :0
Main: completed thread id :2  exiting with status :0
Main: completed thread id :3  exiting with status :0
Main: completed thread id :4  exiting with status :0
Main: program exiting.
```

##### Linux下线程属性

```c++
　　typedef struct{

　　　　　　int                                     detachstate;     //线程的分离状态，flag：PTHREAD_CREATE-JOINABLE（默认）和PTHREAD_CREATE_DETACH（可以使用pthread_detach将线程设置为脱离线程）。前者指定的线程是可以被回收的，后者调用线程脱离与进程中其他线程的同步。脱离了与其他线程同步的线程被称为“脱离线程”，脱离线程在退出时将自行释放其占用的系统资源。

　　　　　　int                                     schedpolicy;    //线程调度策略

　　　　　　struct sched_param         schedparam; //线程的调度参数（策略）。flag：SCHED_FIFO(采用先先进先出算法调度),SCHED_RR（采用轮转算法调度）,SCHED_OTHER(默认)

　　　　　　int                                     inheritsched; //线程的继承性，是否继承调用线程的调度属性。flag：PTHREAD_INHERIT_SCHED,PTHREAD_EXPLICIT_SCHED。前者表示新线程沿用其创建者的线程调度参数（新线程的调度属性不能再改变）。flag为后者时，调用者需要明确指定新线程的调度参数

　　　　　　int                                     scope;              //线程的作用域，线程优先级的有效范围。flag：PTHREAD_SCOPE_SYSTEM,PTHREAD_SCOPE_PROCESS.前者表示目标线程与系统中所有线程一起竞争CPU的使用（优先级会和所有线程比较）。后者表示目标线程只与同一进程下的线程竞争。

　　　　　　size_t                                guardsize;       //线程栈末尾的警戒缓冲区大小，用来防止线程堆栈被错误覆盖。>0时，会在线程的尾部额外分配guardsize字节的空间。

　　　　　　int                                     stackaddr_set; //线程的栈设置

　　　　　　void*                                 stackaddr;       //线程栈的位置（最低地址/起始地址）

　　　　　　size_t                                stacksize;        //线程栈的大小，(一般创建时默认是8mb)。可以用ulimt -s来查看或修改这个默认值

　　　　　　} pthread_attr_t; 
```

其实对于cpu调度而言，操作系统调度的对象实际上是 /proc/$pid/task 目录的对象，线程和进程的区别无非是地址空间是独立的还是共享的，因此内核会为每个 task 对象创建一个 task_struct 结构体,这个结构体叫进程描述符。这些 task 对象拿到 cpu 时间片后，只有在时间片使用完、IO 阻塞、亦或者产生硬中断等外部条件时，才会暂停运行，也就是说线程是不会主动让出 cpu 时间片的，他们之间属于竞争关系

> 从 Linux 内核的角度看，它使用轻量级进程对多线程应用提供支持，其实它的创建也是基于fork()系统调用，只是在进程描述符的初始化当中有所区别。首先，轻量级进程也是一个进程，它有它自己的pid，有它自己的内核栈和进程描述符，甚至还有它自己的调度策略，而轻量级进程和普通进程不同的就是它没有自己的进程地址空间，并且要响应线程组内其他线程接收到的信号(但可以通过修改信号屏蔽字屏蔽某些信号)。轻量级进程使用的是父进程的内存地址空间，也就是在task_struct结构中的内存指针指向父进程的内存地址。而信号描述符指针会指向父进程指向的地址。而在应用层，线程有自己的栈
>
> 轻量级进程和普通进程区别：
>
> - 没有自己的进程地址空间，使用父进程的进程地址空间
> - 与组内所有进程共享信号，但有自己的信号屏蔽字




**线程的属性不能直接设置，需要使用相关函数进行操作**

```

```



#### **线程的注意事项**

**线程共享的资源有：**

a. 堆  由于堆是在进程空间中开辟出来的，所以它是理所当然地被共享的；因此new出来的都是共享的（16位平台上分全局堆和局部堆，局部堆是独享的）

b. 全局变量 它是与具体某一函数无关的，所以也与特定线程无关；因此也是共享的

c. 静态变量 虽然对于局部变量来说，它在代码中是“放”在某一函数中的，但是其存放位置和全局变量一样，存于堆中开辟的.bss和.data段，是共享的

d. 文件等公用资源  这个是共享的，使用这些公共资源的线程必须同步。Win32 提供了几种同步资源的方式，包括信号、临界区、事件和互斥体。

**线程独享的资源有**

a. 栈   线程运行的本质就是函数运行，函数运行时信息是保存在栈帧中的，因此每个线程都有自己独立的、私有的栈区。（但是因为不像进程地址空间之间的严格隔离，线程的栈区没有严格的隔离机制来保护，因此如果一个线程能拿到来自另一个线程栈帧上的指针，**那么该线程就可以改变另一个线程的栈区**，也就是说这些线程可以任意修改本属于另一个线程栈区中的变量。编写程序时应当避免这种情况）

b. 寄存器  这个可能会误解，因为电脑的寄存器是物理的，每个线程去取值难道不一样吗？其实线程里存放的是副本，包括程序计数器PC，部分局部变量等等。**一个线程不可能访问到另一个线程的这类寄存器信息**。

c.   以上所属线程的栈区、程序计数器、栈指针以及函数运行使用的寄存器。。统称为线程上下文信息。**线程上下文信息**使得操作系统可以随时中断线程的运行并且需要线程被暂停后可以继续运行。



**线程局部存储技术，Thread Local Storage，TLS。**

__thread使用规则：只能用于修饰POD类型，不能修饰class类型， 因为无法自动调用构造函数和析构函数。__thread可以用于修饰全局变 量、函数内的静态变量，但是不能用于修饰函数的局部变量或者class的 普通成员变量。另外，__thread变量的初始化只能用编译期常量。

__thread变量是每个线程有一份独立实体，各个线程的变量值互不 干扰。除了这个主要用途，它还可以修饰那些“**值可能会变，带有全局 性，但是又不值得用全局锁保护**”的变量。

```c++
__thread int a = 1; // 线程局部存储
void print_a() {
    cout<<a<<endl;
}

void run() {
    ++a;
    print_a();
}

void main() {
    thread t1(run);
    t1.join();

    thread t2(run);
    t2.join();
}

2
2
线程t1对变量a的修改不会影响到线程t2，线程t1在将变量a加到1后变为2，但对于线程t2来说此时变量a依然是1，因此加1后依然是2。

因此，线程局部存储可以让你使用一个独属于线程的全局变量。也就是说，虽然该变量可以被所有线程访问，但该变量在每个线程中都有一个副本，一个线程对改变量的修改不会影响到其它线程。
```

**线程共享的环境包括：**

进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。

 

进程拥有这许多共性的同时，还拥有自己的个性。有了这些个性，线程才能实现并发性。

**这些个性包括：**

1.线程ID
　　每个线程都有自己的线程ID，这个ID在本进程中是唯一的。进程用此来标识线程。

 

2.寄存器组的值
　　由于线程间是并发运行的，每个线程有自己不同的运行线索，当从一个线 程切换到另一个线程上 时，必须将原有的线程的寄存器集合的状态保存，以便将来该线程在被重新切换到时能得以恢复。

 

3.线程的堆栈
　　堆栈是保证线程独立运行所必须的。线程函数可以调用函数，而被调用函数中又是可以层层嵌套的，所以线程必须拥有自己的函数堆栈， 使得函数调用可以正常执行，不受其他线程的影响。

 

4.错误返回码
   由于同一个进程中有很多个线程在同时运行，可能某个线程进行系统调用 后设置了errno值，而在该 线程还没有处理这个错误，另外一个线程就在此时被调度器投入运行，这样错误值就有可能被修改。所以不同线程应该有自己的错误返回码变量。

 

5.线程的信号屏蔽码
　　由于每个线程所感兴趣的信号不同，所以线程的信号屏蔽码应该由线程自己管理。但所有的线程都 共享同样的信号处理器。

6.线程的优先级
　　由于线程需要像进程那样能够被调度，那么就必须要有可供调度使用的参数，这个参数就是线程的 优先级。



#### 线程同步

##### 线程使用注意事项和线程安全（可重入函数）

- 1．首要原则是尽量最低限度地共享对象，减少需要同步的场合。 一个对象能不暴露给别的线程就不要暴露；如果要暴露，优先考虑 immutable对象；实在不行才暴露可修改的对象，并用同步措施来充分 保护它。
- 2．其次是使用高级的并发编程构件，如TaskQueue、ProducerConsumer Queue、CountDownLatch等等。 
- 3．最后不得已必须使用底层同步原语（primitives）时，只用非递 归的互斥器和条件变量，慎用读写锁，不要用信号量。 
- 4．除了使用atomic整数之外，不自己编写lock-free代码，也不要 用“内核级”同步原语。不凭空猜测“哪种做法性能会更好”，比如spin lock vs. mutex。

此外，编写高性能多线程程序至少还要知道false sharing和CPU cache效应，可以参阅：
https://www.aristeia.com/TalkNotes/ACCU2011_CPUCaches.pdf
http://igoro.com/archive/gallery-of-processor-cache-effects/
http://simplygenius.net/Article/FalseSharing

**我个人遵循的编写多线程C++程序的原则如下**： 

- 线程是宝贵的，一个程序可以使用几个或十几个线程。一台机器 上不应该同时运行几百个、几千个用户线程，这会大大增加内核 scheduler的负担，降低整体性能。 
- 线程的创建和销毁是有代价的，一个程序最好在一开始创建所需 的线程，并一直反复使用。不要在运行期间反复创建、销毁线程，如果 必须这么做，其频度最好能降到1分钟1次（或更低）。 
- 每个线程应该有明确的职责，例如IO线程（运行 EventLoop::loop()，处理IO事件）、计算线程（位于ThreadPool中，负责 计算）等等。 
- 线程之间的交互应该尽量简单，理想情况下，线程之间只用消息 传递（例如BlockingQueue）方式交互。如果必须用锁，那么最好避免 一个线程同时持有两把或更多的锁，这样可彻底防止死锁。 
- 要预先考虑清楚一个mutable shared对象将会暴露给哪些线程，每 个线程是读还是写，读写有无可能并发进行。

**因为线程共享进程的共有数据等等，因此在多个线程对共有数据同时读写时会出错**

例：

```c++
int a = 1; // 共享全局变量
void print_a() {
    cout<<a<<endl;
}

void run1() {
    ++a;
    print_a();
}

void run2() {
    ++a;
    print_a();
}

void main() {
    thread t1(run1);
    thread t2(run2);
}
可能出现一下情况：
t1读取变量a，并将a的值传递到CPU进行+1运算，但是在t1把值写入a前，t2就完成了运算并写入a。也就是说，进程调用线程进行两次+运算，a的值却只加了一次！！！

```

出现这种情况的根本原因是C++中的许多操作并不是原子的，一个线程尚未执行完成时另一个线程可以在中间加塞

**为了解决这些问题，我们使用线程同步**

**线程同步的四种方法**

##### 互斥量Mutex（互斥锁）

使用注意事项（来自陈硕）

- 用RAII手法封装mutex的创建、销毁、加锁、解锁这四个操作。保证锁 的生效期间等于一个作用域（scope),且不会因异常而忘记解锁。
- 只用非递归的mutex（即不可重入的mutex）。
- 不手工调用lock()和unlock()函数，一切交给栈上的Guard对象的构 造和析构函数负责。Guard对象的生命期正好等于临界区（分析对象在 什么时候析构是C++程序员的基本功）。这样我们保证始终在同一个函 数同一个scope里对某个mutex加锁和解锁。避免在foo()里加锁，然后跑 到bar()里解锁；也避免在不同的语句分支中分别加锁、解锁。这种做法 被称为Scoped Locking 。
- 在每次构造Guard对象的时候，思考一路上（调用栈上）已经持有 的锁，防止因加锁顺序不同而导致死锁（deadlock）。由于Guard对象是 栈上对象，看函数调用栈(gdb中使用thread apply all bt命令。)就能分析用锁的情况，非常便利。
- 尽量用非递归锁。即使它造成了bug也能很快排查出来（查看函数调用栈分析用锁的情况，出现死锁的时候很容易定位）

mutex分为**递归（recursive）**和**非递归（non-recursive）**两种，这是 POSIX的叫法，另外的名字是可重入（reentrant）与非可重入。这两种 mutex作为线程间（inter-thread）的同步工具时没有区别，它们的唯一区 别在于：同一个线程可以重复对recursive mutex加锁，但是不能重复对 non-recursive mutex加锁。(在同一个线程里多次对non-recursive mutex加锁会 立刻导致死锁)

**注：** Linux的Pthreads mutex采用futex实现，不必每次加 锁、解锁都陷入系统调用，效率不错。Windows的CRITICAL_SECTION 也是类似的，不过它可以嵌入一小段spin lock。在多CPU系统上，如果 不能立刻拿到锁，它会先spin一小段时间，如果还不能拿到锁，才挂起 当前线程。

互斥锁主要用于实现内核中的互斥访问功能。内核互斥锁是在原子 API 之上实现的，但这对于内核用户是不可见的。对它的访问必须遵循一些规则：同一时间只能有一个任务持有互斥锁，而且只有这个任务可以对互斥锁进行解锁。互斥锁不能进行递归锁定或解锁。一个互斥锁对象必须通过其API初始化，而不能使用memset或复制初始化。一个任务在持有互斥锁的时候是不能结束的。互斥锁所使用的内存区域是不能被释放的。使用中的互斥锁是不能被重新初始化的。并且互斥锁不能用于中断上下文。但是互斥锁比当前的内核信号量选项更快，并且更加紧凑

```c++
/linux/include/linux/mutex.h
struct mutex {
        /* 1: unlocked, 0: locked, negative: locked, possible waiters */
       atomic_t                count;  //互斥锁状态，1没有上锁，0被锁定
       spinlock_t              wait_lock; //等待获取互斥锁中使用的自旋锁。
        struct list_head        wait_list; //等待互斥锁的队列
#ifdef CONFIG_DEBUG_MUTEXES
        struct thread_info      *owner;
        const char              *name;
       void                    *magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
        struct lockdep_map      dep_map;
#endif
};
```

(1)具体参见linux/kernel/mutex.c
void inline fastcall sched mutex_lock(struct mutex *lock);
获取互斥锁。实际上是先给count做自减操作，然后使用本身的自旋锁进入临界区操作。首先取得count的值，在将count置为－1，判断如果原来count的置为1，也即互斥锁可以获得，则直接获取，跳出。否则进入循环反复测试互斥锁的状态。在循环中，也是先取得互斥锁原来的状态，在将其之为－1，判断如果可以获取(等于1)，则退出循环，否则设置当前进程的状态为不可中断状态，解锁自身的自旋锁，进入睡眠状态，待被在调度唤醒时，再获得自身的自旋锁，进入新一次的查询其自身状态(该互斥锁的状态)的循环。
(2)具体参见linux/kernel/mutex.c
int fastcall sched mutex_lock_interruptible(struct mutex *lock)；
和mutex_lock()一样，也是获取互斥锁。在获得了互斥锁或进入睡眠直到获得互斥锁之后会返回0。如果在等待获取锁的时候进入睡眠状态收到一个信号(被信号打断睡眠)，则返回_EINIR。
(3)具体参见linux/kernel/mutex.c
int fastcall __sched mutex_trylock(struct mutex *lock);
试图获取互斥锁，如果成功获取则返回1，否则返回0，不等待。
3、释放互斥锁：
具体参见linux/kernel/mutex.c
void fastcall mutex_unlock(struct mutex *lock);
释放被当前进程获取的互斥锁。该函数不能用在中断上下文中，而且不允许去释放一个没有上锁的互斥锁。



##### **条件变量Condition variable**

互斥器（mutex）是加锁原语，用来排他性地访问共享数据，它不 是等待原语。在使用mutex的时候，我们一般都会期望加锁不要阻塞， 总是能立刻拿到锁。然后尽快访问数据，用完之后尽快解锁，这样才能 不影响并发性和性能。 如果需要**等待某个条件成立**，我们应该使用条件变量（condition variable）。条件变量顾名思义是一个或多个线程等待某个布尔表达式为真，即等待别的线程“唤醒”它。条件变量的学名叫管程（monitor）。 Java Object内置的wait()、notify()、notifyAll()是条件变量。 条件变量只有一种正确使用的方式，几乎不可能用错。

对于wait 端： 

- 1．必须与mutex一起使用，该**布尔表达式的读写需受此mutex**保 护。 
- 2．在mutex**已上锁**的时候才能调用wait()。 
- 3．把判断布尔条件和wait()放到**while循环**中。

**例子：**

- 下面的代码（wait端）必须用while循环来等待条件变量，**而不能使用if语句，原因是spurious wakeup**（参阅：http://en.wikipedia.org/wiki/Spurious_wakeup），这也是面试多线程编程的常见考点
- 虚假唤醒：指的是即便我们**没有 signal 相关的条件变量**(即没有调用 pthread_cond_signal), 等待(调用了 pthread_cond_wait)的**线程也可能被(虚假)唤醒**,（ **即使没有线程\**broadcast 或者signal条件变量，\**wait也可能偶尔返回。这是多核处理器底层实现的问题**）此时我们必须**重新检查对应的标记值**(以确认是否发生了(虚假)唤醒),又由于(虚假)唤醒可能会发生多次,所以我们最终需要使用循环来进行标记值检查.

因为在多核处理器下，pthread_cond_signal可能会激活多于一个线程（阻塞在条件变量上的线程）。结果是，当一个线程调用pthread_cond_signal()后，多个调用pthread_cond_wait()或pthread_cond_timedwait()的线程返回。



```c++
muduo::MutexLock mutex;
muduo Condition cond(mutex);
std::deque<int> queue;
 
//出队列
int dequeue()
{
    MutexLockGuard lock(mutex);
 
    //必须使用循环，在判断之后再wait()，如果队列为空，它会等待有元素入队（cond.notify()）
    while (queue.empty())
    {
        //这一步原子地unlock mutex并进入等待，不会与enqueue()函数死锁
        cond.wait();  
        //wait()执行完毕之后自动重新加锁
    }
    
    //下面是出队列的操作
    assert(!queue.empty());
    int top = queue.front();
    queue.pop_front();
    return top;
}
```

对于signal/broadcast端： 

- 1．不一定要在mutex已上锁的情况下调用signal（理论上）。 
- 2．在signal之前一般要修改布尔表达式。 
- 3．修改布尔表达式通常要用mutex保护（至少用作full memory barrier）。 
- 4．注意区分signal与broadcast：“broadcast通常用于表明状态变化， signal通常用于表示资源可用。

**例：**

signal/broadcast端示例代码：

```c++
muduo::MutexLock mutex;
muduo Condition cond(mutex);
std::deque<int> queue;
 
//入队列
void enqueue(int x)
{
    MutexLockGuard lock(mutex);
    queue.push_back(x);
    cond.notify(); //可以移出临界区之外
}
```



###### 条件变量的一些用法：

倒计时（CountDownLatch）是一种常用且易用的同步手段。它主 要有两种用途： 

- 主线程发起多个子线程，等这些子线程各自都完成一定的任务之 后，主线程才继续执行。通常用于主线程等待多个子线程完成初始化。 
- 主线程发起多个子线程，子线程都等待主线程，主线程完成其他 一些任务之后通知所有子线程开始执行。通常用于多个子线程等待主线 程发出“起跑”命令。

```c++
class CountDownLatch :boost::noncopyable
{
public:
    explicit CountDownLatch(int count);
    void wait();       //等待计数值变为0
    void countDown(); //计数减1
private:
    mutable MutexLock mutex_; //顺序很重要，先mutex后condition
    Condition condition_;
    int count_;
};
 
void CountDownLatch::wait()
{
    MutexLockGuard lock(mutex_);
    while (count_ > 0)
        condition_.wait();
}
 
void CountDownLatch::countDown()
{
    MutexLockGuard lock(mutex_);
    --count_;
    if (count_ == 0)
        condition_.notifyAll();		//唤醒在该对象上等待的所有线程
}
```

**java中的notif和notifAll：**

- 等待池：假设一个线程A调用了某个对象的wait()方法，线程A就会释放该对象的锁后，进入到了该对象的等待池，等待池中的线程不会去竞争该对象的锁。

wait是要释放对象锁，进入等待池。
既然是释放对象锁，那么肯定是先要获得锁。
所以wait必须要写在synchronized代码块中，否则会报异常。

- 锁池：只有获取了对象的锁，线程才能执行对象的 synchronized 代码，对象的锁每次只有一个线程可以获得，其他线程只能在锁池中等待

也需要写在synchronized代码块中,调用对象的这两个方法也需要先获得该对象的锁.
 notify,notifyAll, 唤醒等待该对象同步锁的线程,并放入该对象的锁池中. 对象的锁池中线程可以去竞争得到对象锁,然后开始执行.
 如果是通过notify来唤起的线程,那进入wait的线程会被随机唤醒;**实际上, hotspot是顺序唤醒的**

![img](https://upload-images.jianshu.io/upload_images/1246351-61c7cf4ea2f58715.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

如果是通过notifyAll唤起的线程,默认情况是最后进入的会先被唤起来,即LIFO的策略;

notify() 方法随机唤醒对象的等待池中的一个线程，进入锁池；notifyAll() 唤醒对象的等待池中的所有线程，进入锁池。



**c++中的条件变量**

notify_one()：因为只唤醒等待队列中的第一个线程；不存在锁争用，所以能够立即获得锁。其余的线程不会被唤醒，需要等待再次调用notify_one()或者notify_all()。

notify_all()：会唤醒所有等待队列中阻塞的线程，存在锁争用，只有一个线程能够获得锁。那其余未获取锁的线程接着会怎么样？会阻塞？还是继续尝试获得锁？

- 答案是会继续尝试获得锁(类似于轮询)，而不会再次阻塞。当持有锁的线程释放锁时，这些线程中的一个会获得锁。而其余的会接着尝试获得锁。



##### 信号量**Semaphore**

没啥用

除了[RWC]指出 的“semaphore has no notion of ownership”之外，信号量的另一个问题在 于它有自己的计数值，而通常我们自己的数据结构也有长度值，这就造 成了同样的信息存了两份，需要时刻保持一致		--------陈

```c++

```



##### **高级同步原语：future and async/packaged_task/promise**

```c++

```



#### 多线程并发服务器

**聊天服务器端（将一个客户端发送的消息传递给所有客户端）**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <pthread.h>

#define BUF_SIZE 100
#define MAX_CLNT 256

void * handle_clnt(void * arg);
void send_msg(char * msg, int len);
void error_handling(char * msg);

int clnt_cnt=0;            //建立连接的套接字数量
int clnt_socks[MAX_CLNT];  //管理接入的客户端套接字数组
                           //访问这两个变量的代码就是临界区，线程在读写他们时用互斥锁来保证数据正确
char *inet_ntoa(struct in_addr adr);
pthread_mutex_t mutx;

int main(int argc, char *argv[])
{
	int serv_sock, clnt_sock;
	struct sockaddr_in serv_adr, clnt_adr;
	int clnt_adr_sz;
	pthread_t t_id;
	if(argc!=2) {
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}

	pthread_mutex_init(&mutx, NULL);
	serv_sock=socket(PF_INET, SOCK_STREAM, 0);

	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port=htons(atoi(argv[1]));

	if(bind(serv_sock, (struct sockaddr*) &serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");
	if(listen(serv_sock, 5)==-1)
		error_handling("listen() error");

	while(1)
	{
		clnt_adr_sz=sizeof(clnt_adr);
		clnt_sock=accept(serv_sock, (struct sockaddr*)&clnt_adr,&clnt_adr_sz);

		pthread_mutex_lock(&mutx);         //互斥锁来保护临界区
		clnt_socks[clnt_cnt++]=clnt_sock;  //有新连接时，将他加入clnt_socks数组
		pthread_mutex_unlock(&mutx);

		pthread_create(&t_id, NULL, handle_clnt, (void*)&clnt_sock); //多线程所以调用handle_clnt的read只阻塞这个线程，不会阻塞整个进程。while一直循环监听是否新的连接
		pthread_detach(t_id);  //完全销毁内存中已终止的线程，创建线程后记得销毁线程，否则会造成内存泄漏
		printf("Connected client IP: %s \n", inet_ntoa(clnt_adr.sin_addr));
	}
	close(serv_sock);
	return 0;
}

void * handle_clnt(void * arg)
{
	int clnt_sock=*((int*)arg);
	int str_len=0, i;
	char msg[BUF_SIZE];
    
	//std::cout << "tid=" << std::this_thread::get_id() << std::endl;  //输出线程id，看看如果没有新连接，主进程是否会每次循环都创建一次线程id
    //printf("thread: %5u\n", gettid());

	while((str_len=read(clnt_sock, msg, sizeof(msg)))!=0)
		send_msg(msg, str_len);   //发送数据

	pthread_mutex_lock(&mutx);  //互斥锁来保护临界区
	for(i=0; i<clnt_cnt; i++)   // 消除关闭连接的套接字
	{
		if(clnt_sock==clnt_socks[i])
		{
			while(i++<clnt_cnt-1)
				clnt_socks[i]=clnt_socks[i+1];
			break;
		}
	}
	clnt_cnt--;
	pthread_mutex_unlock(&mutx);
	close(clnt_sock);
	return NULL;
}
void send_msg(char * msg, int len)   // 给所有连接的客户端传送数据
{
	int i;
	pthread_mutex_lock(&mutx);
	for(i=0; i<clnt_cnt; i++)
		write(clnt_socks[i], msg, len);
	pthread_mutex_unlock(&mutx);
}
void error_handling(char * msg)
{
	fputs(msg, stderr);
	fputc('\n', stderr);
	exit(1);
}

```



**聊天客户端**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h> 
#include <string.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <pthread.h>
	
#define BUF_SIZE 100
#define NAME_SIZE 20
	
void * send_msg(void * arg);
void * recv_msg(void * arg);
void error_handling(char * msg);
	
char name[NAME_SIZE]="[DEFAULT]";  //起的名字
char msg[BUF_SIZE];
	
int main(int argc, char *argv[])
{
	int sock;
	struct sockaddr_in serv_addr;
	pthread_t snd_thread, rcv_thread;
	void * thread_return;
	if(argc!=4) {
		printf("Usage : %s <IP> <port> <name>\n", argv[0]);
		exit(1);
	 }
	
	sprintf(name, "[%s]", argv[3]);
	sock=socket(PF_INET, SOCK_STREAM, 0);
	
	memset(&serv_addr, 0, sizeof(serv_addr));
	serv_addr.sin_family=AF_INET;
	serv_addr.sin_addr.s_addr=inet_addr(argv[1]);
	serv_addr.sin_port=htons(atoi(argv[2]));
	  
	if(connect(sock, (struct sockaddr*)&serv_addr, sizeof(serv_addr))==-1)
		error_handling("connect() error");
	
	pthread_create(&snd_thread, NULL, send_msg, (void*)&sock); //发送消息
	pthread_create(&rcv_thread, NULL, recv_msg, (void*)&sock); //接收消息
	pthread_join(snd_thread, &thread_return); //释放内存
	pthread_join(rcv_thread, &thread_return);
	close(sock);  
	return 0;
}
	
void * send_msg(void * arg)   // send thread main
{
	int sock=*((int*)arg);
	char name_msg[NAME_SIZE+BUF_SIZE];
	while(1) 
	{
		fgets(msg, BUF_SIZE, stdin);
		if(!strcmp(msg,"q\n")||!strcmp(msg,"Q\n")) 
		{
			close(sock);
			exit(0);
		}
		sprintf(name_msg,"%s %s", name, msg);
		write(sock, name_msg, strlen(name_msg));
	}
	return NULL;
}
	
void * recv_msg(void * arg)   // read thread main
{
	int sock=*((int*)arg);
	char name_msg[NAME_SIZE+BUF_SIZE];
	int str_len;
	while(1)
	{
		str_len=read(sock, name_msg, NAME_SIZE+BUF_SIZE-1);
		if(str_len==-1) 
			return (void*)-1;
		name_msg[str_len]=0;
		fputs(name_msg, stdout);
	}
	return NULL;
}
	
void error_handling(char *msg)
{
	fputs(msg, stderr);
	fputc('\n', stderr);
	exit(1);
}

```



### 线程池

#### Linux线程库：NPTL

NPTL特点：

- 取消管理线程。终止线程，回收线程堆栈等工作都可以由不同内核完成，所以一个进程的线程可以运行在不同的cpu上，从而充分利用了多处理器系统的优势

- 增加了futex实现同步。线程的同步由内核来完成。隶属于不同进程的线程之间也能共享互斥锁，由此可以实现跨进程的线程同步

- 取消管理线程的好处：

  1）不使用管理线程，线程的创建/终止，发送信号操作均有内核负责，另外内核还负责回收线程的栈空间，在线程终止的时候，会让父线程等待哦其所有孩子终止。

  2）因为不适用管理线程，管理线程就不成为瓶颈，创建和终止效率更高，且减少了很多线程切换。

  3）不使用管理线程，就不需要管理线程作为中介传递信号，在多处理器上可以更加方便的调度、通信。

- futex带来的好处：

  futex成为快速互斥，用于线程之间的同步。LInuxThreads模型使用信号机制，该机制下势必涉及线程的睡眠和唤醒，而睡眠和唤醒操作都是在内核完成的。futex实现了在用户空间操作资源，有内核实现仲裁。可以显著提高同步效率。

  除此之外，信号处理可以基于进程，比如发送终止信号，可以直接针对进程发送，这样进程中的所有线程均收到信号。而对于信号的传递，进程可用线程可实现接收，而之前如果线程阻塞了，那么信号就pending了。

#### 半同步/半异步进程池

```c++
#ifndef PROCESSPOOL_H
#define PROCESSPOOL_H

#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <sys/epoll.h>
#include <signal.h>
#include <sys/wait.h>
#include <sys/stat.h>

class process  //一个子进程对象
{
public:
    process() : m_pid( -1 ){}

public:
    pid_t m_pid;  //子进程的ID
    int m_pipefd[2]; //父子进程通信的管道
};

template< typename T >
class processpool  //进程池
{
private:
    processpool( int listenfd, int process_number = 8 );  //为了保证只存在一个父进程，所以用私有构造函数；在static公有函数中判断是否已创建，那样不论调用几次构造函数父进程都只有一个，被创建一次
public:
    static processpool< T >* create( int listenfd, int process_number = 8 )	//创建一个对象并返回其指针
    {
        if( !m_instance )
        {
            m_instance = new processpool< T >( listenfd, process_number );  //创建一个对象
        }
        return m_instance;  //
    }
    ~processpool()  //
    {
        delete [] m_sub_process;
    }
    void run();

private:
    void setup_sig_pipe();
    void run_parent();
    void run_child();

private:
    static const int MAX_PROCESS_NUMBER = 16;  //进程池的子进程容量
    static const int USER_PER_PROCESS = 65536; //每个子进程最多能处理的客户数量
    static const int MAX_EVENT_NUMBER = 10000; //epoll最多能处理的事件数
    int m_process_number;  //进程池中的进程总数
    int m_idx;		//子进程在池中的序号，从0开始（父进程m_id=-1）
    int m_epollfd;	//每个进程都有一个epoll
    int m_listenfd; //监听的socket
    int m_stop;		//子进程通过m_stop来判断是否停止运行
    process* m_sub_process;  //保存所有子进程的描述信息
    static processpool< T >* m_instance;  //进程池静态实例
};
template< typename T >
processpool< T >* processpool< T >::m_instance = NULL;

static int sig_pipefd[2];  //信号管道

static int setnonblocking( int fd )   //设置文件描述符为非阻塞
{
    int old_option = fcntl( fd, F_GETFL ); //获取文件描述符符标志
    int new_option = old_option | O_NONBLOCK;  //添加非阻塞标志
    fcntl( fd, F_SETFL, new_option );  //设置文件描述符新标志
    return old_option;
}

static void addfd( int epollfd, int fd )  //epoll添加监听fd
{
    epoll_event event;
    event.data.fd = fd;
    event.events = EPOLLIN | EPOLLET;
    epoll_ctl( epollfd, EPOLL_CTL_ADD, fd, &event );
    setnonblocking( fd );
}

static void removefd( int epollfd, int fd )  //epoll移除fd
{
    epoll_ctl( epollfd, EPOLL_CTL_DEL, fd, 0 );
    close( fd );
}

static void sig_handler( int sig )  //满足信号时（SIGCHLD,SIGTERM,SIGINT）调用的函数，发送信号（sig_pipefd[1]）
{
    int save_errno = errno;
    int msg = sig;
    send( sig_pipefd[1], ( char* )&msg, 1, 0 );
    errno = save_errno;
}

static void addsig( int sig, void( handler )(int), bool restart = true )
{
    struct sigaction sa;
    memset( &sa, '\0', sizeof( sa ) );
    sa.sa_handler = handler;
    if( restart )
    {
        sa.sa_flags |= SA_RESTART;
    }
    sigfillset( &sa.sa_mask );
    assert( sigaction( sig, &sa, NULL ) != -1 );
}

template< typename T >
processpool< T >::processpool( int listenfd, int process_number ) 
    : m_listenfd( listenfd ), m_process_number( process_number ), m_idx( -1 ), m_stop( false )
        //进程池构造函数，listenfd是监听的socket，在创建进程池前创建以便子进程可以直接引用它，m_process_number是进程池中子进程的数量
{
    assert( ( process_number > 0 ) && ( process_number <= MAX_PROCESS_NUMBER ) );

    m_sub_process = new process[ process_number ];
    assert( m_sub_process );
	//创建process_number个子进程，并建立他们和父进程之间的管道
    for( int i = 0; i < process_number; ++i )
    {
        int ret = socketpair( PF_UNIX, SOCK_STREAM, 0, m_sub_process[i].m_pipefd );  //创建套接字管道
        assert( ret == 0 ); //确保创建成功

        m_sub_process[i].m_pid = fork(); 
        assert( m_sub_process[i].m_pid >= 0 );
        if( m_sub_process[i].m_pid > 0 )  //父进程
        {
            close( m_sub_process[i].m_pipefd[1] );  //子进程要读父进程的发送数据，所以关闭写描述符
            continue;  //继续创建子进程；
        }
        else  //子进程
        {
            close( m_sub_process[i].m_pipefd[0] );  //父进程要给子进程发送数据，所以光比读描述符
            m_idx = i;
            break;   //子进程break跳出循环，否则子进程也会调用fork()
        }
    }
}
//统一事件源
template< typename T >
void processpool< T >::setup_sig_pipe()
{
    m_epollfd = epoll_create( 5 );
    assert( m_epollfd != -1 );

    int ret = socketpair( PF_UNIX, SOCK_STREAM, 0, sig_pipefd );
    assert( ret != -1 );

    setnonblocking( sig_pipefd[1] );  //非阻塞
    addfd( m_epollfd, sig_pipefd[0] ); //将fd放进监听队列
	//设置信号处理函数
    addsig( SIGCHLD, sig_handler );  
    addsig( SIGTERM, sig_handler );
    addsig( SIGINT, sig_handler );
    addsig( SIGPIPE, SIG_IGN );
}

template< typename T >
void processpool< T >::run()  //如果是父进程就运行父进程程序，子进程运行子进程程序
{
    if( m_idx != -1 )
    {
        run_child();
        return;
    }
    run_parent();
}

template< typename T >
void processpool< T >::run_child()
{
    setup_sig_pipe();  //创建epoll和信号管道

    int pipefd = m_sub_process[m_idx].m_pipefd[ 1 ];
    addfd( m_epollfd, pipefd );  //子进程需要监听管道文件描述符pipefd，因为父进程通过它来通知子进程accept新连接；

    epoll_event events[ MAX_EVENT_NUMBER ];
    T* users = new T [ USER_PER_PROCESS ];
    assert( users );
    int number = 0;
    int ret = -1;

    while( ! m_stop )
    {
        number = epoll_wait( m_epollfd, events, MAX_EVENT_NUMBER, -1 );
        if ( ( number < 0 ) && ( errno != EINTR ) )
        {
            printf( "epoll failure\n" );
            break;
        }

        for ( int i = 0; i < number; i++ )
        {
            int sockfd = events[i].data.fd;
            if( ( sockfd == pipefd ) && ( events[i].events & EPOLLIN ) )  //如果pipefd，说明是父进程传数据过来，有新连接
            {
                int client = 0;
                ret = recv( sockfd, ( char* )&client, sizeof( client ), 0 );
                if( ( ( ret < 0 ) && ( errno != EAGAIN ) ) || ret == 0 ) 
                {
                    continue;
                }
                else  //创建新连接
                {
                    struct sockaddr_in client_address;
                    socklen_t client_addrlength = sizeof( client_address );
                    int connfd = accept( m_listenfd, ( struct sockaddr* )&client_address, &client_addrlength );
                    if ( connfd < 0 )
                    {
                        printf( "errno is: %d\n", errno );
                        continue;
                    }
                    addfd( m_epollfd, connfd );
                    users[connfd].init( m_epollfd, connfd, client_address );  //初始化客户链接
                }
            }
            else if( ( sockfd == sig_pipefd[0] ) && ( events[i].events & EPOLLIN ) )  //处理收到的父进程信号
            {
                int sig;
                char signals[1024];
                ret = recv( sig_pipefd[0], signals, sizeof( signals ), 0 );
                if( ret <= 0 )
                {
                    continue;
                }
                else
                {
                    for( int i = 0; i < ret; ++i )
                    {
                        switch( signals[i] )
                        {
                            case SIGCHLD:  
                            {
                                pid_t pid;
                                int stat;
                                while ( ( pid = waitpid( -1, &stat, WNOHANG ) ) > 0 )
                                {
                                    continue;
                                }
                                break;
                            }
                            case SIGTERM:
                            case SIGINT:  //中断信号
                            {
                                m_stop = true;
                                break;
                            }
                            default:
                            {
                                break;
                            }
                        }
                    }
                }
            }
            //
            else if( events[i].events & EPOLLIN )
            {
                 users[sockfd].process();
            }
            else
            {
                continue;
            }
        }
    }

    delete [] users;
    users = NULL;
    close( pipefd );
    //close( m_listenfd );
    close( m_epollfd );
}

template< typename T >
void processpool< T >::run_parent()
{
    setup_sig_pipe();

    addfd( m_epollfd, m_listenfd ); //父进程监听是否m_listenfd是否有连接请求

    epoll_event events[ MAX_EVENT_NUMBER ];
    int sub_process_counter = 0;
    int new_conn = 1;
    int number = 0;
    int ret = -1;

    while( ! m_stop )
    {
        number = epoll_wait( m_epollfd, events, MAX_EVENT_NUMBER, -1 );
        if ( ( number < 0 ) && ( errno != EINTR ) )
        {
            printf( "epoll failure\n" );
            break;
        }

        for ( int i = 0; i < number; i++ )
        {
            int sockfd = events[i].data.fd;
            if( sockfd == m_listenfd )  //有连接请求，用rr算法分配给一个子进程处理
            {
                int i =  sub_process_counter;
                do
                {
                    if( m_sub_process[i].m_pid != -1 )
                    {
                        break;
                    }
                    i = (i+1)%m_process_number;
                }
                while( i != sub_process_counter );
                
                if( m_sub_process[i].m_pid == -1 )
                {
                    m_stop = true;
                    break;
                }
                sub_process_counter = (i+1)%m_process_number;
                //send( m_sub_process[sub_process_counter++].m_pipefd[0], ( char* )&new_conn, sizeof( new_conn ), 0 );
                send( m_sub_process[i].m_pipefd[0], ( char* )&new_conn, sizeof( new_conn ), 0 );
                printf( "send request to child %d\n", i );
                //sub_process_counter %= m_process_number;
            }
            //父进程处理收到的信号
            else if( ( sockfd == sig_pipefd[0] ) && ( events[i].events & EPOLLIN ) )
            {
                int sig;
                char signals[1024];
                ret = recv( sig_pipefd[0], signals, sizeof( signals ), 0 );
                if( ret <= 0 )
                {
                    continue;
                }
                else
                {
                    for( int i = 0; i < ret; ++i )
                    {
                        switch( signals[i] )
                        {
                            case SIGCHLD:
                            {
                                pid_t pid;
                                int stat;
                                while ( ( pid = waitpid( -1, &stat, WNOHANG ) ) > 0 )
                                {
                                    for( int i = 0; i < m_process_number; ++i )
                                    {
                                        if( m_sub_process[i].m_pid == pid )  //如果进程池中第i个子进程退出了，则主进程关闭相应的通信管道，设置相应的m_pid=-1，以标记该子进程已经退出；
                                        {
                                            printf( "child %d join\n", i );
                                            close( m_sub_process[i].m_pipefd[0] );
                                            m_sub_process[i].m_pid = -1;
                                        }
                                    }
                                }
                                m_stop = true; 
                                for( int i = 0; i < m_process_number; ++i )//如果子进程全都退出，则父进程也退出
                                {
                                    if( m_sub_process[i].m_pid != -1 )
                                    {
                                        m_stop = false;
                                    }
                                }
                                break;
                            }
                            case SIGTERM:
                            case SIGINT:  //终止信号：终止所有子进程（等待他们全都结束）
                            {
                                printf( "kill all the clild now\n" ); 
                                for( int i = 0; i < m_process_number; ++i )
                                {
                                    int pid = m_sub_process[i].m_pid;
                                    if( pid != -1 )
                                    {
                                        kill( pid, SIGTERM );
                                    }
                                }
                                break;
                            }
                            default:
                            {
                                break;
                            }
                        }
                    }
                }
            }
            else
            {
                continue;
            }
        }
    }

    //close( m_listenfd );
    close( m_epollfd );
}

#endif
```



##### 进程池实现CGI

```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <sys/epoll.h>
#include <signal.h>
#include <sys/wait.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <fcntl.h>

#define BUFFER_SIZE 1024
#define MAX_EVENT_NUMBER 1024
#define PROCESS_COUNT 5
#define USER_PER_PROCESS 65535

struct process_in_pool
{
    pid_t pid;
    int pipefd[2];
};

struct client_data
{
    sockaddr_in address;
    char buf[ BUFFER_SIZE ];
    int read_idx;
};

int sig_pipefd[2];
int epollfd;
int listenfd;
process_in_pool sub_process[ PROCESS_COUNT ];
bool stop_child = false;

int setnonblocking( int fd )
{
    int old_option = fcntl( fd, F_GETFL );
    int new_option = old_option | O_NONBLOCK;
    fcntl( fd, F_SETFL, new_option );
    return old_option;
}

void addfd( int epollfd, int fd )
{
    epoll_event event;
    event.data.fd = fd;
    event.events = EPOLLIN | EPOLLET;
    epoll_ctl( epollfd, EPOLL_CTL_ADD, fd, &event );
    setnonblocking( fd );
}

void sig_handler( int sig )
{
    int save_errno = errno;
    int msg = sig;
    send( sig_pipefd[1], ( char* )&msg, 1, 0 );
    errno = save_errno;
}

void addsig( int sig, void(*handler)(int), bool restart = true )
{
    struct sigaction sa;
    memset( &sa, '\0', sizeof( sa ) );
    sa.sa_handler = handler;
    if( restart )
    {
        sa.sa_flags |= SA_RESTART;
    }
    sigfillset( &sa.sa_mask );
    assert( sigaction( sig, &sa, NULL ) != -1 );
}

void del_resource()
{
    close( sig_pipefd[0] );
    close( sig_pipefd[1] );
    close( listenfd );
    close( epollfd );
}

void child_term_handler( int sig )
{
    stop_child = true;
}

void child_child_handler( int sig )
{
    pid_t pid;
    int stat;
    while ( ( pid = waitpid( -1, &stat, WNOHANG ) ) > 0 )
    {
        continue;
    }
}

int run_child( int idx )
{
    epoll_event events[ MAX_EVENT_NUMBER ];
    int child_epollfd = epoll_create( 5 );
    assert( child_epollfd != -1 );
    int pipefd = sub_process[idx].pipefd[1];
    addfd( child_epollfd, pipefd );
    int ret;
    addsig( SIGTERM, child_term_handler, false );
    addsig( SIGCHLD, child_child_handler );
    client_data* users = new client_data[ USER_PER_PROCESS ];

    while( !stop_child )
    {
        int number = epoll_wait( child_epollfd, events, MAX_EVENT_NUMBER, -1 );
        if ( ( number < 0 ) && ( errno != EINTR ) )
        {
            printf( "epoll failure\n" );
            break;
        }

        for ( int i = 0; i < number; i++ )
        {
            int sockfd = events[i].data.fd;
            if( ( sockfd == pipefd ) && ( events[i].events & EPOLLIN ) ) //有clnt请求连接
            {
                int client = 0;
                ret = recv( sockfd, ( char* )&client, sizeof( client ), 0 );
                if( ret < 0 )
                {
                    if( errno != EAGAIN )
                    {
                        stop_child = true;
                    }
                }
                else if( ret == 0 )
                {
                    stop_child = true;
                }
                else  //处理连接请求
                {
                    struct sockaddr_in client_address;
                    socklen_t client_addrlength = sizeof( client_address );
                    int connfd = accept( listenfd, ( struct sockaddr* )&client_address, &client_addrlength );
                    if ( connfd < 0 )
                    {
                        printf( "errno is: %d\n", errno );
                        continue;
                    }
                    memset( users[connfd].buf, '\0', BUFFER_SIZE );
                    users[connfd].address = client_address;
                    users[connfd].read_idx = 0;
                    addfd( child_epollfd, connfd );
                }
            }
            else if( events[i].events & EPOLLIN )   //clnt传数据
            {
                int idx = 0;  //读取的数据下标
                while( true )
                {
                    idx = users[sockfd].read_idx;
                    ret = recv( sockfd, users[sockfd].buf + idx, BUFFER_SIZE-1-idx, 0 );
                    if( ret < 0 )
                    {
                        if( errno != EAGAIN )
                        {
                            epoll_ctl( child_epollfd, EPOLL_CTL_DEL, sockfd, 0 );
                            close( sockfd );
                        }
                        break;
                    }
                    else if( ret == 0 )
                    {
                        epoll_ctl( child_epollfd, EPOLL_CTL_DEL, sockfd, 0 );
                        close( sockfd );
                        break;
                    }
                    else
                    {
                        users[sockfd].read_idx += ret;
                        printf( "user content is: %s\n", users[sockfd].buf );
                        idx = users[sockfd].read_idx;
                        if( ( idx < 2 ) || ( users[sockfd].buf[idx-2] != '\r' ) || ( users[sockfd].buf[idx-1] != '\n' ) )  //如果没有遇到/r/n（空格），则需要读取更多客户数据
                        {
                            continue;
                        }
                        users[sockfd].buf[users[sockfd].read_idx-2] = '\0';
                        char* file_name = users[sockfd].buf;   //需要运行的CGI程序
                        if( access( file_name, F_OK ) == -1 )  //判断客户需要运行的CGI程序是否存在
                        {
                            epoll_ctl( child_epollfd, EPOLL_CTL_DEL, sockfd, 0 );
                            close( sockfd );
                            break;
                        }
                        ret = fork();  //如果存在，创建子进程来执行CGI程序
                        if( ret == -1 )
                        {
                            epoll_ctl( child_epollfd, EPOLL_CTL_DEL, sockfd, 0 );
                            close( sockfd );
                            break;
                        }
                        else if( ret > 0 )  //父进程close，并在epoll中删除对应事件
                        {
                            epoll_ctl( child_epollfd, EPOLL_CTL_DEL, sockfd, 0 );
                            close( sockfd );
                            break;
                        }
                        else 
                        {
                            //子进程将标准输出定向到m_sockfd，并执行CGI程序
                            close( STDOUT_FILENO );
                            dup( sockfd );
                            execl( users[sockfd].buf, users[sockfd].buf, 0 );
                            exit( 0 );
                        }
                    }
                }
            }
            else
            {
                continue;
            }
        }
    }

    delete [] users;
    close( pipefd );
    close( child_epollfd );
    return 0;
}

int main( int argc, char* argv[] )
{
    if( argc <= 2 )
    {
        printf( "usage: %s ip_address port_number\n", basename( argv[0] ) );
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi( argv[2] );

    int ret = 0;
    struct sockaddr_in address;
    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );

    listenfd = socket( PF_INET, SOCK_STREAM, 0 );
    assert( listenfd >= 0 );

    ret = bind( listenfd, ( struct sockaddr* )&address, sizeof( address ) );
    assert( ret != -1 );

    ret = listen( listenfd, 5 );
    assert( ret != -1 );

    for( int i = 0; i < PROCESS_COUNT; ++i )  //生成PROCESS_COUNT个子进程
    {
        ret = socketpair( PF_UNIX, SOCK_STREAM, 0, sub_process[i].pipefd );
        assert( ret != -1 );
        sub_process[i].pid = fork();
        if( sub_process[i].pid < 0 )
        {
            continue;
        }
        else if( sub_process[i].pid > 0 )
        {
            close( sub_process[i].pipefd[1] );
            setnonblocking( sub_process[i].pipefd[0] );
            continue;
        }
        else
        {
            close( sub_process[i].pipefd[0] );
            setnonblocking( sub_process[i].pipefd[1] );
            run_child( i );  //运行子进程，父进程运行mian函数；
            exit( 0 );    //为什么用exit？？
        }
    }

    epoll_event events[ MAX_EVENT_NUMBER ];
    epollfd = epoll_create( 5 );
    assert( epollfd != -1 );
    addfd( epollfd, listenfd );

    ret = socketpair( PF_UNIX, SOCK_STREAM, 0, sig_pipefd );
    assert( ret != -1 );
    setnonblocking( sig_pipefd[1] );
    addfd( epollfd, sig_pipefd[0] );

    addsig( SIGCHLD, sig_handler );
    addsig( SIGTERM, sig_handler );
    addsig( SIGINT, sig_handler );
    addsig( SIGPIPE, SIG_IGN );
    bool stop_server = false;
    int sub_process_counter = 0;

    while( !stop_server )
    {
        int number = epoll_wait( epollfd, events, MAX_EVENT_NUMBER, -1 );
        if ( ( number < 0 ) && ( errno != EINTR ) )
        {
            printf( "epoll failure\n" );
            break;
        }

        for ( int i = 0; i < number; i++ )
        {
            int sockfd = events[i].data.fd;
            if( sockfd == listenfd )  //有连接，通过pipefd向子进程发送数据。
            {
                int new_conn = 1;
                send( sub_process[sub_process_counter++].pipefd[0], ( char* )&new_conn, sizeof( new_conn ), 0 );
                printf( "send request to child %d\n", sub_process_counter-1 );
                sub_process_counter %= PROCESS_COUNT;
            }
            else if( ( sockfd == sig_pipefd[0] ) && ( events[i].events & EPOLLIN ) )
            {
                int sig;
                char signals[1024];
                ret = recv( sig_pipefd[0], signals, sizeof( signals ), 0 );
                if( ret == -1 )
                {
                    continue;
                }
                else if( ret == 0 )
                {
                    continue;
                }
                else
                {
                    for( int i = 0; i < ret; ++i )
                    {
                        switch( signals[i] )
                        {
                            case SIGCHLD:
                            {
	                        pid_t pid;
	                        int stat;
	                        while ( ( pid = waitpid( -1, &stat, WNOHANG ) ) > 0 )
                                {
                                    for( int i = 0; i < PROCESS_COUNT; ++i )
                                    {
                                        if( sub_process[i].pid == pid )
                                        {
                                            close( sub_process[i].pipefd[0] );
                                            sub_process[i].pid = -1;
                                        }
                                    }
                                }
                                stop_server = true;
                                for( int i = 0; i < PROCESS_COUNT; ++i )
                                {
                                    if( sub_process[i].pid != -1 )
                                    {
                                        stop_server = false;
                                    }
                                }
                                break;
                            }
                            case SIGTERM:
                            case SIGINT:
                            {
                                printf( "kill all the clild now\n" );
                                for( int i = 0; i < PROCESS_COUNT; ++i )
                                {
                                    int pid = sub_process[i].pid;
                                    kill( pid, SIGTERM );
                                }
                                break;
                            }
                            default:
                            {
                                break;
                            }
                        }
                    }
                }
            }
            else 
            {
                continue;
            }
        }
    }

    del_resource();
    return 0;
}
```



#### 半同步/半反应堆线程池

```c++
#ifndef THREADPOOL_H
#define THREADPOOL_H

#include <list>
#include <cstdio>
#include <exception>
#include <pthread.h>
#include "locker.h"  //封装的线程同步机制

template< typename T >
class threadpool
{
public:
    threadpool( int thread_number = 8, int max_requests = 10000 );  //线程池中线程的数量，请求队列大小
    ~threadpool();
    bool append( T* request );  //往请求队列中添加任务

private:
    static void* worker( void* arg );  //工作线程运行的函数，从工作队列中取出任务并执行
    void run();

private:
    int m_thread_number;
    int m_max_requests;
    pthread_t* m_threads;   //线程数组
    std::list< T* > m_workqueue;	//请求队列
    locker m_queuelocker;		//保护请求队列的互斥锁
    sem m_queuestat;		//是否有任务要处理
    bool m_stop;		//是否结束线程
};

template< typename T >
threadpool< T >::threadpool( int thread_number, int max_requests ) : 
        m_thread_number( thread_number ), m_max_requests( max_requests ), m_stop( false ), m_threads( NULL )
{
    if( ( thread_number <= 0 ) || ( max_requests <= 0 ) )
    {
        throw std::exception();
    }

    m_threads = new pthread_t[ m_thread_number ];
    if( ! m_threads )
    {
        throw std::exception();
    }

    for ( int i = 0; i < thread_number; ++i )  //创建thread_number个线程，并将它们都设置为脱离线程
    {
        printf( "create the %dth thread\n", i );
        if( pthread_create( m_threads + i, NULL, worker, this ) != 0 )
        {
            delete [] m_threads;
            throw std::exception();
        }
        if( pthread_detach( m_threads[i] ) )
        {
            delete [] m_threads;
            throw std::exception();   //
        }
    }
}

template< typename T >
threadpool< T >::~threadpool()
{
    delete [] m_threads;
    m_stop = true;
}

template< typename T >
bool threadpool< T >::append( T* request )  //往工作队列中添加任务
{
    m_queuelocker.lock();  //加锁，因为它是共享的
    if ( m_workqueue.size() > m_max_requests )  //已经超出队列大小
    {
        m_queuelocker.unlock();
        return false;
    }
    m_workqueue.push_back( request );
    m_queuelocker.unlock();
    m_queuestat.post();
    return true;
}

template< typename T >
void* threadpool< T >::worker( void* arg )  //
{
    threadpool* pool = ( threadpool* )arg;
    pool->run();
    return pool;
}

template< typename T >
void threadpool< T >::run()
{
    while ( ! m_stop )
    {
        m_queuestat.wait();
        m_queuelocker.lock();
        if ( m_workqueue.empty() )
        {
            m_queuelocker.unlock();
            continue;
        }
        T* request = m_workqueue.front();  //取出队列请求
        m_workqueue.pop_front();
        m_queuelocker.unlock();
        if ( ! request )
        {
            continue;
        }
        request->process();   //执行相应的请求函数
    }
}

#endif
```



### 协程

**协程是什么？**

上面说到线程之间是竞争关系，线程不会主动让出 cpu 时间片，因此当系统中的线程越来越多的时候，操作系统为了让每个线程都有机会执行，会频繁的进行线程切换。线程切换的代价比进程切换要少很多，因为各个线程之间共享进程地址空间，共享内存，共享全局数据等，因此只需要保存当前线程的局部变量，数据，以及 pc 寄存器的值，然后加载新线程的资源即可

具体来讲，**非对称协程**（asymmetric coroutines）是跟一个特定的调用者绑定的，协程让出 CPU 时，只能让回给原调用者。那到底是什么东西“不对称”呢？其实，非对称在于程序控制流转移到被调协程时使用的是 call/resume 操作，而当被调协程让出 CPU时使用的却是 return/yield 操作。此外，协程间的地位也不对等，caller 与 callee 关系是确定的，不可更改的，非对称协程只能返回最初调用它的协程。**对称协程**（symmetric coroutines）则不一样，启动之后就跟启动之前的协程没有任何关系了。协程的切换操作，一般而言只有一个操作，yield，用于将程序控制流转移给另外的协程。对称协程机制一般需要一个调度器的支持，按一定调度算法去选择 yield的目标协程。Go 语言提供的协程，其实就是典型的对称协程。不但对称，goroutines 还可以在多个线程上迁移。这种协程跟操作系统中的线程非常相似，甚至可以叫做“用户级线程”了。

#### 用户态线程

对于 web 应用而言，线程切换最频繁的场景就在于 IO 了。当 IO 阻塞时，操作系统会挂起线程，然后让其他线程执行，不会让 cpu 傻傻等着

线程切换无非是改变 CPU 下一条指令执行的地址，那我们能不能在应用程序的用户态做到？

```php
function main()
{
    A();
    B();
    C();
}

function A()
{
    //IO Blocking
}

function B()
{
    //IO Blocking
}

function C()
{
    //IO Blocking
}
复制代码
```

上面这段代码，A，B，C，三个函数互不依赖，但是各自都有 IO 阻塞的代码。用传统的 php-fpm 执行，这个进程会阻塞三次，执行时间为 A+B+C；假如为这个三个函数各创建三个子线程，然后执行，理想情况下执行时间为 MAX(A,B,C)，但是存在线程切换（单核）

**如果能在 A 进行 IO 阻塞时，让 cpu 执行函数 B 的指令，然后等 A 的 IO 结束后，再重新执行后续的逻辑。那么程序既不会挂起，也不需要进程线程切换，而且执行时间也是 MAX(A,B,C)**

可以根据线程的思路，在用户态为这三个函数创建各自的栈，以及各自独立的内存空间，从 A 切换到 B 时，把 A 下一条该执行的指令地址和 A 的局部变量保存起来，然后把指令切换到 B 的初始地址。这个过程不需要操作系统，而是由用户态代码逻辑来切换。**这样对于操作系统而言，这个程序实际上只阻塞了一次**

**在C++里面实现协程要解决的问题有如下几个：**

- 何时挂起协程？何时唤醒协程？
- 如何挂起、唤醒协程，如何保护协程运行时的上下文？
- 如何封装异步操作？

1. 现代操作系统是分时操作系统，资源分配的基本单位是进程，CPU调度的基本单位是线程。
2. C++程序运行时会有一个运行时栈，一次函数调用就会在栈上生成一个record
3. 运行时内存空间分为全局变量区（存放函数，全局变量）,栈区，堆区。栈区内存分配从高地址往低地址分配，堆区从低地址往高地址分配。
4. 下一条指令地址存在于指令寄存器IP，ESP寄存值指向当前栈顶地址，EBP指向当前活动栈帧的基地址。
5. 发生函数调用时操作为：将参数从右往左依次压栈，将返回地址压栈，将当前EBP寄存器的值压栈，在栈区分配当前函数局部变量所需的空间，表现为修改ESP寄存器的值。
6. 协程的上下文包含属于他的栈区和寄存器里面存放的值。

**协程切换的时机和流程**



- 保存当前协程的上下文（运行栈，返回地址，寄存器状态）
- 设置将要唤醒的协程的入口指令地址到IP寄存器
- 恢复将要唤醒的协程的上下文

#### linux协程函数

**常见协程库**

- 1.1.1 boost.context
  提供了上下文的抽象，并给了两种方式，fiber和call/cc的方式保留和执行上下文切换;性能佳，切换性能可达到1.25亿次/秒；
- 1.1.2 boost.coroutine
  提供的协程只能单向传递数据，数据只能单向的从一个代码块流向另一个代码块。流入流出分别对应着push_type和pull_type类型，由这两个类型组成协程间跳转的通道，同时也是数据传递的通道；
- 1.1.3 ucontext
  该库是在unix下提供的，使用是最安全可靠，但性能较差，大概200万次/秒；
- 1.1.4 fiber
  该库是在window下提供的，与ucontext类似；
- 1.1.5 libgo
  libgo为了有更广阔的适用性，支持了多线程调度、HookSyscall、Worksteal等，同时突破了传统协程库仅用来处理网络io密集型业务的局限，也能适用于cpu密集型业务，充当并行编程库来使用；

**协程栈**

- 1.2.1 静态栈
  固定大小的栈，容易造成溢出等现象；
- 1.2.2 分段栈
  插入栈内存检测代码，若栈不够用，则申请新内存扩展；但该方法难以在第三方库中进行使用；
- 1.2.3 共享栈
  申请一块大内存作为共享栈，在运行前，先把协程栈的内存copy到共享栈中，运行结束后再计算协程栈真正使用的内存，copy出来保存起来，这样每次只需保存真正使用到的栈内存量即可；
  该方案极大程度上避免了内存的浪费，做到了用多少占多少，同等内存条件下，可以启动的协程数量更多；
  但该方案在copy上花费了时间，降低速度，导致协程切换慢；
- 1.2.4 虚拟内存栈
  机制：进程申请的内存并不会立即被映射成物理内存，而是仅管理于虚拟内存中，真正对其读写时会触发缺页中断，此时才会映射为物理内存；
  可以做到用多少占多少，冗余不超过一个内存页大小

**协程调度**

- 1.3.1 栈式调度
  协程队列是一个栈式结构，创建的协程都置于栈顶，并且会立即暂停当前协程并切换至子协程中运行，子协程运行结束后，继续切换回来执行父协程；越是栈底部的协程，被调度到的机会将越少，甚至出现只有栈顶的协程在互相切换；

- 1.3.2 星切调度
  调度线程 -> 协程A -> 调度线程 -> 协程B -> 调度线程 -> …；
  将当前可调度的协程组织成队列，按顺序从头部取出协程调度；新协程则从尾部入队，调度后再将协程从尾部入队；
- 1.3.3 环切调度
- 调度线程 -> 协程A -> 协程B -> 协程C -> 协程D -> 调度线程 -> …
  从调度顺序上可知，环切的切换次数仅为星切的一半，可以提高整体切换速度；但在多线程调度、WorkSteal方面会带来一定的挑战；
  ![img](网络编程/register.png)

这些寄存器是最基本也是汇编中直接使用的寄存器最多的。
其中AX，BX，CX，DX这些寄存器用来保存操作数和运算结果等信息，从而节省读取操作数所需占用总线和访问存储器的时间，可以认为随便用。

- 指针寄存器：SP，BP，用于维护和访问堆栈存储单元。
  BP为基指针(Base Pointer)寄存器，用它可直接存取堆栈中的数据；
  SP为堆栈指针(Stack Pointer)寄存器，用它只可访问栈顶。
- 变址寄存器:寄存器SI，DI称为变址寄存器(Index Register)，它们主要用于存放存储单元在段内的偏移量，
- 指令指针寄存器
  指令指针IP(Instruction Pointer)是存放下次将要执行的指令在代码段的偏移量。在具有预取指令功能的系统中，下次要执行的指令通常已被预取到指令队列中，除非发生转移情况。

GCC中对这些寄存器的调用规则如下：

- rax 作为函数返回值使用。
- rsp 栈指针寄存器，指向栈顶；
- rdi，rsi，rdx，rcx，r8，r9 用作函数参数，依次对应第1参数，第2参数。。。当参数超过6个，才会通过压栈的方式传参数。
- rbx，rbp，r12，r13，r14，r15 用作数据存储，遵循被调用者使用规则，简单说就是随便用，调用子函数之前要备份它，以防他被修改；
- r10，r11 用作数据存储，遵循调用者使用规则，简单说就是使用之前要先保存原值；

```c++
/* Userlevel context.  */
typedef struct ucontext
{
    unsigned long int uc_flags;
    struct ucontext *uc_link;	//后继上下文
    stack_t uc_stack;			//用户自定义栈
    mcontext_t uc_mcontext;		//保存当前上下文，即各个寄存器的状态
__sigset_t uc_sigmask;		//保存当前线程的信号屏蔽掩码
} ucontext_t;


 * int getcontext(ucontext_t *ucp);  //获取上下文
 * -------
 * func：函数将ucp指向的结构初始化为调用线程的当前用户上下文；
 * param ucp：用户上下文，包括调用线程的机器寄存器的内容、信号掩码和当前执行堆栈；
 * return：成功不返回， 否则返回-1；
 * */

 * int setcontext(const ucontext_t *ucp); //恢复上下文
 * -------
 * func：函数恢复ucp指向的用户上下文；
 * param ucp：用户上下文，包括调用线程的机器寄存器的内容、信号掩码和当前执行堆栈；
 * return：成功返回0， 否则返回-1；
 * */

 * void makecontext(ucontext_t *ucp, (void *func)(), int argc, ...);  //修改上下文
 * -------
 * func：修改ucp指定的上下文；
 * param ucp：调用的函数，在调用 makecontext()之前，被修改的上下文应该有一个为其分配的堆栈；
        m_ctx.uc_link = nullptr;    // 当前上下文返回之后指向的上下文
        m_ctx.uc_stack.ss_sp;     	// 栈空间
        m_ctx.uc_stack.ss_size ;  	// 栈大小
 * param argc：argc的值 必须与传递给func的整数参数的数量相匹配 ，否则行为未定义；
 * */

 * int swapcontext(ucontext_t *oucp, const ucontext_t *ucp); //保存上下文并切换上下文
 * -------
 * func：当前上下文保存在 oucp指向的上下文结构中，并将上下文设置为ucp 指向的上下文结构；
 * param oucp：上下文；
 * param ucp：上下文；
 * return：成功返回0， 否则返回-1；
 * */

```



### 多路复用服务器：通过捆绑并统一管理I/O对象提供服务

**举一个简单地网络服务器的例子，如果你的服务器需要和多个客户端保持连接，处理客户端的请求，属于多进程的并发问题，如果创建很多个进程来处理这些IO流，会导致CPU占有率很高。所以人们提出了I/O多路复用模型：一个线程，通过记录I/O流的状态来同时管理多个I/O。select只是IO复用的一种方式，其他的还有：poll，epoll等。**

select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。**但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的**，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。

### 标准I/O函数库

**什么是标准I/O？是指C语言里的文件操作函数，如：fopen,feof,fgetc,fputs等函数，他们和平台无关。**

**对于标准I/O库（有些I/O函数是针对文件描述符的），它们的操作都是围绕流来进行的。当用标准I/O库打开或创建一个文件时，我们已经使一个流与文件相结合。**

**标准 I/O 提供流缓冲的目的是尽可能减少使用read和write调用的数量**。标准I/O 提供了3 种类型的缓冲存储。
**· 全缓冲**。在这种情况下，当填满标准I/O 缓存后才进行实际I/O 操作。对于驻在磁盘上的文件通常是由标准I/O 库实施全缓冲的。在一个流上执行第一次I/O 操作时，通常调用malloc就是使用全缓冲。
**· 行缓冲**。在这种情况下，当在输入和输出中遇到新行符时，标准I/O 库执行I/O 操作。这允许我们一次输出一个字符（如fputc 函数），但只有写了一行之后才进行实际I/O 操作。当流涉及一个终端时（例如标准输入和标准输出），典型地使用行缓冲。
**· 不带缓冲**。标准I/O库不对字符进行缓冲。如果用标准I/O函数写若干字符到不带缓冲的流中，则相当于用write系统的用函数将这些字符写全相比较的打开文件上。标准出错况stderr通常是不带缓存后，这就使得出错信息可以尽快显示出来，而不管它们是否含有一个新行字符。

网络通信中使用标准I/O的优点：

-  良好的移植性。良好移植性这个不需多解释，不仅是I/O函数，所有的标准函数都具有良好的移植性。因为，为了支持所有的操作系统(编译器)，这些函数都是按照ANSI C标准定义的。 
-  标准I/O函数可以利用缓冲提高性能。在网络通信中，read,write传输数据只有一种套接字缓冲，但使用标准I/O传输会有额外的缓冲，即I/O缓冲和套接字缓冲两个。使用I/O缓冲主要是为了提高性能，需要传输的数据越多时越明显。因为，一次发送更多的数据要比分多次发送同样的数据性能要高。发送一次数据就对应一个数据包，往往数据包的头信息比较大，它与数据大小无关。 

![img](http://hiphotos.baidu.com/lammy/pic/item/4e6c01d7dd5b67e9a044df65.jpg)

例：通过fputs函数传输字符串，首先将数据传递到标准I/O函数的缓冲。然后数据将移动到套接字输出缓冲，最后发送到对方主机



网络通信中使用标准I/O的缺点：

- 不容易进行双向通信。
- 有时可能频繁调用fflush函数。
- 需要以FILE结构体指针的形式返回文件描述符。

```c++
1. 打开、关闭I/O流函数
下面三个函数可用于打开一个标准流：
 字符串    含义
 r 或 rb    以只读方式打开
 w 或 wb    以只写方式打开，若文件有内容，则清空
 a 或 ab    以只写方式打开，原内容保留，写入的内容附加在文件流尾部
 r+ 或 rb+ 或 r+b  以更新方式打开，此时文件可读可写
 w+ 或 wb+ 或 w+b  以更新方式打开，文件可读可写，但打开时清空文件内容
 a+ 或 ab+ 或 a+b  以更新方式打开，文件可读可写，写入的内容附加在文件流尾部

FILE *fopen(const char *pathname, const char *type) ;

FILE *freopen(const char *pathname, const char *type, FILE *fp) ;

FILE *fdopen(int filedes, const char *type) ;

三个函数的返回：若成功则为文件指针，若出错则为NULL


这三个函数的区别是：

(1) fopen打开路径名由pathname指示的一个文件。

(2) freopen在一个特定的流上(由fp指示)打开一个指定的文件（其路径名由pathname指示），如若该流已经打开，则先关闭该流。此函数一般用于将一个指定的文件打开为一个预定义的流：标准输入、标准输出或标准出错。

(3) fdopen取一个现存的文件描述符（我们可能从open,dup,dup2,fcntl或pipe函数得到此文件描述符），并使一个标准的I/O流与该描述符相结合。

下面函数用于关闭一个标准流：


int fclose(FILE *fp)


2. 读、写I/O流函数
1)以字节为单位的I/O函数

int getc(FILE *stream);

int fgetc(FILE *stream);

int getchar(void);

返回值：成功返回读到的字节，出错或者读到文件末尾时返回EOF


l 第一个跟第三个本身不是函数，是通过宏定义借助fgetc来实现的。比如：

# define getc(_stream) fgetc(_stream)

# define getchar fgetc(stdin)

l 所以fgetc允许作为一个参数传递给另一个函数。

l fgetc成功时返回读到一个字节，本来应该是unsigned char型的，但由于函数原型中返回值是int型，所以这个字节要转换成int型再返回，那为什么要规定返回值是int型呢？因为出错或读到文件末尾时fgetc将返回EOF，即-1，保存在int型的返回值中是0xffffffff，如果读到字节0xff，由unsigned char型转换为int型是0x000000ff，只有规定返回值是int型才能把这两种情况区分开，如果规定返回值是unsigned char型，那么当返回值是0xff时无法区分到底是EOF还是字节0xff。如果需要保存fgetc的返回值，一定要保存在int型变量中，如果写成unsigned char c = fgetc(fp);，那么根据c的值又无法区分EOF和0xff字节了。注意，fgetc读到文件末尾时返回EOF，只是用这个返回值表示已读到文件末尾，并不是说每个文件末尾都有一个字节是EOF（根据上面的分析，EOF并不是一个字节）。


int putc(int c, FILE *stream);

int fputc(int c, FILE *stream);

int putchar(int c);

返回值：若成功返回c，出错则为EOF


l 同样第一个跟第三个本身不是函数，是通过宏定义借助fgetc来实现的。

2)以字符串为单位的I/O函数


char *fgets(char *s, int size, FILE *stream);

char *gets(char *s);

返回值：成功时s指向哪返回的指针就指向哪，出错或者读到文件末尾时返回NULL


l 这两个函数都指定了缓存地址，读入的字符串放入其中。gets是从标准输入读，fgets是从指定流读。

l gets不推荐程序员使用，它的存在只是为了兼容以前的程序，我们写的代码不应该有调用这个函数。

l 现在说说fgets函数，参数s是缓冲区的首地址，size是缓冲区的长度，该函数从stream所指的文件中读取以'\n'结尾的一行（包括'\n'在内）存到缓冲区s中，并且在该行末尾添加一个'\0'组成完整的字符串。如果文件中的一行太长，fgets从文件中读了size-1个字符还没有读到'\n'，就把已经读到的size-1个字符和一个'\0'字符存入缓冲区，文件中剩下的半行可以在下次调用fgets时继续读。如果一次fgets调用在读入若干个字符后到达文件末尾，则将已读到的字符串加上'\0'存入缓冲区并返回，如果再次调用fgets则返回NULL，可以据此判断是否读到文件末尾。注意，对于fgets来说，'\n'是一个特别的字符，而'\0'并无任何特别之处，如果读到'\0'就当作普通字符读入。如果文件中存在'\0'字符（或者说0x00字节），调用fgets之后就无法判断缓冲区中的'\0'究竟是从文件读上来的字符还是由fgets自动添加的结束符，所以fgets只适合读文本文件而不适合读二进制文件，并且文本文件中的所有字符都应该是可见字符，不能有'\0'。对于二进制文件可以通过fread来实现

int fputs(const char *s, FILE *stream);

int puts(const char *s);

返回值：成功返回一个非负整数，出错返回EOF


l 缓冲区s中保存的是以'\0'结尾的字符串，fputs将该字符串写入文件stream，但并不写入结尾的'\0'。与fgets不同的是，fputs并不关心的字符串中的'\n'字符，字符串中可以有'\n'也可以没有'\n'。puts将字符串s写到标准输出（不包括结尾的'\0'），然后自动写一个'\n'到标准输出。

l 

3)二进制I/O函数

l 上面也提到过用字符串为单位的IO函数不适合二进制文本。当然对于二进制文件，我们可以通过使用fgetc跟fputc来实现，但是必须循环整个二进制文件，明显比较低效。因此标准IO库提供了如下两个函数对二进制文件操作:


size_t fread(void *ptr, size_t size, size_t nmemb, FILE *stream);

size_t fwrite(const void *ptr, size_t size, size_t nmemb, FILE *stream);

返回值：读或写的记录数，成功时返回的记录数等于nmemb，出错或读到文件末尾时返回的记录数小于nmemb，也可能返回0


l 使用二进制I/O的基本问题是，它只能用于读已写在同一系统上的数据。其原因是：

(1) 在一个结构中，同一成员的位移量可能随编译程序和系统的不同而异（由于不同的对准要求）。确实，某些编译程序有一选择项，它允许紧密包装结构（节省存储空间，而运行性能则可能有所下降）或准确对齐，以便在运行时易于存取结构中的各成员。这意味着即使在单一系统上，一个结构的二进制存放方式也可能因编译程序的选择项而不同。

(2) 用来存储多字节整数和浮点值的二进制格式在不同的系统结构间也可能不同。

3)二进制I/O函数

l 上面也提到过用字符串为单位的IO函数不适合二进制文本。当然对于二进制文件，我们可以通过使用fgetc跟fputc来实现，但是必须循环整个二进制文件，明显比较低效。因此标准IO库提供了如下两个函数对二进制文件操作:


size_t fread(void *ptr, size_t size, size_t nmemb, FILE *stream);

size_t fwrite(const void *ptr, size_t size, size_t nmemb, FILE *stream);

返回值：读或写的记录数，成功时返回的记录数等于nmemb，出错或读到文件末尾时返回的记录数小于nmemb，也可能返回0


l 使用二进制I/O的基本问题是，它只能用于读已写在同一系统上的数据。其原因是：

(1) 在一个结构中，同一成员的位移量可能随编译程序和系统的不同而异（由于不同的对准要求）。确实，某些编译程序有一选择项，它允许紧密包装结构（节省存储空间，而运行性能则可能有所下降）或准确对齐，以便在运行时易于存取结构中的各成员。这意味着即使在单一系统上，一个结构的二进制存放方式也可能因编译程序的选择项而不同。

(2) 用来存储多字节整数和浮点值的二进制格式在不同的系统结构间也可能不同。

3. 定位I/O流函数
两种方法定位标准I/O流：

(1) ftell和fseek。这两个函数自V7以来就存在了，但是它们都假定文件的位置可以存放在一个长整型中。

(2) fgetpos和fsetpos。这两个函数是新由ANSI C引入的。它们引进了一个新的抽象数据类型fpost，它记录文件的位置。在非UNIX系统中，这种数据类型可以定义为记录一个文件的位置所需的长度。所以移植到非UNIX系统的应用程序应当使用fgetpos和fsetpos。


int fseek(FILE *stream, long offset, int whence);

返回值：成功返回0，出错返回-1并设置errno

 

long ftell(FILE *stream);

返回值：成功返回当前读写位置，出错返回-1并设置errno

 

void rewind(FILE *stream);

把读写位置移到文件开头


fseek的whence和offset参数共同决定了读写位置移动到何处，whence参数的含义如下：

SEEK_SET 

从文件开头移动offset个字节

SEEK_CUR 

从当前位置移动offset个字节

SEEK_END 

从文件末尾移动offset个字节

 

offset可正可负，负值表示向前（向文件开头的方向）移动，正值表示向后（向文件末尾的方向）移动，如果向前移动的字节数超过了文件开头则出错返回，如果向后移动的字节数超过了文件末尾，再次写入时将增大文件尺寸，从原来的文件末尾到fseek移动之后的读写位置之间的字节都是0。


int fgetpos(FILEf *p, fpos_t *pos) ;

int fsetpos(FILEf *p, const fpos_t *pos) ;

两个函数返回：若成功则为0，若出错则为非0


fgetpos将文件位置指示器的当前值存入由pos指向的对象中。在以后调用fsetpos时，可以使用此值将流重新定位至该位置。

 

4. 格式化I/O流函数
l 格式化输入函数：


int printf(const char *format, ...);

int fprintf(FILE *stream, const char *format, ...);

int sprintf(char *str, const char *format, ...);

int snprintf(char *str, size_t size, const char *format, ...);

 

int vprintf(const char *format, va_list ap);

int vfprintf(FILE *stream, const char *format, va_list ap);

int vsprintf(char *str, const char *format, va_list ap);

int vsnprintf(char *str, size_t size, const char *format, va_list ap);

 

返回值：成功返回格式化输出的字节数（不包括字符串的结尾'\0'），出错返回一个负值


l 格式化输出函数：


int scanf(const char *format, ...);

int fscanf(FILE *stream, const char *format, ...);

int sscanf(const char *str, const char *format, ...);

 

#include <stdarg.h>

 

int vscanf(const char *format, va_list ap);

int vsscanf(const char *str, const char *format, va_list ap);

int vfscanf(FILE *stream, const char *format, va_list ap);

返回值：返回成功匹配和赋值的参数个数，成功匹配的参数可能少于所提供的赋值参数，返回0表示一个都不匹配，出错或者读到文件或字符串末尾时返回EOF并设置errno


这里仅仅说一下printf的一个小技巧，我们在%后面加上#，打印到终端的值，会在前面自动加上0、0x。比如pintf("%#x",1)语句在终端会打印0x1。

 

5. 创建临时文件I/O流函数
很多情况下，程序会创建一些文件形式的临时文件，这些临时文件可能保存这一个计算的中间结果，也可能是关键操作前的备份等等。这都是临时文件的好处。

标准I/O提供了两个函数创建临时文件


char *tmpnam(char *ptr) ;

返回：指向一唯一路径名的指针

 

FILE *tmpfile(void);

返回：若成功则为文件指针，若出错则为NULL


l tmpnam函数返回一个不与任何已存在文件同名的有效文件名。每次调用它都会产生一个不同的文件名，但是一个进程中调用最多次数为TMP_MAX【在stdio.h中有定义】。如果ptr不为NULL，则认为字符串ptr的长度至少是L_tmpnam【在stdio.h中有定义】,所产生的文件名会放入该字符串ptr中，因此返回值为ptr的值；如果ptr为NULL，则所产生的文件名存放在一个静态区中，下一次调用时，会重写改静态区。

l tmpfile 创建一个临时二进制文件（类型为wb+），关闭文件或程序结束时将自动删除这种文件。

l 需要注意的是，tmpnam仅仅是创建一个临时文件，并没有打开它，所以我们如果要用它必须尽可能快的打开它，这样减小另一个程序用同样的名字打开文件的风险。而tmpfile除了创建外，会同时以读写方式打开。
```



### 基于I/O复用的服务器

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211010164436094.png" alt="image-20211010164436094" style="zoom:67%;" />

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211010164453918.png" alt="image-20211010164453918" style="zoom:67%;" />

　**IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合：**

　　（1）当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用I/O复用。

　　（2）当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现。

　　（3）如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。

　　（4）如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。

　　（5）如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。

　　与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。

#### 调用select函数监视文件描述符

**使用select函数可以将多个文件描述符集中统一监视**

**该函数准许进程指示内核等待多个事件中的任何一个发送，并只在有一个或多个事件发生或经历一段指定的时间后才唤醒。**

- 是否存在套接字接收数据

- 有哪些套接字准备好（文件描述符不再是阻塞状态，可以用于某类IO操作了，包括可读，可写，发生异常三种。）

- 哪些套接字发生了异常

  ![img](https://images2015.cnblogs.com/blog/305504/201509/305504-20150918012828961-1176245587.png)

**select调用底层实现**

（1）使用copy_from_user从用户空间拷贝fd_set到内核空间

（2）注册回调函数__pollwait

（3）遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）

（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。

（5）__pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。

（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。

（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。

（8）把fd_set从内核空间拷贝到用户空间。

**select的几大缺点：**

**（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大**

**（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大**

**（3）select支持的文件描述符数量太小了，默认是1024**

```c++
#include <sys/select.h>
#include <sys/time.h>
#include <sys/types.h>
#include <unistd.h>

int select(int nfds, fd_set *readfds, fd_set *writefds,
           			 fd_set *exceptfds, struct timeval *timeout);
 
 nfds：监视对象文件描述符数量
     
 fd_set:用来存放需要监视的文件描述符数组（存放时也会按照监视项：接收，传输，异常进行区分），实际上是一long类型的数组，每一个数组元素都能与一打开的文件句柄(不管是socket句柄，还是其他文件或命名管道或设备句柄)建立联系，建立联系的工作由程序员完成，当调用select()时，由内核根据IO状态修改fe_set的内容，由此来通知执行了select()的进程哪一socket或文件可读。
     
 readnfds: 指向检查可读性（是否存在待读取数据）的套接字集合的可选的指针。
     
 writefds: 指向检查可写性（是否可传输无阻塞数据）的套接字集合的可选的指针。
     
 exceptfds: 指向检查错误（是否发生异常）的套接字集合的可选的指针。
     
 timeout: select函数需要等待的最长时间，需要以TIMEVAL结构体格式提供此参数，对于阻塞操作，此参数为null（永远等待下去直到有一个描述字准备好I/O时才放回）。若超时则传递time-out信息。   等待一段固定时间：在有一个描述字准备好I/O时返回，但是不超过由该参数所指向的timeval结构中指定的秒数和微秒数。  根本不等待：检查描述字后立即返回，这称为轮询。为此，该参数必须指向一个timeval结构，而且其中的定时器值必须为0。

void FD_CLR(int fd, fd_set *set);//从集合中删除指定的文件描述符。
int  FD_ISSET(int fd, fd_set *set);//检查集合中指定的文件描述符是否准备好（可读或可写）,有则返回真
void FD_SET(int fd, fd_set *set);//将一个给定的文件描述符加入到集合之中
void FD_ZERO(fd_set *set);//将某一个集合清空（将fd_set变量的所有位初始化为0）

	struct timeval{//timeout告知内核等待所指定描述字中的任何一个就绪可花多少时间。其timeval结构用于指定这段时间的秒数和微秒数。

                   long tv_sec;   //seconds

                   long tv_usec;  //microseconds

       };

```

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20211011230436490.png" alt="image-20211011230436490" style="zoom:67%;" />



```c++
#include <stdio.h>
#include <unistd.h>
#include <sys/time.h>
#include <sys/select.h>

#define BUF_SIZE 30

int main(int argc, char *argv[])
{
	fd_set reads, temps;//
	int result, str_len;
	char buf[BUF_SIZE];
	struct timeval timeout;

	FD_ZERO(&reads); //初始化fd_set变量
	FD_SET(0, &reads); // 0 is standard input(console)，第0位的文件描述符（fd==0的文件描述字）设置成监视位

	/*
	timeout.tv_sec=5;
	timeout.tv_usec=5000;
	*/

	while(1)
	{
		temps=reads;//将准备好的fd-set 变量reads 的内容复制到temps 变量，因为之前讲过，调用 select函数后，除发生变化的文件描述符对应位外，剩下的所有位将初始化为0 。因此，为了记住初始值，必须经过这种复制过程。
		timeout.tv_sec=5;
		timeout.tv_usec=0;
		result=select(1, &temps, 0, 0, &timeout);//调用 select 函数。如果有控制台输入数据，则返回大于0的整数;如果没有输入数据而引发超时，返回0

		if(result==-1)
		{
			puts("select() error!");
			break;
		}
		else if(result==0)
		{
			puts("Time-out!");
		}
		else 
		{
			if(FD_ISSET(0, &temps))   //select返回大于0的值时，验证发生变化的文件描述符是否为标准输入。若是，从标准输入读取数据并向控制台输出
			{
				str_len=read(0, buf, BUF_SIZE);
				buf[str_len]=0;
				printf("message from console: %s", buf);
			}
		}
	}
	return 0;
}
```



**select的回声服务器端**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/time.h>
#include <sys/select.h>

#define BUF_SIZE 100
void error_handling(char *buf);

int main(int argc, char *argv[])
{
	int serv_sock, clnt_sock;
	struct sockaddr_in serv_adr, clnt_adr;
	struct timeval timeout;
	fd_set reads, cpy_reads;

	socklen_t adr_sz;  //clnt_addr 的长度
	int fd_max, str_len, fd_num, i;
	char buf[BUF_SIZE];
	if(argc!=2) {
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}

	serv_sock=socket(PF_INET, SOCK_STREAM, 0);
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port=htons(atoi(argv[1]));

	if(bind(serv_sock, (struct sockaddr*) &serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");
	if(listen(serv_sock, 5)==-1)
		error_handling("listen() error");

	FD_ZERO(&reads);
	FD_SET(serv_sock, &reads);  //把serv_sock置进监听队列.客户端连接请求同样通过传输数据完成，因此，服务器端套接字中有接收的数据就意味着有新的连接请求
	fd_max=serv_sock;

	while(1)
	{
		cpy_reads=reads;
		timeout.tv_sec=5;
		timeout.tv_usec=5000;

		if((fd_num=select(fd_max+1, &cpy_reads, 0, 0, &timeout))==-1){  //当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞 O(n)
            puts("select() erro");break;
		}

		if(fd_num==0){ //如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。O(n)
            puts("time-out");continue;    //select超时
		}

		for(i=0; i<fd_max+1; i++) //轮询查找
		{
			if(FD_ISSET(i, &cpy_reads))  //查找发生转态变换的（有接收数据的套接字）文件描述符
			{
				if(i==serv_sock)     // 如果是服务器端套接字的变化，受理连接请求
				{
					adr_sz=sizeof(clnt_adr);
					clnt_sock=
						accept(serv_sock, (struct sockaddr*)&clnt_adr, &adr_sz);
					FD_SET(clnt_sock, &reads);  //将客户端描述符置入监视位
					if(fd_max<clnt_sock)
						fd_max=clnt_sock;
					printf("connected client: %d \n", clnt_sock);
				}
				else    // 发生变化的套接字不是服务器端套接字时，即有要接受的数据
				{
                    printf("changed sock: %d \n", i);  //i是之前连接的clnt_sock
					str_len=read(i, buf, BUF_SIZE);
					if(str_len==0)    //如果接收的数据是代表关闭连接的EOF
					{
						FD_CLR(i, &reads); //接收的数据为EOF需关闭套接字，并从reads中删除相应的信息
						close(i);
						printf("closed client: %d \n", i);
					}
					else
					{
						write(i, buf, str_len);    // echo
					}
				}
			}
		}
	}
	close(serv_sock);
	return 0;
}

void error_handling(char *buf)
{
	fputs(buf, stderr);
	fputc('\n', stderr);
	exit(1);
}

```



#### 调用poll函数监视文件描述符

poll的机制与select类似，与select在本质上没有多大差别（它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态），管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制。（原因是它是基于链表来存储的）poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。

poll只解决了上面的问题3，并没有解决问题1，2的性能开销问题。

```c++
# include <poll.h>
int poll ( struct pollfd * fds, unsigned int nfds, int timeout);
成功时，poll()返回结构体中revents域不为0的文件描述符个数；如果在超时前没有任何事件发生，poll()返回0；失败时，poll()返回-1，并设置errno为下列值之一：
　　EBADF　　       一个或多个结构体中指定的文件描述符无效。

　　EFAULTfds　　 指针指向的地址超出进程的地址空间。

　　EINTR　　　　  请求的事件之前产生一个信号，调用可以重新发起。

　　EINVALnfds　　参数超出PLIMIT_NOFILE值。

　　ENOMEM　　     可用内存不足，无法完成请求。
 
fds：监视的文件描述符
    
nfds：监视的文件描述符的数量
    
timeout参数指定等待的毫秒数，无论I/O是否准备好，poll都会返回。timeout指定为负数值表示无限超时，使poll()一直挂起直到一个指定事件发生；timeout为0指示poll调用立即返回并列出准备好I/O的文件描述符，但并不等待其它的事件。这种情况下，poll()就像它的名字那样，一旦选举出来，立即返回。
struct pollfd {

int fd;         /* 文件描述符 */
short events;         /* 等待的事件 */ 监视该文件描述符的事件掩码
short revents;       /* 实际发生了的事件 */  文件描述符的操作结果事件掩码，内核在调用返回时设置这个域（events域中请求的任何事件都可能在revents域中返回）
} ; 
   POLLIN 　　　　　　　　有数据可读。（需要读取数据的情况）

　　POLLRDNORM 　　　　  有普通数据可读。

　　POLLRDBAND　　　　　 有优先数据可读。

　　POLLPRI　　　　　　　　 有紧迫数据可读。（收到OOB数据的情况）

　　POLLOUT　　　　　　    写数据不会导致阻塞。（输出缓冲为空，可以立即发送数据的情况）

　　POLLWRNORM　　　　　  写普通数据不会导致阻塞。

　　POLLWRBAND　　　　　   写优先数据不会导致阻塞。

　　POLLMSGSIGPOLL 　　　　消息可用。

　此外，revents域中还可能返回下列事件：
　　POLLER　　   指定的文件描述符发生错误。

　　POLLHUP　　 指定的文件描述符挂起事件。

　　POLLNVAL　　指定的文件描述符非法。

这些事件在events域中无意义，因为它们在合适的时候总是会从revents中返回。
```

**基于poll的服务器端**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <errno.h>

#include <netinet/in.h>
#include <sys/socket.h>
#include <poll.h>
#include <unistd.h>
#include <sys/types.h>

#define IPADDRESS   "127.0.0.1"
#define PORT        8787
#define MAXLINE     1024
#define LISTENQ     5
#define OPEN_MAX    1000
#define INFTIM      -1

//函数声明
//创建套接字并进行绑定
static int socket_bind(const char* ip,int port);
//IO多路复用poll
static void do_poll(int listenfd);
//处理多个连接
static void handle_connection(struct pollfd *connfds,int num);

int main(int argc,char *argv[])
{
    int  listenfd,connfd,sockfd;
    struct sockaddr_in cliaddr;
    socklen_t cliaddrlen;
    listenfd = socket_bind(IPADDRESS,PORT);
    listen(listenfd,LISTENQ);
    do_poll(listenfd);
    return 0;
}

static int socket_bind(const char* ip,int port)
{
    int  listenfd;
    struct sockaddr_in servaddr;
    listenfd = socket(AF_INET,SOCK_STREAM,0); //服务器端套接字
    if (listenfd == -1)
    {
        perror("socket error:");
        exit(1);
    }
    bzero(&servaddr,sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    inet_pton(AF_INET,ip,&servaddr.sin_addr);
    servaddr.sin_port = htons(port);
    if (bind(listenfd,(struct sockaddr*)&servaddr,sizeof(servaddr)) == -1)
    {
        perror("bind error: ");
        exit(1);
    }
    return listenfd;
}

static void do_poll(int listenfd)
{
    int  connfd  //客户端套接字
    int  sockfd;
    struct sockaddr_in cliaddr;
    socklen_t cliaddrlen;
    struct pollfd clientfds[OPEN_MAX];
    int maxi;
    int i;
    int nready;
    //添加监听描述符
    clientfds[0].fd = listenfd;   //把服务器端套接字置入监听位
    clientfds[0].events = POLLIN;  //有数据可读。
    //初始化客户连接描述符
    for (i = 1;i < OPEN_MAX;i++)
        clientfds[i].fd = -1;
    maxi = 0;
    //循环处理
    for ( ; ; )
    {
        //获取可用描述符的个数
        nready = poll(clientfds,maxi+1,INFTIM);
        if (nready == -1)
        {
            perror("poll error:");
            exit(1);
        }
        //测试监听描述符是否准备好
        if (clientfds[0].revents & POLLIN)  //服务器端有数据可读---有客户端请求连接
        {
            cliaddrlen = sizeof(cliaddr);
            //接受新的连接
            if ((connfd = accept(listenfd,(struct sockaddr*)&cliaddr,&cliaddrlen)) == -1)
            {
                if (errno == EINTR)
                    continue;
                else
                {
                   perror("accept error:");
                   exit(1);
                }
            }
            fprintf(stdout,"accept a new client: %s:%d\n", inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port);
            //将新的连接描述符添加到数组中
            for (i = 1;i < OPEN_MAX;i++)
            {
                if (clientfds[i].fd < 0)
                {
                    clientfds[i].fd = connfd;
                    break;
                }
            }
            if (i == OPEN_MAX)
            {
                fprintf(stderr,"too many clients.\n");
                exit(1);
            }
            //将新的描述符添加到读描述符集合中
            clientfds[i].events = POLLIN;
            //记录客户连接套接字的个数
            maxi = (i > maxi ? i : maxi);
            if (--nready <= 0)
                continue;
        }
        //处理客户连接
        handle_connection(clientfds,maxi);
    }
}

static void handle_connection(struct pollfd *connfds,int num)
{
    int i,n;
    char buf[MAXLINE];
    memset(buf,0,MAXLINE);
    for (i = 1;i <= num;i++)
    {
        if (connfds[i].fd < 0)
            continue;
        //测试客户描述符是否准备好
        if (connfds[i].revents & POLLIN)
        {
            //接收客户端发送的信息
            n = read(connfds[i].fd,buf,MAXLINE);
            if (n == 0)
            {
                close(connfds[i].fd);
                connfds[i].fd = -1;
                continue;
            }
           // printf("read msg is: ");
            write(STDOUT_FILENO,buf,n);
            //向客户端发送buf
            write(connfds[i].fd,buf,n);
        }
    }
}
```



#### epoll！！！

**(1)select==>时间复杂度O(n)**

它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。

**(2)poll==>时间复杂度O(n)**

poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态， 但是它没有最大连接数的限制，原因是它是基于链表来存储的.

**(3)epoll==>时间复杂度O(1)**

epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。（使用了内核文件级别的回调机制，复杂度降低到了O(1)）

**总结：**

（1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。

（2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。

![select-poll-epoll](https://itqiankun.oss-cn-beijing.aliyuncs.com/picture/blogArticles/2020-04-11/1586596177.png)



**epoll的原理**

1. 调用epoll_create建立一个epoll对象(在epoll文件系统中给这个句柄分配资源)；

2. 调用epoll_ctl向epoll对象中添加这100万个连接的套接字；

3. 调用epoll_wait收集发生事件的连接。

这样只需要在进程启动时建立1个epoll对象，并在需要的时候向它添加或删除连接就可以了，因此，在实际收集事件时，epoll_wait的效率就会非常高，因为调用epoll_wait时并没有向它传递这100万个连接，内核也不需要去遍历全部的连接。



![img](https://pic4.zhimg.com/80/v2-e3467895734a9d97f0af3c7bf875aaeb_1440w.jpg)

当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关，如下所示：

> struct eventpoll {
> 　　...
> 　　/*红黑树的根节点，这棵树中存储着所有添加到epoll中的事件，
> 　　也就是这个epoll监控的事件*/
> 　　struct rb_root rbr;
> 　　/*双向链表rdllist保存着将要通过epoll_wait返回给用户的、满足条件的事件*/
> 　　struct list_head rdllist;
> 　　...
> };

我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，

![img](https://pic2.zhimg.com/80/v2-b49bb08a6a1b7159073b71c4d6591185_1440w.jpg)

还会再建立一个rdllist双向链表，用于存储准备就绪的事件，

![img](https://pic1.zhimg.com/80/v2-18b89b221d5db3b5456ab6a0f6dc5784_1440w.jpg)

当epoll_wait调用时，仅仅观察这个rdllist双向链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。

所有添加到epoll中的事件都会与设备(如网卡)驱动程序建立回调关系，也就是说相应事件的发生时会调用这里的回调方法。这个回调方法在内核中叫做ep_poll_callback，它会把这样的事件放到上面的rdllist双向链表中。

例：假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程。

![img](https://pic1.zhimg.com/80/v2-90632d0dc3ded7f91379b848ab53974c_1440w.jpg)

当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。

![img](https://pic4.zhimg.com/80/v2-40bd5825e27cf49b7fd9a59dfcbe4d6f_1440w.jpg)



在epoll中对于每一个事件都会建立一个epitem结构体，如下所示：

> struct epitem {
> 　　...
> 　　//红黑树节点
> 　　struct rb_node rbn;
> 　　//双向链表节点
> 　　struct list_head rdllink;
> 　　//事件句柄等信息
> 　　struct epoll_filefd ffd;
> 　　//指向其所属的eventepoll对象
> 　　struct eventpoll *ep;
> 　　//期待的事件类型
> 　　struct epoll_event event;
> 　　...
> }; // 这里包含每一个事件对应着的信息。

![img](https://pic3.zhimg.com/80/v2-466cb228a75ab03031306e0cdedeffd2_1440w.jpg)



当调用epoll_wait检查是否有发生事件的连接时，只是检查eventpoll对象中的rdllist双向链表是否有epitem元素而已，如果rdllist链表不为空，则这里的事件复制到用户态内存（使用共享内存提高效率）中，同时将事件数量返回给用户。因此epoll_waitx效率非常高。epoll_ctl在向epoll对象中添加、修改、删除事件时，从rbr红黑树中查找事件也非常快，也就是说epoll是非常高效的，它可以轻易地处理百万级别的并发连接。

【总结】：

一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。

执行epoll_create()时，创建了红黑树和就绪链表；

执行epoll_ctl()时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据；

执行epoll_wait()时立刻返回准备就绪链表里的数据即可。

![img](https://pic1.zhimg.com/80/v2-f4af10c10479a9e0ec0ec9f327d78d70_1440w.jpg)

【epoll反应堆模型的流程】：

> epoll_create(); // 创建监听红黑树
> epoll_ctl(); // 向书上添加监听fd
> epoll_wait(); // 监听
> 有客户端连接上来--->lfd调用acceptconn()--->将cfd挂载到红黑树上监听其读事件--->
> epoll_wait()返回cfd--->cfd回调recvdata()--->将cfd摘下来监听写事件--->
> epoll_wait()返回cfd--->cfd回调senddata()--->将cfd摘下来监听读事件--->...--->

![img](https://pic4.zhimg.com/80/v2-d6ef740d762bf8e270ee20b9a7053857_1440w.jpg)

```c++

int epoll_create(int size);    //创建保存epoll文件描述符的空间，成功返回一个epoll句柄描述符

size:内核要监听的描述符数量，创建的文件描述符空间大小并非由size决定，是由操作系统决定，size只是向操作系统建议的空间大小
    
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);          //向空间注册并注销文件描述符，成功时返回0，每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。

epfd:epoll句柄
op:fd操作类型
    EPOLL_CTL_ADD 注册新的fd到epfd中
	EPOLL_CTL_MOD 修改已注册的fd的监听事件
	EPOLL_CTL_DEL 从epfd中删除一个fd
fd:要监听的描述符
event:要监听的事件
    
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);  //等待文件描述符发生变化,成功时返回发生事件的文件描述符

epfd: epoll句柄
events: 保存发生事件的文件描述符集合的结构体地址，表示从内核得到的就绪事件集合
maxevents: events中可以保存的最大事件数，告诉内核events的大小
timeout: 等待时间，-1时一直等待直到发生事件

struct epoll_event {
    __uint32_t events;  /* Epoll events */ 
    epoll_data_t data;  /* User data variable */
};
events可以是以下几个宏的集合：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
/*声明完epoll_event结构体数组后，传递给epoll_wait函数时，发生变化的文件描述符信息将被填入该数组，无需像select函数那样针对所有文件描述符进行循环 */
    
typedef union epoll_data {
    void *ptr;
    int fd;         //文件描述符
    __uint32_t u32;
    __uint64_t u64;
} epoll_data_t;




epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。

水平触发（LT）：默认工作模式，即当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；下次调用epoll_wait时，会再次通知此事件
边缘触发（ET）： 当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次通知此事件。（直到你做了某些操作导致该描述符变成未就绪状态了，也就是说边缘触发只在状态由未就绪变为就绪时只通知一次）。
LT和ET原本应该是用于脉冲信号的，可能用它来解释更加形象。Level和Edge指的就是触发点，Level为只要处于水平，那么就一直触发，而Edge则为上升沿和下降沿的时候触发。比如：0->1 就是Edge，1
    ->1 就是Level。

ET模式很大程度上减少了epoll事件的触发次数，因此效率比LT模式下高。
```

**epoll回声服务器端**

```c++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <sys/epoll.h>

#define BUF_SIZE 100
#define EPOLL_SIZE 50
void error_handling(char *buf);

int main(int argc, char *argv[])
{
	int serv_sock, clnt_sock;
	struct sockaddr_in serv_adr, clnt_adr;
	socklen_t adr_sz;
	int str_len, i;
	char buf[BUF_SIZE];

	struct epoll_event *ep_events;  //
	struct epoll_event event;
	int epfd, event_cnt;

	if(argc!=2) {
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}

	serv_sock=socket(PF_INET, SOCK_STREAM, 0);
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port=htons(atoi(argv[1]));
	
	if(bind(serv_sock, (struct sockaddr*) &serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");
	if(listen(serv_sock, 5)==-1)
		error_handling("listen() error");

	epfd=epoll_create(EPOLL_SIZE);   //创建一个epoll例程 epfd
	ep_events=malloc(sizeof(struct epoll_event)*EPOLL_SIZE);  //给epoll_event结构体数组开空间

	event.events=EPOLLIN;  //发生需要读取数据的事件时
	event.data.fd=serv_sock;	
	epoll_ctl(epfd, EPOLL_CTL_ADD, serv_sock, &event);  //将sockfd注册到epoll例程epfd中，并在需要读取数据的情况下产生相应事件

	while(1)
	{
		event_cnt=epoll_wait(epfd, ep_events, EPOLL_SIZE, -1);  //获取已经准备好的描述符事件   (返回发生事件的文件描述符数，同时在第2个参数指向的缓冲巾保存发生事件的文件描述符集合 因此，无需像select那样插入针对所有文件描述符的循环)
		if(event_cnt==-1)
		{
			puts("epoll_wait() error");
			break;
		}

		for(i=0; i<event_cnt; i++)
		{
			if(ep_events[i].data.fd==serv_sock) //由客户端请求连接
			{
				adr_sz=sizeof(clnt_adr);
				clnt_sock=
					accept(serv_sock, (struct sockaddr*)&clnt_adr, &adr_sz);
				event.events=EPOLLIN;
				event.data.fd=clnt_sock;
				epoll_ctl(epfd, EPOLL_CTL_ADD, clnt_sock, &event); //将clnt_sock注册到epoll例程epfd中，并在需要读取数据的情况下产生相应事件（添加一个描述符和事件）
				printf("connected client: %d \n", clnt_sock);
			}
			else
			{
					str_len=read(ep_events[i].data.fd, buf, BUF_SIZE);
					if(str_len==0)    // close request!
					{
						epoll_ctl(
							epfd, EPOLL_CTL_DEL, ep_events[i].data.fd, NULL);
						close(ep_events[i].data.fd);
						printf("closed client: %d \n", ep_events[i].data.fd);
					}
					else
					{
						write(ep_events[i].data.fd, buf, str_len);    // echo!
					}
	
			}
		}
	}
	close(serv_sock);
	close(epfd);
	return 0;
}

void error_handling(char *buf)
{
	fputs(buf, stderr);
	fputc('\n', stderr);
	exit(1);
}

```



**epoll的lt和et模式**

epoll有两种触发的方式即LT（水平触发）和ET（边缘触发）两种，在前者，只要存在着事件就会不断的触发，直到处理完成，而后者只触发一次相同事件或者说只在从非触发到触发两个状态转换的时候儿才触发。

- 对于水平触发模式(LT)，一个事件只要有，就会一直触发；
- 对于边缘触发模式(ET)，只有一个事件从无到有才会触发。不会重复触发。使用ET模式的文件描述符应该是非阻塞的，否则有可能造成饥饿

以 socket 的读事件为例，对于水平模式，只要 socket 上有未读完的数据，就会**一直产生 EPOLLIN 事件**；而对于边缘模式，socket 上每新来一次数据就会触发一次，**如果上一次触发后，未将 socket 上的数据读完，也不会再触发，除非再新来一次数据**。对于 socket 写事件，如果 socket 的 TCP 窗口一直不饱和，会一直触发 EPOLLOUT 事件；而对于边缘模式，只会触发一次，除非 TCP 窗口由不饱和变成饱和再一次变成不饱和，才会再次触发 EPOLLOUT 事件。

**socket 可读事件水平模式触发条件：**

```javascript
1. socket上无数据 => socket上有数据
2. socket处于有数据状态
```

**socket 可读事件边缘模式触发条件：**

```javascript
1. socket上无数据 => socket上有数据
2. socket又新来一次数据
```

**socket 可写事件水平模式触发条件：**

```javascript
1. socket可写   => socket可写
2. socket不可写 => socket可写
```

**socket 可写事件边缘模式触发条件：**

```javascript
1. socket不可写 => socket可写
```

也就是说，如果对于一个非阻塞 socket，如果使用 epoll 边缘模式去检测数据是否可读，触发可读事件以后，一定要一次性把 socket 上的数据收取干净才行，也就是说一定要循环调用 recv 函数直到 recv 出错，错误码是**EWOULDBLOCK**（**EAGAIN** 一样）（此时表示 socket 上本次数据已经读完）；如果使用水平模式，则不用，你可以根据业务一次性收取固定的字节数，或者收完为止。

- **LT 模式下，读事件触发后，可以按需收取想要的字节数，不用把本次接收到的数据收取干净（即不用循环到 recv 或者 read 函数返回 -1，错误码为 EWOULDBLOCK 或 EAGAIN）；ET 模式下，读事件必须把数据收取干净，因为你不一定有下一次机会再收取数据了，即使有机会，也可能存在上次没读完的数据没有及时处理，造成客户端响应延迟。**
- **LT 模式下，不需要写事件一定要及时移除，避免不必要的触发，浪费 CPU 资源；ET 模式下，写事件触发后，如果还需要下一次的写事件触发来驱动任务（例如发上次剩余的数据），你需要继续注册一次检测可写事件。**
- **LT 模式和 ET 模式各有优缺点，无所谓孰优孰劣。使用 LT 模式，我们可以自由决定每次收取多少字节（对于普通 socket）或何时接收连接（对于侦听 socket），但是可能会导致多次触发；使用 ET 模式，我们必须每次都要将数据收完（对于普通 socket）或必须理解调用 accept 接收连接（对于侦听socket），其优点是触发次数少。**

```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <sys/epoll.h>
#include <pthread.h>

#define MAX_EVENT_NUMBER 1024
#define BUFFER_SIZE 10

int setnonblocking( int fd )
{
    int old_option = fcntl( fd, F_GETFL );
    int new_option = old_option | O_NONBLOCK;
    fcntl( fd, F_SETFL, new_option );    //设置fd为非阻塞
    return old_option;
}

void addfd( int epollfd, int fd, bool enable_et )
{
    epoll_event event;
    event.data.fd = fd;
    event.events = EPOLLIN;
    if( enable_et )
    {
        event.events |= EPOLLET;
    }
    epoll_ctl( epollfd, EPOLL_CTL_ADD, fd, &event );
    setnonblocking( fd );
}

void lt( epoll_event* events, int number, int epollfd, int listenfd )
{
    char buf[ BUFFER_SIZE ];
    for ( int i = 0; i < number; i++ )
    {
        int sockfd = events[i].data.fd;
        if ( sockfd == listenfd )
        {
            struct sockaddr_in client_address;
            socklen_t client_addrlength = sizeof( client_address );
            int connfd = accept( listenfd, ( struct sockaddr* )&client_address, &client_addrlength );
            addfd( epollfd, connfd, false );
        }
        else if ( events[i].events & EPOLLIN )    //不用while循环接收，因为如果没接收完，lt后续会继续触发
        {
            printf( "event trigger once\n" );
            memset( buf, '\0', BUFFER_SIZE );
            int ret = recv( sockfd, buf, BUFFER_SIZE-1, 0 );
            if( ret <= 0 )
            {
                close( sockfd );
                continue;
            }
            printf( "get %d bytes of content: %s\n", ret, buf );
        }
        else
        {
            printf( "something else happened \n" );
        }
    }
}

void et( epoll_event* events, int number, int epollfd, int listenfd )
{
    char buf[ BUFFER_SIZE ];
    for ( int i = 0; i < number; i++ )
    {
        int sockfd = events[i].data.fd;
        if ( sockfd == listenfd )
        {
            struct sockaddr_in client_address;
            socklen_t client_addrlength = sizeof( client_address );
            int connfd = accept( listenfd, ( struct sockaddr* )&client_address, &client_addrlength );
            addfd( epollfd, connfd, true );
        }
        else if ( events[i].events & EPOLLIN )  //因为et不会重复触发，所以第一次触发就要保证所有数据都收到
        {
            printf( "event trigger once\n" );
            while( 1 )  
            {
                memset( buf, '\0', BUFFER_SIZE );
                int ret = recv( sockfd, buf, BUFFER_SIZE-1, 0 );
                if( ret < 0 )
                {
                    //对于非阻塞io，下面的条件成立表示数据已经全部读取完毕。此后，epoll就能再次触发sockfd上的epollin事件，以驱动下一次读操作
                    if( ( errno == EAGAIN ) || ( errno == EWOULDBLOCK ) ) 
                    {
                        printf( "read later\n" );
                        break;
                    }
                    close( sockfd ); 
                    break;
                }
                else if( ret == 0 )
                {
                    close( sockfd );
                }
                else
                {
                    printf( "get %d bytes of content: %s\n", ret, buf );
                }
            }
        }
        else
        {
            printf( "something else happened \n" );
        
        }
    }
}

int main( int argc, char* argv[] )
{
    if( argc <= 2 )
    {
        printf( "usage: %s ip_address port_number\n", basename( argv[0] ) );
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi( argv[2] );

    int ret = 0;
    struct sockaddr_in address;
    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );

    int listenfd = socket( PF_INET, SOCK_STREAM, 0 );
    assert( listenfd >= 0 );

    ret = bind( listenfd, ( struct sockaddr* )&address, sizeof( address ) );
    assert( ret != -1 );

    ret = listen( listenfd, 5 );
    assert( ret != -1 );

    epoll_event events[ MAX_EVENT_NUMBER ];
    int epollfd = epoll_create( 5 );
    assert( epollfd != -1 );
    addfd( epollfd, listenfd, true );

    while( 1 )
    {
        int ret = epoll_wait( epollfd, events, MAX_EVENT_NUMBER, -1 );
        if ( ret < 0 )
        {
            printf( "epoll failure\n" );
            break;
        }
    
        lt( events, ret, epollfd, listenfd );  
        //et( events, ret, epollfd, listenfd );
    }

    close( listenfd );
    return 0;
}
```

**多线程下epoll可能造成的问题**

果是多线程在处理，一个SOCKET事件到来，数据开始解析，这时候这个SOCKET又来了同样一个这样的事件，而你的数据解析尚未完成，那么程序会自动调度另外一个线程或者进程来处理新的事件，这造成一个很严重的问题，不同的线程或者进程在处理同一个SOCKET的事件，这会使程序的健壮性大降低而编程的复杂度大大增加！！即使在ET模式下也有可能出现这种情况！！

解决这种现象有两种方法：

第一种方法是在单独的线程或进程里解析数据，也就是说，接收数据的线程接收到数据后立刻将数据转移至另外的线程。

第二种方法就是**EPOLLONESHOT**，可以在epoll上注册这个事件，注册这个事件后，如果在处理写成当前的SOCKET后不再重新注册相关事件，那么这个事件就不再响应了或者说触发了。要想重新注册事件则需要调用epoll_ctl重置文件描述符上的事件，这样前面的socket就不会出现竞态这样就可以通过手动的方式来保证**同一SOCKET只能被一个线程处理，不会跨越多个线程。**

对于注册了**EPOLLONESHOT**的sockfd，操作系统最多会触发一个它注册的读/写/异常事件，且只触发一次（除非用epoll_ctl函数重置该文件描述符上注册的**EPOLLONESHOT**事件。）这样当一个线程在处理sockfd时，其他线程是不可能有机会操作该sockfd的。**注：**当该sockfd处理完毕，应该重置这个sockfd上的**EPOLLONESHOT**，以确保该sockfd下一次可读时，其事件能被触发。

**示例**

```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <sys/epoll.h>
#include <pthread.h>

#define MAX_EVENT_NUMBER 1024
#define BUFFER_SIZE 1024
struct fds
{
   int epollfd;
   int sockfd;
};

int setnonblocking( int fd )
{
    int old_option = fcntl( fd, F_GETFL );
    int new_option = old_option | O_NONBLOCK;
    fcntl( fd, F_SETFL, new_option );
    return old_option;
}

void addfd( int epollfd, int fd, bool oneshot )  //将fd上的事件注册到epollfd内核事件，oneshot指定是否注册fd上的EPOLLONESHOT事件
{
    epoll_event event;
    event.data.fd = fd;
    event.events = EPOLLIN | EPOLLET;
    if( oneshot )
    {
        event.events |= EPOLLONESHOT;
    }
    epoll_ctl( epollfd, EPOLL_CTL_ADD, fd, &event );
    setnonblocking( fd );
}

void reset_oneshot( int epollfd, int fd ) //重置事件，以便下次触发
{
    epoll_event event;
    event.data.fd = fd;
    event.events = EPOLLIN | EPOLLET | EPOLLONESHOT;
    epoll_ctl( epollfd, EPOLL_CTL_MOD, fd, &event );
}
//工作线程
void* worker( void* arg )
{
    int sockfd = ( (fds*)arg )->sockfd;
    int epollfd = ( (fds*)arg )->epollfd;
    printf( "start new thread to receive data on fd: %d\n", sockfd );
    char buf[ BUFFER_SIZE ];
    memset( buf, '\0', BUFFER_SIZE );
    while( 1 )  //读取sockfd上的数据，直到EAGAIN
    {
        int ret = recv( sockfd, buf, BUFFER_SIZE-1, 0 );
        if( ret == 0 )
        {
            close( sockfd );
            printf( "foreiner closed the connection\n" );
            break;
        }
        else if( ret < 0 )
        {
            if( errno == EAGAIN )  
            {
                reset_oneshot( epollfd, sockfd );
                printf( "read later\n" );
                break;
            }
        }
        else
        {
            printf( "get content: %s\n", buf );
            sleep( 5 ); //休眠5s，模拟数据处理过程
        }
    }
    printf( "end thread receiving data on fd: %d\n", sockfd );
}

int main( int argc, char* argv[] )
{
    if( argc <= 2 )
    {
        printf( "usage: %s ip_address port_number\n", basename( argv[0] ) );
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi( argv[2] );

    int ret = 0;
    struct sockaddr_in address;
    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );

    int listenfd = socket( PF_INET, SOCK_STREAM, 0 ); //服务器端套接字
    assert( listenfd >= 0 );

    ret = bind( listenfd, ( struct sockaddr* )&address, sizeof( address ) );
    assert( ret != -1 );

    ret = listen( listenfd, 5 );
    assert( ret != -1 );

    epoll_event events[ MAX_EVENT_NUMBER ];
    int epollfd = epoll_create( 5 );
    assert( epollfd != -1 );
    addfd( epollfd, listenfd, false ); //!!!监听socket listenfd上是不能注册EPOLLONESHOT事件，否则应用程序只能处理一个客户链接，因为后续的客户连接请求不再触发listenfd上的EPOLLIN事件

    while( 1 )
    {
        int ret = epoll_wait( epollfd, events, MAX_EVENT_NUMBER, -1 );
        if ( ret < 0 )
        {
            printf( "epoll failure\n" );
            break;
        }
    
        for ( int i = 0; i < ret; i++ )
        {
            int sockfd = events[i].data.fd;
            if ( sockfd == listenfd )  //如果是服务器端套接字的请求说明是客户端连接请求
            {
                struct sockaddr_in client_address;
                socklen_t client_addrlength = sizeof( client_address );
                int connfd = accept( listenfd, ( struct sockaddr* )&client_address, &client_addrlength );
                addfd( epollfd, connfd, true ); //每个客户fd都要注册ONESHOT
            }
            else if ( events[i].events & EPOLLIN ) //
            {
                pthread_t thread;
                fds fds_for_new_worker;
                fds_for_new_worker.epollfd = epollfd;
                fds_for_new_worker.sockfd = sockfd;
                pthread_create( &thread, NULL, worker, ( void* )&fds_for_new_worker );  //新建一个线程为fd服务
            }
            else
            {
                printf( "something else happened \n" );
            }
        }
    }

    close( listenfd );
    return 0;
}
```



### I/O复用实现非阻塞connect



```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdlib.h>
#include <assert.h>
#include <stdio.h>
#include <time.h>
#include <errno.h>
#include <fcntl.h>
#include <sys/ioctl.h>
#include <unistd.h>
#include <string.h>

#define BUFFER_SIZE 1023

int setnonblocking( int fd )
{
    int old_option = fcntl( fd, F_GETFL );
    int new_option = old_option | O_NONBLOCK;
    fcntl( fd, F_SETFL, new_option );
    return old_option;
}

int unblock_connect( const char* ip, int port, int time ) //超时连接函数，服务器IP，端口号，超时时间。成功返回已经处于连接的socket，失败返回-1
{
    int ret = 0;
    struct sockaddr_in address;
    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );

    int sockfd = socket( PF_INET, SOCK_STREAM, 0 );
    int fdopt = setnonblocking( sockfd );
    ret = connect( sockfd, ( struct sockaddr* )&address, sizeof( address ) );
    if ( ret == 0 )
    {
        printf( "connect with server immediately\n" );
        fcntl( sockfd, F_SETFL, fdopt );
        return sockfd;
    }
    else if ( errno != EINPROGRESS ) //如果没有立即建立连接，errno=EINPROGRESS说明连接还在进行，！=说明出错
    {
        printf( "unblock connect not support\n" );
        return -1;
    }

    fd_set readfds;
    fd_set writefds;
    struct timeval timeout;

    FD_ZERO( &readfds );
    FD_SET( sockfd, &writefds );

    timeout.tv_sec = time;
    timeout.tv_usec = 0;

    ret = select( sockfd + 1, NULL, &writefds, NULL, &timeout );
    if ( ret <= 0 )
    {
        printf( "connection time out\n" );
        close( sockfd );
        return -1;
    }

    if ( ! FD_ISSET( sockfd, &writefds  ) )
    {
        printf( "no events on sockfd found\n" );
        close( sockfd );
        return -1;
    }

    int error = 0;
    socklen_t length = sizeof( error );
    if( getsockopt( sockfd, SOL_SOCKET, SO_ERROR, &error, &length ) < 0 )  //调用getsockopt来获取并清除sockfd上的错误
    {
        printf( "get socket option failed\n" );
        close( sockfd );
        return -1;
    }

    if( error != 0 )
    {
        printf( "connection failed after select with the error: %d \n", error );
        close( sockfd );
        return -1;
    }
    
    printf( "connection ready after select with the socket: %d \n", sockfd );
    fcntl( sockfd, F_SETFL, fdopt );
    return sockfd;
}

int main( int argc, char* argv[] )
{
    if( argc <= 2 )
    {
        printf( "usage: %s ip_address port_number\n", basename( argv[0] ) );
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi( argv[2] );

    int sockfd = unblock_connect( ip, port, 10 );
    if ( sockfd < 0 )
    {
        return 1;
    }
    shutdown( sockfd, SHUT_WR );
    sleep( 200 );
    printf( "send data out\n" );
    send( sockfd, "abc", 3, 0 );
    //sleep( 600 );
    return 0;
}
```



### I/O复用实现聊天室

**服务器端使用POLL同时监听servfd和clntfd**

```c++
#define _GNU_SOURCE 1
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <poll.h>

#define USER_LIMIT 5
#define BUFFER_SIZE 64
#define FD_LIMIT 65535

struct client_data
{
    sockaddr_in address;
    char* write_buf;
    char buf[ BUFFER_SIZE ];
};

int setnonblocking( int fd )
{
    int old_option = fcntl( fd, F_GETFL );
    int new_option = old_option | O_NONBLOCK;
    fcntl( fd, F_SETFL, new_option );
    return old_option;
}

int main( int argc, char* argv[] )
{
    if( argc <= 2 )
    {
        printf( "usage: %s ip_address port_number\n", basename( argv[0] ) );
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi( argv[2] );

    int ret = 0;
    struct sockaddr_in address;
    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );

    int listenfd = socket( PF_INET, SOCK_STREAM, 0 );
    assert( listenfd >= 0 );

    ret = bind( listenfd, ( struct sockaddr* )&address, sizeof( address ) );
    assert( ret != -1 );

    ret = listen( listenfd, 5 );
    assert( ret != -1 );

    client_data* users = new client_data[FD_LIMIT]; //创建user数组，用来存储clntfd，client_data[clntfd]=clntfd下标和fd一一对应
    pollfd fds[USER_LIMIT+1];
    int user_counter = 0;
    for( int i = 1; i <= USER_LIMIT; ++i )
    {
        fds[i].fd = -1;
        fds[i].events = 0;
    }
    fds[0].fd = listenfd;
    fds[0].events = POLLIN | POLLERR;
    fds[0].revents = 0;

    while( 1 )
    {
        ret = poll( fds, user_counter+1, -1 );
        if ( ret < 0 )
        {
            printf( "poll failure\n" );
            break;
        }
    
        for( int i = 0; i < user_counter+1; ++i )
        {
            if( ( fds[i].fd == listenfd ) && ( fds[i].revents & POLLIN ) )
            {
                struct sockaddr_in client_address;
                socklen_t client_addrlength = sizeof( client_address );
                int connfd = accept( listenfd, ( struct sockaddr* )&client_address, &client_addrlength );
                if ( connfd < 0 )
                {
                    printf( "errno is: %d\n", errno );
                    continue;
                }
                if( user_counter >= USER_LIMIT )
                {
                    const char* info = "too many users\n";
                    printf( "%s", info );
                    send( connfd, info, strlen( info ), 0 );
                    close( connfd );
                    continue;
                }
                user_counter++;
                users[connfd].address = client_address; 
                setnonblocking( connfd );
                fds[user_counter].fd = connfd;
                fds[user_counter].events = POLLIN | POLLRDHUP | POLLERR;
                fds[user_counter].revents = 0;
                printf( "comes a new user, now have %d users\n", user_counter );
            }
            else if( fds[i].revents & POLLERR )  //如果客户端关闭连接
            {
                printf( "get an error from %d\n", fds[i].fd );
                char errors[ 100 ];
                memset( errors, '\0', 100 );
                socklen_t length = sizeof( errors );
                if( getsockopt( fds[i].fd, SOL_SOCKET, SO_ERROR, &errors, &length ) < 0 )
                {
                    printf( "get socket option failed\n" );
                }
                continue;
            }
            else if( fds[i].revents & POLLRDHUP )
            {
                users[fds[i].fd] = users[fds[user_counter].fd];
                close( fds[i].fd );
                fds[i] = fds[user_counter];
                i--;
                user_counter--;
                printf( "a client left\n" );
            }
            else if( fds[i].revents & POLLIN )
            {
                int connfd = fds[i].fd;
                memset( users[connfd].buf, '\0', BUFFER_SIZE );
                ret = recv( connfd, users[connfd].buf, BUFFER_SIZE-1, 0 );
                printf( "get %d bytes of client data %s from %d\n", ret, users[connfd].buf, connfd );
                if( ret < 0 )
                {
                    if( errno != EAGAIN )
                    {
                        close( connfd );
                        users[fds[i].fd] = users[fds[user_counter].fd];
                        fds[i] = fds[user_counter];
                        i--;
                        user_counter--;
                    }
                }
                else if( ret == 0 )
                {
                    printf( "code should not come to here\n" );
                }
                else
                {
                    for( int j = 1; j <= user_counter; ++j ) //如果接收到客户端数据，通知其他socketfd装备写数据
                    {
                        if( fds[j].fd == connfd )
                        {
                            continue;
                        }
                        
                        fds[j].events |= ~POLLIN;  //注册其他fd POLLOUT
                        fds[j].events |= POLLOUT;
                        users[fds[j].fd].write_buf = users[connfd].buf;
                    }
                }
            }
            else if( fds[i].revents & POLLOUT )
            {
                int connfd = fds[i].fd;
                if( ! users[connfd].write_buf )
                {
                    continue;
                }
                ret = send( connfd, users[connfd].write_buf, strlen( users[connfd].write_buf ), 0 ); 
                users[connfd].write_buf = NULL;
                fds[i].events |= ~POLLOUT;
                fds[i].events |= POLLIN;
            }
        }
    }

    delete [] users;
    close( listenfd );
    return 0;
}
```



### I/O复用监听多个窗口



```c++
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <assert.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <fcntl.h>
#include <stdlib.h>
#include <sys/epoll.h>
#include <pthread.h>

#define MAX_EVENT_NUMBER 1024
#define TCP_BUFFER_SIZE 512
#define UDP_BUFFER_SIZE 1024

int setnonblocking( int fd )
{
    int old_option = fcntl( fd, F_GETFL );
    int new_option = old_option | O_NONBLOCK;
    fcntl( fd, F_SETFL, new_option );
    return old_option;
}

void addfd( int epollfd, int fd )
{
    epoll_event event;
    event.data.fd = fd;
    //event.events = EPOLLIN | EPOLLET;
    event.events = EPOLLIN;
    epoll_ctl( epollfd, EPOLL_CTL_ADD, fd, &event );
    setnonblocking( fd );
}

int main( int argc, char* argv[] )
{
    if( argc <= 2 )
    {
        printf( "usage: %s ip_address port_number\n", basename( argv[0] ) );
        return 1;
    }
    const char* ip = argv[1];
    int port = atoi( argv[2] );

    int ret = 0;
    struct sockaddr_in address;
    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );

    int listenfd = socket( PF_INET, SOCK_STREAM, 0 );
    assert( listenfd >= 0 );

    ret = bind( listenfd, ( struct sockaddr* )&address, sizeof( address ) );
    assert( ret != -1 );

    ret = listen( listenfd, 5 );
    assert( ret != -1 );

    bzero( &address, sizeof( address ) );
    address.sin_family = AF_INET;
    inet_pton( AF_INET, ip, &address.sin_addr );
    address.sin_port = htons( port );
    int udpfd = socket( PF_INET, SOCK_DGRAM, 0 );
    assert( udpfd >= 0 );

    ret = bind( udpfd, ( struct sockaddr* )&address, sizeof( address ) );
    assert( ret != -1 );

    epoll_event events[ MAX_EVENT_NUMBER ];
    int epollfd = epoll_create( 5 );
    assert( epollfd != -1 );
    addfd( epollfd, listenfd );   //注册TCP和UDP上的可读事件
    addfd( epollfd, udpfd );

    while( 1 )
    {
        int number = epoll_wait( epollfd, events, MAX_EVENT_NUMBER, -1 );
        if ( number < 0 )
        {
            printf( "epoll failure\n" );
            break;
        }
    
        for ( int i = 0; i < number; i++ )
        {
            int sockfd = events[i].data.fd;
            if ( sockfd == listenfd )
            {
                struct sockaddr_in client_address;
                socklen_t client_addrlength = sizeof( client_address );
                int connfd = accept( listenfd, ( struct sockaddr* )&client_address, &client_addrlength );
                addfd( epollfd, connfd );
            }
            else if ( sockfd == udpfd )
            {
                char buf[ UDP_BUFFER_SIZE ];
                memset( buf, '\0', UDP_BUFFER_SIZE );
                struct sockaddr_in client_address;
                socklen_t client_addrlength = sizeof( client_address );

                ret = recvfrom( udpfd, buf, UDP_BUFFER_SIZE-1, 0, ( struct sockaddr* )&client_address, &client_addrlength );
                if( ret > 0 )
                {
                    sendto( udpfd, buf, UDP_BUFFER_SIZE-1, 0, ( struct sockaddr* )&client_address, client_addrlength );
                }
            }
            else if ( events[i].events & EPOLLIN )
            {
                char buf[ TCP_BUFFER_SIZE ];
                while( 1 )
                {
                    memset( buf, '\0', TCP_BUFFER_SIZE );
                    ret = recv( sockfd, buf, TCP_BUFFER_SIZE-1, 0 );
                    if( ret < 0 )
                    {
                        if( ( errno == EAGAIN ) || ( errno == EWOULDBLOCK ) )
                        {
                            break;
                        }
                        close( sockfd );
                        break;
                    }
                    else if( ret == 0 )
                    {
                        close( sockfd );
                    }
                    else
                    {
                        send( sockfd, buf, ret, 0 );
                    }
                }
            }
            else
            {
                printf( "something else happened \n" );
            }
        }
    }

    close( listenfd );
    return 0;
}
```



## 多播/广播服务器

**你是一位电台主持人，现在有1000人在收听你的广播，你需要向1000人发送数据，如果使用TCP，你需要维护1000个套接字，如果用UDP，你也需要1000次传输，这是在是太慢了。而且也会对服务器端和网络流量产生负面影响**

### 基于UDP完成多播（Multica）数据传输



### 基于UDP完成广播（Broadcast）数据传输



## END--简易HTTP服务器

### HTTP(Hypertex Transfer Protocol) 超文本传输协议

**1.什么是协议？**

网络协议是计算机之间为了实现网络通信而达成的一种“约定”或者”规则“，有了这种”约定“，不同厂商的生产设备，以及不同操作系统组成的计算机之间，就可以实现通信。

**2.HTTP协议是什么？**

HTTP协议是**超文本传输协议**的缩写，英文是Hyper Text Transfer Protocol。它是从WEB服务器传输超文本标记语言(HTML)到本地浏览器的传送协议。

设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。

HTPP有多个版本，目前广泛使用的是HTTP/1.1版本。

**3.HTTP原理**

HTTP是一个基于TCP/IP通信协议来传递数据的协议，传输的数据类型为HTML 文件,、图片文件, 查询结果等。

HTTP协议一般用于B/S架构（）。浏览器作为HTTP客户端通过URL向HTTP服务端即WEB服务器发送所有请求。

我们以访问百度为例：

![img](https://pic4.zhimg.com/80/v2-fbef2c48d13068978904f3d1688728ab_1440w.jpg)访问百度流程

**4.HTTP特点**

1. http协议支持客户端/服务端模式，也是一种请求/响应模式的协议。
2. 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。
3. 灵活：HTTP允许传输任意类型的数据对象。传输的类型由Content-Type加以标记。
4. 无连接：限制每次连接只处理一个请求。**服务器处理完请求，并收到客户的应答后，即断开连接**，但是却不利于客户端与服务器保持会话连接，为了弥补这种不足，产生了两项**记录http状态的技术，一个叫做Cookie,一个叫做Session。**
5. 无状态：无状态是指协议对于事务处理没有记忆，后续处理需要前面的信息，则必须重传。

**5.URI和URL的区别**

HTTP使用统一资源标识符（Uniform Resource Identifiers, URI）来传输数据和建立连接。

- URI：Uniform Resource Identifier 统一资源**标识**符(区分互联网上不同的资源。)

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/3/22/170ffd677629b70d~tplv-t2oaga2asx-watermark.awebp)

**scheme** 表示协议名，比如`http`, `https`, `file`等等。后面必须和`://`连在一起。

**user:passwd**@ 表示登录主机时的用户信息，不过很不安全，不推荐使用，也不常用。

**host:port**表示主机名和端口。

**path**表示请求路径，标记资源所在位置。

**query**表示查询参数，为`key=val`这种形式，多个键值对之间用`&`隔开。

**fragment**表示 URI 所定位的资源内的一个**锚点**，浏览器可以根据这个锚点跳转到对应的位置。

举个例子:

```css
https://www.baidu.com/s?wd=HTTP&rsv_spt=1
```

这个 URI 中，`https`即`scheme`部分，`www.baidu.com`为`host:port`部分（注意，http 和 https 的默认端口分别为80、443），`/s`为`path`部分，而`wd=HTTP&rsv_spt=1`就是`query`部分。

- URL：Uniform Resource Location 统一资源**定位**符



URI 是用来标示 一个具体的资源的，我们可以通过 URI 知道一个资源是什么。

URL 则是用来定位具体的资源的，标示了一个具体的资源位置。互联网上的每个文件都有一个唯一的URL。

#### **6.HTTP报文组成**

**请求报文构成**

1. 请求行：包括请求方法、URL、协议/版本
2. 请求头(Request Header)

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/3/22/170ffd6012e2fc88~tplv-t2oaga2asx-watermark.awebp)

头部字段的格式：

- 1. 字段名不区分大小写
- 1. 字段名不允许出现空格，不可以出现下划线`_`
- 1. 字段名后面必须**紧接着`:`**

1. 请求正文

![img](https://pic4.zhimg.com/80/v2-770cc76b2cb7da75d04a886015a9565b_1440w.jpg)请求报文组成

**响应报文构成**

1. 状态行
2. 响应头

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/3/22/170ffd62af8538e4~tplv-t2oaga2asx-watermark.awebp)

1. 响应正文

![img](https://pic4.zhimg.com/80/v2-58506e2188987db01ffb1589e208d83b_1440w.jpg)响应报文组成

**7.常见请求方法**

- GET:请求指定的页面信息，并返回实体主体。
- POST:向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。
- HEAD:类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头
- PUT:从客户端向服务器传送的数据取代指定的文档的内容。
- DELETE:请求服务器删除指定的页面。

**get请求**

![img](https://pic4.zhimg.com/80/v2-c3de118ff545b4d49e07874063c34a4f_1440w.jpg)GET请求

**post请求**

![img](https://pic4.zhimg.com/80/v2-770cc76b2cb7da75d04a886015a9565b_1440w.jpg)POST请求

**post和get的区别：**

- 都包含请求头请求行，post多了请求body。
- get多用来查询，请求参数放在url中，不会对服务器上的内容产生作用。post用来提交，如把账号密码放入body中。
- GET是直接添加到URL后面的，直接就可以在URL中看到内容，而POST是放在报文内部的，用户无法直接看到。
- GET提交的数据长度是有限制的，因为URL长度有限制，具体的长度限制视浏览器而定。而POST没有。
- 从**缓存**的角度，GET 请求会被浏览器主动缓存下来，留下历史记录，而 POST 默认不会。
- 从**编码**的角度，GET 只能进行 URL 编码，只能接收 ASCII 字符，而 POST 没有限制。
- 从**参数**的角度，GET 一般放在 URL 中，因此不安全，POST 放在请求体中，更适合传输敏感信息。
- 从**幂等性**的角度，`GET`是**幂等**的，而`POST`不是。(`幂等`表示执行相同的操作，结果也是相同的)
- 从**TCP**的角度，GET 请求会把请求报文一次性发出去，而 POST 会分为两个 TCP 数据包，首先发 header 部分，如果服务器响应 100(continue)， 然后发 body 部分。(**火狐**浏览器除外，它的 POST 请求只发一个 TCP 包)



**8.响应状态码**

**访问一个网页时，浏览器会向web服务器发出请求。此网页所在的服务器会返回一个包含HTTP状态码的信息头用以响应浏览器的请求。**

**状态码分类**：

- 1XX- 信息型，服务器收到请求，需要请求者继续操作。

**101 Switching Protocols**。在`HTTP`升级为`WebSocket`的时候，如果服务器同意变更，就会发送状态码 101。

- 2XX- 成功型，请求成功收到，理解并处理。

**200 OK**是见得最多的成功状态码。通常在响应体中放有数据。

**204 No Content**含义与 200 相同，但响应头后没有 body 数据。

**206 Partial Content**顾名思义，表示部分内容，它的使用场景为 HTTP 分块下载和断点续传，当然也会带上相应的响应头字段`Content-Range`。

- 3XX - 重定向，需要进一步的操作以完成请求。

**301 Moved Permanently**即永久重定向，对应着**302 Found**，即临时重定向。

比如你的网站从 HTTP 升级到了 HTTPS 了，以前的站点再也不用了，应当返回`301`，这个时候浏览器默认会做缓存优化，在第二次访问的时候自动访问重定向的那个地址。

而如果只是暂时不可用，那么直接返回`302`即可，和`301`不同的是，浏览器并不会做缓存优化。

**304 Not Modified**: 当协商缓存命中时会返回这个状态码。详见[浏览器缓存](https://link.juejin.cn?target=http%3A%2F%2F47.98.159.95%2Fmy_blog%2Fperform%2F001.html)

- 4XX - 客户端错误，请求包含语法错误或无法完成请求。

**400 Bad Request**: 开发者经常看到一头雾水，只是笼统地提示了一下错误，并不知道哪里出错了。

**403 Forbidden**: 这实际上并不是请求报文出错，而是服务器禁止访问，原因有很多，比如法律禁止、信息敏感。

**404 Not Found**: 资源未找到，表示没在服务器上找到相应的资源。

**405 Method Not Allowed**: 请求方法不被服务器端允许。

**406 Not Acceptable**: 资源无法满足客户端的条件。

**408 Request Timeout**: 服务器等待了太长时间。

**409 Conflict**: 多个请求发生了冲突。

**413 Request Entity Too Large**: 请求体的数据过大。

**414 Request-URI Too Long**: 请求行里的 URI 太大。

**429 Too Many Request**: 客户端发送的请求过多。

**431 Request Header Fields Too Large**请求头的字段内容太大。

- 5XX - 服务器错误，服务器在处理请求的过程中发生了错误。

**500 Internal Server Error**: 仅仅告诉你服务器出错了，出了啥错咱也不知道。

**501 Not Implemented**: 表示客户端请求的功能还不支持。

**502 Bad Gateway**: 服务器自身是正常的，但访问的时候出错了，啥错误咱也不知道。

**503 Service Unavailable**: 表示服务器当前很忙，暂时无法响应服务。

#### HTTP 特点

HTTP 的特点概括如下:

1. 灵活可扩展，主要体现在两个方面。一个是语义上的自由，只规定了基本格式，比如空格分隔单词，换行分隔字段，其他的各个部分都没有严格的语法限制。另一个是传输形式的多样性，不仅仅可以传输文本，还能传输图片、视频等任意数据，非常方便。
2. 可靠传输。HTTP 基于 TCP/IP，因此把这一特性继承了下来。这属于 TCP 的特性，不具体介绍了。
3. 请求-应答。也就是`一发一收`、`有来有回`， 当然这个请求方和应答方不单单指客户端和服务器之间，如果某台服务器作为代理来连接后端的服务端，那么这台服务器也会扮演**请求方**的角色。
4. 无状态。这里的状态是指**通信过程的上下文信息**，而每次 http 请求都是独立、无关的，默认不需要保留状态信息。

**HTTP 缺点**

**无状态**

所谓的优点和缺点还是要分场景来看的，对于 HTTP 而言，最具争议的地方在于它的**无状态**。

在需要长连接的场景中，需要保存大量的上下文信息，以免传输大量重复的信息，那么这时候无状态就是 http 的缺点了。

但与此同时，另外一些应用仅仅只是为了获取一些数据，不需要保存连接上下文信息，无状态反而减少了网络开销，成为了 http 的优点。

**明文传输**

即协议里的报文(主要指的是头部)不使用二进制数据，而是文本形式。

这当然对于调试提供了便利，但同时也让 HTTP 的报文信息暴露给了外界，给攻击者也提供了便利。`WIFI陷阱`就是利用 HTTP 明文传输的缺点，诱导你连上热点，然后疯狂抓你所有的流量，从而拿到你的敏感信息。

**队头阻塞问题**

当 http 开启长连接时，共用一个 TCP 连接，同一时刻只能处理一个请求，那么当前请求耗时过长的情况下，其它的请求只能处于阻塞状态，也就是著名的**队头阻塞**问题。接下来会有一小节讨论这个问题。



#### 简易HTTP服务器

```c++
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <pthread.h>

#define BUF_SIZE 1024
#define SMALL_BUF 100

void* request_handler(void* arg);
void  send_data(FILE* fp, char* ct, char* file_name);
char* content_type(char* file);
void send_error(FILE* fp);
void error_handling(char* message);

int main(int argc, char *argv[])
{
	int serv_sock, clnt_sock;
	struct sockaddr_in serv_adr, clnt_adr;
	socklen_t clnt_adr_size;
	char buf[BUF_SIZE];
	pthread_t t_id;
	if(argc!=2) {
		printf("Usage : %s <port>\n", argv[0]);
		exit(1);
	}

	serv_sock=socket(PF_INET, SOCK_STREAM, 0);
	memset(&serv_adr, 0, sizeof(serv_adr));
	serv_adr.sin_family=AF_INET;
	serv_adr.sin_addr.s_addr=htonl(INADDR_ANY);
	serv_adr.sin_port = htons(atoi(argv[1]));
	if(bind(serv_sock, (struct sockaddr*)&serv_adr, sizeof(serv_adr))==-1)
		error_handling("bind() error");
	if(listen(serv_sock, 20)==-1)
		error_handling("listen() error");

	while(1)
	{
		clnt_adr_size=sizeof(clnt_adr);
		clnt_sock=accept(serv_sock, (struct sockaddr*)&clnt_adr, &clnt_adr_size);
		printf("Connection Request : %s:%d\n",
			inet_ntoa(clnt_adr.sin_addr), ntohs(clnt_adr.sin_port));
		pthread_create(&t_id, NULL, request_handler, &clnt_sock);
		pthread_detach(t_id);
	}
	close(serv_sock);
	return 0;
}

void* request_handler(void *arg)
{
	int clnt_sock=*((int*)arg);
	char req_line[SMALL_BUF];
	FILE* clnt_read;
	FILE* clnt_write;

	char method[10];
	char ct[15];
	char file_name[30];

	clnt_read=fdopen(clnt_sock, "r");
	clnt_write=fdopen(dup(clnt_sock), "w");
	fgets(req_line, SMALL_BUF, clnt_read);
	if(strstr(req_line, "HTTP/")==NULL) //查看是否为HTTP提出的请求
	{
		send_error(clnt_write);
		fclose(clnt_read);
		fclose(clnt_write);
		return ;
	 }

	strcpy(method, strtok(req_line, " /"));
	strcpy(file_name, strtok(NULL, " /")); //查看请求文件名
	strcpy(ct, content_type(file_name));  //查看content-type
	if(strcmp(method, "GET")!=0)   //是否为GET方式的请求
	{
		send_error(clnt_write);
		fclose(clnt_read);
		fclose(clnt_write);
		return ;
	 }

	fclose(clnt_read);
	send_data(clnt_write, ct, file_name);
}

void send_data(FILE* fp, char* ct, char* file_name)
{
	char protocol[]="HTTP/1.0 200 OK\r\n";
	char server[]="Server:Linux Web Server \r\n";
	char cnt_len[]="Content-length:2048\r\n";
	char cnt_type[SMALL_BUF];
	char buf[BUF_SIZE];
	FILE* send_file;

	sprintf(cnt_type, "Content-type:%s\r\n\r\n", ct);
	send_file=fopen(file_name, "r");
	if(send_file==NULL)
	{
		send_error(fp);
		return;
	}
	//传输头信息
	fputs(protocol, fp);
	fputs(server, fp);
	fputs(cnt_len, fp);
	fputs(cnt_type, fp);
    //传输请求数据
	while(fgets(buf, BUF_SIZE, send_file)!=NULL)
	{
		fputs(buf, fp);
		fflush(fp);
	}
	fflush(fp);
	fclose(fp);
}

char* content_type(char* file)
{
	char extension[SMALL_BUF];
	char file_name[SMALL_BUF];
	strcpy(file_name, file);
	strtok(file_name, ".");
	strcpy(extension, strtok(NULL, "."));

	if(!strcmp(extension, "html")||!strcmp(extension, "htm"))
		return "text/html";
	else
		return "text/plain";
}

void send_error(FILE* fp)
{
	char protocol[]="HTTP/1.0 400 Bad Request\r\n";
	char server[]="Server:Linux Web Server \r\n";
	char cnt_len[]="Content-length:2048\r\n";
	char cnt_type[]="Content-type:text/html\r\n\r\n";
	char content[]="<html><head><title>NETWORK</title></head>"
	       "<body><font size=+5><br>发生错误!查看请求文件名和请求方式"
		   "</font></body></html>";

	fputs(protocol, fp);
	fputs(server, fp);
	fputs(cnt_len, fp);
	fputs(cnt_type, fp);
	fflush(fp);
}

void error_handling(char* message)
{
	fputs(message, stderr);
	fputc('\n', stderr);
	exit(1);
}

```



## 常用I/O模型

**BIO**

**阻塞同步I/O模型**，服务器需要监听端口号，客户端通过IP和端口与服务器简历TCP连接，以同步阻塞的方式传输数据。服务端设计一般都是 客户端-线程模型，新来一个客户端连接请求，就新建一个线程处理连接和数据传输

当客户端连接较多时就会大大消耗服务器的资源，线程数量可能超过最大承受量

**伪异步I/O**

与BIO类似，只是将客户端-线程的模式换成了线程池，可以灵活设置线程池的大小。但这只是对BIO的一种优化手段，并没有解决线程连接的阻塞问题。

**NIO**

**同步非阻塞I/O模型**，利用selector多路复用器轮询为每一个用户创建连接，这样就不用阻塞用户线程，也不用每个线程忙等待。只使用一个线程轮询I/O事件，比较适合高并发，高负载的网络应用，充分利用系统资源快速处理请求返回响应消息，是和连接较多连接时间I/O任务较短

**AIO**

**异步非阻塞**，需要操作系统内核线程支持，一个用户线程发起一个请求后就可以继续执行，内核线程执行完系统调用后会根据回调函数完成处理工作。比较适合较多I/O任务较长的场景。

**SIGIO**

**信号驱动I/O模型**。首先我们允许套接口进行信号驱动I/O,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。当数据报准备好读取时，内核就为该进程产生一个SIGIO信号。我们随后既可以在信号处理函数中调用recvfrom读取数据报，并通知主循环数据已准备好待处理，也可以立即通知主循环，让它来读取数据报。无论如何处理SIGIO信号，这种模型的优势在于等待数据报到达(第一阶段)期间，进程可以继续执行，不被阻塞。免去了select的阻塞与轮询，当有活跃套接字时，由注册的handler处理。

## 服务器日志系统

诊断日志（diagnostic log） 即log4j、logback、slf4j、glog、 g2log、log4cxx、log4cpp、log4cplus、Pantheios、ezlogger等常用日志库 提供的日志功能。

在服务端编程中，日志是必不可少的，在生产环境中应该做到“Log Everything All The Time” 4。对于关键进程，日志通常要记录 

- 1．收到的每条内部消息的id（还可以包括关键字段、长度、hash 等）； 
- 2．收到的每条外部消息的全文5； 
- 3．发出的每条消息的全文，每条消息都有全局唯一的id 6； 
- 4．关键内部状态的变更，等等。 每条日志都有时间戳，这样就能完整追踪分布式系统中一个事件的 来龙去脉。也只有这样才能查清楚发生故障时究竟发生了什么，比如业 务处理流程卡在了哪一步。 诊断日志不光是给程序员看的，更多的时候是给运维人员看的，因 此日志的内容应避免造成误解，不要误导调查故障的主攻方向，拖延故 障解决的时间。

日志的功能

- 1．**最重要，日志消息有多种级别（level）**，如TRACE、DEBUG、INFO、 WARN、ERROR、FATAL等。 

日志的输出级别在运行时可调，这样同一个可执行文件可以分别在 QA测试环境的时候输出DEBUG级别的日志，在生产环境输出INFO级 别的日志15。在必要的时候也可以临时在线调整日志的输出级别。例如 某台机器的消息量过大、日志文件太多、磁盘空间紧张，那么可以临时 调整为WARNING级别输出，减少日志数目。又比如某个新上线的进程 的行为略显古怪，则可以临时调整为DEBUG级别输出，打印更细节的 日志消息以便分析查错。

- 2．日志消息可能有多个目的地（appender），如文件、socket、 SMTP等。 

以本地文件为日志的destination，那么日志文件的滚动（rolling）是 必需的，这样可以简化日志归档（archive）的实现。rolling的条件通常 有两个：文件大小（例如每写满1GB就换下一个文件）和时间（例如每 天零点新建一个日志文件，不论前一个文件有没有写满）。

- 3．日志消息的格式可配置（layout），例如 org.apache.log4j.PatternLayout。 
- 4．可以设置运行时过滤器（filter），控制不同组件的日志消息的 级别和目的地。



编写Linux服务端程序的时候，我们需要一个高效的日志库。只有 日志库足够高效，程序员才敢在代码中输出足够多的诊断信息，减小运 维难度，提升效率。高效性体现在几方面： 

- 每秒写几千上万条日志的时候没有明显的性能损失。 
- 能应对一个进程产生大量日志数据的场景，例如1GB/min。 
- 不阻塞正常的执行流程。 
- 在多线程程序中，不造成争用（contention）。这里列举一些具体 的性能指标，考虑往普通7200rpm SATA硬盘写日志文件的情况： 
- 磁盘带宽约是110MB/s，日志库应该能瞬时写满这个带宽（不必持 续太久）。 
- 假如每条日志消息的平均长度是110字节，这意味着1秒要写100万 条日志。 以上是“高性能”日志库的最低指标。如果磁盘带宽更高，那么日志 库的预期性能指标也会相应提高。反过来说，在磁盘带宽确定的情况 下，日志库的性能只要“足够好”就行了。



#### 记录日志（LogEvent）

**异步日志**

用一个背景线程负 责收集日志消息，并写入日志文件，其他业务线程只管往这个“日志线 程”发送日志消息

```c++

```



#### 日志输出（LogAppender）



```c++

```



#### 日志信息进行格式化输出（LogFormatter）



```c++

```



#### 日志管理(LoggerManager)



```c++

```



# RPC（Remote Procedure Call）远程过程调用 框架

**RPC 是指计算机 A 上的进程，调用另外一台计算机 B 上的进程，其中 A 上的调用进程被挂起，而 B 上的被调用进程开始执行，当值返回给 A 时，A 进程继续执行。调用方可以通过使用参数将信息传送给被调用方，而后可以通过传回的结果得到信息。而这一过程，对于开发人员来说是透明的。**

![图1 描述了数据报在一个简单的RPC传递的过程](https://waylau.com/images/post/20160630-rpc.png)

远程过程调用采用客户机/服务器(C/S)模式。请求程序就是一个客户机，而服务提供程序就是一台服务器。和常规或本地过程调用一样，远程过程调用是同步操作，在远程过程结果返回之前，需要暂时中止请求程序。使用相同地址空间的低权进程或低权线程允许同时运行多个远程过程调用。













# REST (Representational State Transfer)（http+json）表现层状态转化架构

**如果一个架构符合REST原则，就称它为RESTful架构**  [RESTful API 一种流行的 API 设计风格](https://restfulapi.cn/)

**要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。**如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。

**三、资源（Resources）**

REST的名称"表现层状态转化"中，省略了主语。"表现层"其实指的是"资源"（Resources）的"表现层"。

**所谓"资源"，就是网络上的一个实体，或者说是网络上的一个具体信息。**它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。

所谓"上网"，就是与互联网上一系列的"资源"互动，调用它的URI。

**四、表现层（Representation）**

"资源"是一种信息实体，它可以有多种外在表现形式。**我们把"资源"具体呈现出来的形式，叫做它的"表现层"（Representation）。**

比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。

URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的".html"后缀名是不必要的，因为这个后缀名表示格式，属于"表现层"范畴，而URI应该只代表"资源"的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对"表现层"的描述。

**五、状态转化（State Transfer）**

访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。

互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，**如果客户端想要操作服务器，必须通过某种手段，让服务器端发生"状态转化"（State Transfer）。而这种转化是建立在表现层之上的，所以就是"表现层状态转化"。**

客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：**GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。**

**六、综述**

综合上面的解释，我们总结一下什么是RESTful架构：

　　（1）每一个URI代表一种资源；

　　（2）客户端和服务器之间，传递这种资源的某种表现层；

　　（3）客户端通过四个HTTP动词，对服务器端资源进行操作，实现"表现层状态转化"。

**七、误区**

RESTful架构有一些典型的设计误区。

**最常见的一种设计错误，就是URI包含动词。**因为"资源"表示一种实体，所以应该是名词，URI不应该有动词，动词应该放在HTTP协议中。

举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。

如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：

> 　　POST /accounts/1/transfer/500/to/2

正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：

> 　　POST /transaction HTTP/1.1
> 　　Host: 127.0.0.1
> 　　
> 　　from=1&to=2&amount=500.00

**另一个设计误区，就是在URI中加入版本号**：

> 　　http://www.example.com/app/1.0/foo
>
> 　　http://www.example.com/app/1.1/foo
>
> 　　http://www.example.com/app/2.0/foo

因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见[Versioning REST Services](http://www.informit.com/articles/article.aspx?p=1566460)）：

> 　　Accept: vnd.example-com.foo+json; version=1.0
>
> 　　Accept: vnd.example-com.foo+json; version=1.1
>
> 　　Accept: vnd.example-com.foo+json; version=2.0



# Reactor(反应器)模式

### event-driven architecture

事件驱动体系结构是目前比较广泛使用的一种。这种方式会定义一系列的事件处理器来响应事件的发生，并且将服务端接受连接与对事件的处理分离。其中，事件是一种状态的改变。比如，tcp中socket的new incoming connection、ready for read、ready for write。

**reactor设计模式是event-driven architecture的一种实现方式**，处理多个客户端并发的向服务端请求服务的场景。每种服务在服务端可能由多个方法组成。reactor会解耦并发请求的服务并分发给对应的事件处理器来处理。目前，许多流行的开源框架都用到了reactor模式，如：netty、node.js等，包括java的nio。

**基本的reactor模型**

![img](https://upload-images.jianshu.io/upload_images/10345180-fdaf4d307916cd8f.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp)

reactor主要由以下几个角色构成：handle、Synchronous Event Demultiplexer、Initiation Dispatcher、Event Handler、Concrete Event Handler

![img](https://pic1.zhimg.com/80/v2-614eb69d0186c32de123115b10c3c682_1440w.jpg?source=1940ef5c)



**Handle**

handle在linux中一般称为文件描述符，而在window称为句柄，两者的含义一样。handle是事件的发源地。比如一个网络socket、磁盘文件等。而发生在handle上的事件可以有connection、ready for read、ready for write等。

**Synchronous Event Demultiplexer**

同步事件分离器，本质上是系统调用。比如linux中的select、poll、epoll等。比如，select方法会一直阻塞直到handle上有事件发生时才会返回。

**Event Handler**

事件处理器，其会定义一些回调方法或者称为钩子函数，当handle上有事件发生时，回调方法便会执行，一种事件处理机制。

**Concrete Event Handler**

具体的事件处理器，实现了Event Handler。在回调方法中会实现具体的业务逻辑。

**Initiation Dispatcher**

初始分发器，也是reactor角色，提供了注册、删除与转发event handler的方法。当Synchronous Event Demultiplexer检测到handle上有事件发生时，便会通知initiation dispatcher调用特定的event handler的回调方法。

#### 处理流程

-    1.当应用向Initiation Dispatcher(初始分发器)注册Concrete Event Handler(事件处理器)时，应用会标识出该事件处理器希望Initiation Dispatcher在**某种类型的事件发生发生时向其通知**，事件与handle关联

-       2.Initiation Dispatcher要求注册在其上面的Concrete EventHandler**传递内部关联的handle**，该handle会向操作系统标识

- 3. 当所有的Concrete Event Handler都注册到 Initiation Dispatcher上后，应用会调用handle_events方法来启动Initiation Dispatcher的事件循环，这时Initiation Dispatcher会将每个Concrete Event Handler关联的handle合并，并使用Synchronous Event Demultiplexer（同步事件分离器：epoll）来等待这些handle上事件的发生

- 4. 当与某个事件源对应的handle变为ready时，Synchronous Event Demultiplexer便会通知 Initiation Dispatcher。比如tcp的socket变为ready for reading

- 5. Initiation Dispatcher会触发事件处理器的回调方法。当事件发生时， Initiation Dispatcher会将被一个“key”（表示一个激活的handle）定位和分发给特定的Event Handler的回调方法

- 6. Initiation Dispatcher调用特定的Concrete Event Handler的回调方法来响应其关联的handle上发生的事件

简单点来说就是一个I/O线程内部会有一个一直处于循环状态的事件循环（EventLoop），这个EventLoop会一直监听是否有外部的事件触发（比如说客户端向服务器端发来的连接请求），如果有就调用对应的回调函数进行处理。_（这就是reackor名字的由来：反应堆，对事件进行反应，收到事件后便根据事件类型分配（Dispatch）给某个进程/线程处理）_



# Proactor(前摄器)模式



# Asio(异步I/O)模式



# Actor模型